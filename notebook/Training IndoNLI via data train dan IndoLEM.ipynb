{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c80393",
   "metadata": {},
   "source": [
    "## Mendefinisikan hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0140dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "MODEL_NAME = \"indolem/indobert-base-uncased\"\n",
    "EPOCH = 1\n",
    "SAMPLE = 25\n",
    "# EPOCH = 16\n",
    "# SAMPLE = sys.maxsize\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_LENGTH = 400\n",
    "STRIDE = 100\n",
    "LOGGING_STEPS = 50\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0a8e6",
   "metadata": {},
   "source": [
    "## Instalasi setiap module yang digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c823cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nusacrowd@ git+https://github.com/IndoNLP/nusa-crowd.git@7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Cloning https://github.com/IndoNLP/nusa-crowd.git (to revision 7748513d20331e72f9969f94f5d43c7f2d4a59a5) to /tmp/pip-install-88wfvpqe/nusacrowd_e0e75e3eb6574602ab327056ee04cc93\n",
      "  Running command git clone --filter=blob:none -q https://github.com/IndoNLP/nusa-crowd.git /tmp/pip-install-88wfvpqe/nusacrowd_e0e75e3eb6574602ab327056ee04cc93\n",
      "  Running command git rev-parse -q --verify 'sha^7748513d20331e72f9969f94f5d43c7f2d4a59a5'\n",
      "  Running command git fetch -q https://github.com/IndoNLP/nusa-crowd.git 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Running command git checkout -q 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Resolved https://github.com/IndoNLP/nusa-crowd.git to commit 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: aiohttp==3.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (3.8.1)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: appdirs==1.4.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: async-timeout==4.0.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (4.0.2)\n",
      "Requirement already satisfied: attrs==22.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (22.2.0)\n",
      "Requirement already satisfied: audioread==3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: bioc==1.3.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (1.3.7)\n",
      "Requirement already satisfied: black==22.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (22.12.0)\n",
      "Requirement already satisfied: cachetools==5.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (5.3.0)\n",
      "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (2022.12.7)\n",
      "Requirement already satisfied: cffi==1.15.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (1.15.1)\n",
      "Requirement already satisfied: cfgv==3.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (3.3.1)\n",
      "Requirement already satisfied: charset-normalizer==2.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 14)) (2.1.1)\n",
      "Requirement already satisfied: click==8.1.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (8.1.3)\n",
      "Requirement already satisfied: colorama==0.4.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: conllu==4.5.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 17)) (4.5.2)\n",
      "Requirement already satisfied: datasets==2.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 18)) (2.2.0)\n",
      "Requirement already satisfied: decorator==5.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 19)) (5.1.1)\n",
      "Requirement already satisfied: dill==0.3.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 20)) (0.3.6)\n",
      "Requirement already satisfied: distlib==0.3.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 21)) (0.3.6)\n",
      "Requirement already satisfied: docutils==0.19 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 22)) (0.19)\n",
      "Requirement already satisfied: et-xmlfile==1.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 23)) (1.1.0)\n",
      "Requirement already satisfied: evaluate==0.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 24)) (0.4.0)\n",
      "Requirement already satisfied: ffmpeg==1.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 25)) (1.4)\n",
      "Requirement already satisfied: filelock==3.9.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 26)) (3.9.0)\n",
      "Requirement already satisfied: flake8==6.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 27)) (6.0.0)\n",
      "Requirement already satisfied: frozenlist==1.3.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 28)) (1.3.3)\n",
      "Requirement already satisfied: fsspec==2023.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 29)) (2023.1.0)\n",
      "Requirement already satisfied: git-lfs==1.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 30)) (1.6)\n",
      "Requirement already satisfied: google-auth==2.16.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 31)) (2.16.1)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 32)) (0.4.6)\n",
      "Requirement already satisfied: grpcio==1.51.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 33)) (1.51.1)\n",
      "Requirement already satisfied: huggingface-hub==0.12.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 34)) (0.12.1)\n",
      "Requirement already satisfied: identify==2.5.18 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 35)) (2.5.18)\n",
      "Requirement already satisfied: idna==3.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 36)) (3.4)\n",
      "Requirement already satisfied: importlib-metadata==6.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 37)) (6.0.0)\n",
      "Requirement already satisfied: isort==5.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 38)) (5.12.0)\n",
      "Requirement already satisfied: joblib==1.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 39)) (1.2.0)\n",
      "Requirement already satisfied: jsonlines==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 40)) (3.1.0)\n",
      "Requirement already satisfied: librosa==0.9.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 41)) (0.9.2)\n",
      "Requirement already satisfied: llvmlite==0.39.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 42)) (0.39.1)\n",
      "Requirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 43)) (0.5.3)\n",
      "Requirement already satisfied: lxml==4.9.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 44)) (4.9.2)\n",
      "Requirement already satisfied: Markdown==3.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 45)) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe==2.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 46)) (2.1.2)\n",
      "Requirement already satisfied: mccabe==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 47)) (0.7.0)\n",
      "Requirement already satisfied: multidict==6.0.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 48)) (6.0.4)\n",
      "Requirement already satisfied: multiprocess==0.70.14 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 49)) (0.70.14)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 50)) (1.0.0)\n",
      "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 51)) (3.8.1)\n",
      "Requirement already satisfied: nodeenv==1.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 52)) (1.7.0)\n",
      "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 53)) (0.56.4)\n",
      "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 54)) (1.23.5)\n",
      "Requirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 56)) (3.2.2)\n",
      "Requirement already satisfied: openpyxl==3.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 57)) (3.1.1)\n",
      "Requirement already satisfied: packaging==23.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 58)) (23.0)\n",
      "Requirement already satisfied: pandas==1.3.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 59)) (1.3.3)\n",
      "Requirement already satisfied: pathspec==0.11.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 60)) (0.11.0)\n",
      "Requirement already satisfied: platformdirs==3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 61)) (3.0.0)\n",
      "Requirement already satisfied: pooch==1.6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 62)) (1.6.0)\n",
      "Requirement already satisfied: pre-commit==2.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 63)) (2.19.0)\n",
      "Requirement already satisfied: protobuf==4.22.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 64)) (4.22.0)\n",
      "Requirement already satisfied: pyarrow==11.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 65)) (11.0.0)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 66)) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 67)) (0.2.8)\n",
      "Requirement already satisfied: pycodestyle==2.10.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 68)) (2.10.0)\n",
      "Requirement already satisfied: pycparser==2.21 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 69)) (2.21)\n",
      "Requirement already satisfied: pyflakes==3.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 70)) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 71)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2022.7.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 72)) (2022.7.1)\n",
      "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 73)) (6.0)\n",
      "Requirement already satisfied: regex==2022.10.31 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 74)) (2022.10.31)\n",
      "Requirement already satisfied: requests==2.28.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 75)) (2.28.2)\n",
      "Requirement already satisfied: requests-oauthlib==1.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 76)) (1.3.1)\n",
      "Requirement already satisfied: resampy==0.4.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 77)) (0.4.2)\n",
      "Requirement already satisfied: responses==0.18.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 78)) (0.18.0)\n",
      "Requirement already satisfied: rsa==4.9 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 79)) (4.9)\n",
      "Requirement already satisfied: scikit-learn==1.2.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 80)) (1.2.1)\n",
      "Requirement already satisfied: scipy==1.10.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 81)) (1.10.0)\n",
      "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 82)) (1.16.0)\n",
      "Requirement already satisfied: soundfile==0.12.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 83)) (0.12.1)\n",
      "Requirement already satisfied: tensorboard==2.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 84)) (2.12.0)\n",
      "Requirement already satisfied: tensorboard-data-server==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 85)) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 86)) (1.8.1)\n",
      "Requirement already satisfied: termcolor==2.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 87)) (2.2.0)\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 88)) (3.1.0)\n",
      "Requirement already satisfied: tokenizers==0.13.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 89)) (0.13.2)\n",
      "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 90)) (0.10.2)\n",
      "Requirement already satisfied: tomli==2.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 91)) (2.0.1)\n",
      "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 92)) (1.13.1)\n",
      "Requirement already satisfied: torchaudio==0.13.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 93)) (0.13.1)\n",
      "Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 94)) (4.64.1)\n",
      "Requirement already satisfied: transformers==4.26.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 95)) (4.26.1)\n",
      "Requirement already satisfied: translate-toolkit==3.8.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 96)) (3.8.4)\n",
      "Requirement already satisfied: typing_extensions==4.5.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 97)) (4.5.0)\n",
      "Requirement already satisfied: urllib3==1.26.14 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 98)) (1.26.14)\n",
      "Requirement already satisfied: virtualenv==20.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 99)) (20.19.0)\n",
      "Requirement already satisfied: Werkzeug==2.2.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 100)) (2.2.3)\n",
      "Requirement already satisfied: win32-setctime==1.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 101)) (1.1.0)\n",
      "Requirement already satisfied: xxhash==3.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 102)) (3.2.0)\n",
      "Requirement already satisfied: yarl==1.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 103)) (1.8.2)\n",
      "Requirement already satisfied: zipp==3.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 104)) (3.14.0)\n",
      "Requirement already satisfied: zstandard==0.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 105)) (0.19.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nodeenv==1.7.0->-r requirements.txt (line 52)) (45.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard==2.12.0->-r requirements.txt (line 84)) (0.34.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.7.99)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a75e2908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 23 13:18:23 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    58W / 300W |   1372MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    58W / 300W |   2864MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    45W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    44W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    60W / 300W |    898MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    58W / 300W |   8628MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    44W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    42W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad31c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c07fa7",
   "metadata": {},
   "source": [
    "## Import setiap library yang digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2243097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "import operator\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from evaluate import load\n",
    "from nusacrowd import NusantaraConfigHelper\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from datasets import (\n",
    "  load_dataset, \n",
    "  load_from_disk,\n",
    "  Dataset\n",
    ")\n",
    "from transformers import (\n",
    "  BigBirdTokenizerFast,\n",
    "  BigBirdForSequenceClassification,\n",
    "  DataCollatorWithPadding,\n",
    "  TrainingArguments,\n",
    "  Trainer,\n",
    "  BertForSequenceClassification,\n",
    "  BertForQuestionAnswering,\n",
    "  AutoModel, \n",
    "  BertTokenizerFast,\n",
    "  AutoTokenizer, \n",
    "  AutoModel, \n",
    "  BertTokenizer, \n",
    "  BertForPreTraining,\n",
    "  AutoModelForSequenceClassification,\n",
    "  AutoModelForQuestionAnswering\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cab2f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73826639",
   "metadata": {},
   "source": [
    "## Gunakan tokenizer yang sudah pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c0f6efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/b6663c19a819c04798e7a93d681f9bc34ed57b4a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"indolem/indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/b6663c19a819c04798e7a93d681f9bc34ed57b4a/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/b6663c19a819c04798e7a93d681f9bc34ed57b4a/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/b6663c19a819c04798e7a93d681f9bc34ed57b4a/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/b6663c19a819c04798e7a93d681f9bc34ed57b4a/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/b6663c19a819c04798e7a93d681f9bc34ed57b4a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"indolem/indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/b6663c19a819c04798e7a93d681f9bc34ed57b4a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"indolem/indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae5ffe",
   "metadata": {},
   "source": [
    "## Import dataset IndoNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d511d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset indo_nli (/root/.cache/huggingface/datasets/indo_nli/indonli/1.1.0/d34041bd1d1a555a4bcb4ffdb9fe904778da6f7c5343209fc1485dd68121cb62)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eafefc44aad4cb7a97d141e693d1a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_indonli = load_dataset(\"indonli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a057b",
   "metadata": {},
   "source": [
    "## Fungsi utilitas untuk pre-process data IndoNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2375ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_indonli(examples, tokenizer, MAX_LENGTH):\n",
    "    return tokenizer(\n",
    "        examples['premise'], examples['hypothesis'],\n",
    "        truncation=True, return_token_type_ids=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930214c",
   "metadata": {},
   "source": [
    "## Melakukan tokenisasi data IndoNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2637960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/indo_nli/indonli/1.1.0/d34041bd1d1a555a4bcb4ffdb9fe904778da6f7c5343209fc1485dd68121cb62/cache-1c80317fa3b1799d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/indo_nli/indonli/1.1.0/d34041bd1d1a555a4bcb4ffdb9fe904778da6f7c5343209fc1485dd68121cb62/cache-bdd640fb06671ad1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/indo_nli/indonli/1.1.0/d34041bd1d1a555a4bcb4ffdb9fe904778da6f7c5343209fc1485dd68121cb62/cache-3eb13b9046685257.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/indo_nli/indonli/1.1.0/d34041bd1d1a555a4bcb4ffdb9fe904778da6f7c5343209fc1485dd68121cb62/cache-23b8c1e9392456de.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_data_indonli = data_indonli.map(\n",
    "    preprocess_function_indonli,\n",
    "    batched=True,\n",
    "    load_from_cache_file=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=['premise', 'hypothesis'],\n",
    "    fn_kwargs={'tokenizer': tokenizer, 'MAX_LENGTH': MAX_LENGTH}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2740b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_indonli.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\"], output_all_columns=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a25916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_indonli_train = Dataset.from_dict(tokenized_data_indonli[\"train\"][:SAMPLE])\n",
    "tokenized_data_indonli_validation = Dataset.from_dict(tokenized_data_indonli[\"validation\"][:SAMPLE])\n",
    "tokenized_data_indonli_test_lay = Dataset.from_dict(tokenized_data_indonli[\"test_lay\"][:SAMPLE])\n",
    "tokenized_data_indonli_test_expert = Dataset.from_dict(tokenized_data_indonli[\"test_expert\"][:SAMPLE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00703fff",
   "metadata": {},
   "source": [
    "# Tahapan fine-tune IndoNLI diatas IndoBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac668c92",
   "metadata": {},
   "source": [
    "## Fungsi utilitas untuk komputasi metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79ad2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(\n",
    "        predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8428c0",
   "metadata": {},
   "source": [
    "## Dictionary untuk mapping label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79c934e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'entailment', 1: 'neutral', \n",
    "            2: 'contradiction'}\n",
    "label2id = {'entailment': 0, 'neutral': \n",
    "            1, 'contradiction': 2}\n",
    "accuracy = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a2bbc",
   "metadata": {},
   "source": [
    "## Gunakan model Sequence Classification yang sudah pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a414ff02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/b6663c19a819c04798e7a93d681f9bc34ed57b4a/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/b6663c19a819c04798e7a93d681f9bc34ed57b4a/pytorch_model.bin\n",
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_sc = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=3, \n",
    "    id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "faa3f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sc = model_sc.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d37f2d",
   "metadata": {},
   "source": [
    "## Melakukan pengumpulan data dengan padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57b6709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76747d21",
   "metadata": {},
   "source": [
    "## Mendefinisikan argumen (dataops) untuk training nanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67517378",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_NOW = str(datetime.now()).replace(\":\", \"-\").replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "NAME = 'IndoNLI-data_train-with_IndoLEM'\n",
    "SC = f'./results/{NAME}-{TIME_NOW}'\n",
    "\n",
    "CHECKPOINT_DIR = f'{SC}/checkpoint/'\n",
    "MODEL_DIR = f'{SC}/model/'\n",
    "OUTPUT_DIR = f'{SC}/output/'\n",
    "ACCURACY_DIR = f'{SC}/accuracy/'\n",
    "\n",
    "REPO_NAME = f'fine-tuned-{NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4c1cd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args_sc = TrainingArguments(\n",
    "    \n",
    "    # Checkpoint\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=EPOCH,\n",
    "    \n",
    "    # Log\n",
    "    report_to='tensorboard',\n",
    "    logging_strategy='steps',\n",
    "    logging_first_step=True,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    \n",
    "    # Train\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    bf16=False,\n",
    "    dataloader_num_workers=cpu_count(),\n",
    "    \n",
    "    # Miscellaneous\n",
    "    evaluation_strategy='epoch',\n",
    "    seed=SEED,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=REPO_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad8ad5",
   "metadata": {},
   "source": [
    "## Mulai training untuk fine-tune IndoNLI diatas IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16e07363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/muhammadravi251001/fine-tuned-IndoNLI-data_train-with_IndoLEM into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a3f00c435b4dacb2d2988698eda1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 7.37k/422M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b14983789ab4976bf0baac547121f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Feb22_14-46-36_muhammad-ravi-tensorrt-pod/1677077220.4803088/events.out.tfevents.1677077220…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5428dfd419491f946dea9d8e13e41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Feb23_10-02-03_muhammad-ravi-tensorrt-pod/events.out.tfevents.1677146596.muhammad-ravi-tens…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b960c940517a4c84addf09d721211557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Feb22_14-46-36_muhammad-ravi-tensorrt-pod/events.out.tfevents.1677077220.muhammad-ravi-tens…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503232915a50452f8fce0156921ba1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Feb23_10-02-03_muhammad-ravi-tensorrt-pod/1677146596.596313/events.out.tfevents.1677146596.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac50c3fd7ec40869042246c59b3423d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Feb22_14-46-36_muhammad-ravi-tensorrt-pod/1677077220.4803088/events.out.tfevents.1677077220.mu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2360f0419760475dbcbb00fdd1b341c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Feb23_10-02-03_muhammad-ravi-tensorrt-pod/events.out.tfevents.1677146596.muhammad-ravi-tensorr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d691e35004241f0969a0d06a786e151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Feb22_14-46-36_muhammad-ravi-tensorrt-pod/events.out.tfevents.1677077220.muhammad-ravi-tensorr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e098a98c9148169a2a6b3b644adae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Feb23_10-02-03_muhammad-ravi-tensorrt-pod/1677146596.596313/events.out.tfevents.1677146596.muh…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31d2b961c2848efa76d4e80d1b9a72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file training_args.bin: 100%|##########| 3.62k/3.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10a4f2540614bbbb7184e19d247b3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file training_args.bin:  28%|##7       | 1.00k/3.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2aeed650e545d69fa30d5fea5f261f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Feb23_13-18-53_muhammad-ravi-tensorrt-pod/1677158397.3992813/events.out.tfevents.1677158397…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d36423b22b942f691b104e81d222e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Feb23_13-18-53_muhammad-ravi-tensorrt-pod/1677158397.3992813/events.out.tfevents.1677158397.mu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8211a884bede4b0aa29389aaa7c1e307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Feb23_13-18-53_muhammad-ravi-tensorrt-pod/events.out.tfevents.1677158397.muhammad-ravi-tens…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c667c3b833384d63aba82d87240dd7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Feb23_13-18-53_muhammad-ravi-tensorrt-pod/events.out.tfevents.1677158397.muhammad-ravi-tensorr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656d662392144141bacc5353602f6bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/422M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_sc = Trainer(\n",
    "    model=model_sc,\n",
    "    args=training_args_sc,\n",
    "    train_dataset=tokenized_data_indonli_train,\n",
    "    eval_dataset=tokenized_data_indonli_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16432588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 25\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 110560515\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.558800</td>\n",
       "      <td>1.239740</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/checkpoint/checkpoint-1\n",
      "Configuration saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/checkpoint/checkpoint-1/config.json\n",
      "Model weights saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/checkpoint/checkpoint-1/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/checkpoint/checkpoint-1/tokenizer_config.json\n",
      "Special tokens file saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/checkpoint/checkpoint-1/special_tokens_map.json\n",
      "tokenizer config file saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/checkpoint/tokenizer_config.json\n",
      "Special tokens file saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/checkpoint/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=0.5588150024414062, metrics={'train_runtime': 28.448, 'train_samples_per_second': 0.879, 'train_steps_per_second': 0.035, 'total_flos': 956869499628.0, 'train_loss': 0.5588150024414062, 'epoch': 1.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_sc.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc4820",
   "metadata": {},
   "source": [
    "## Simpan model Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7806a1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/model/\n",
      "Configuration saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/model/config.json\n",
      "Model weights saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/model/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/model/tokenizer_config.json\n",
      "Special tokens file saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/model/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/checkpoint/\n",
      "Configuration saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/checkpoint/config.json\n",
      "Model weights saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/checkpoint/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/checkpoint/tokenizer_config.json\n",
      "Special tokens file saved in ./results/IndoNLI-data_train-with_IndoLEM-2023-02-23_13-28-04_584765/checkpoint/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505a7a88584a4c01ad6534207062fe8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Feb23_13-28-05_muhammad-ravi-tensorrt-pod/1677158967.0945685/events.out.tfevents.1677158967.m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7495c153103446e58ac47c32fd30e5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Feb23_13-28-05_muhammad-ravi-tensorrt-pod/events.out.tfevents.1677158967.muhammad-ravi-tensor…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292ef99a730c4f069823f538ba58260b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin: 100%|##########| 3.56k/3.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/muhammadravi251001/fine-tuned-IndoNLI-data_train-with_IndoLEM\n",
      "   1b50ef4..062bcd9  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.08}]}\n"
     ]
    }
   ],
   "source": [
    "trainer_sc.save_model(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007fe77f",
   "metadata": {},
   "source": [
    "# Melakukan prediksi dari model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87595639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 25\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_result = trainer_sc.predict(tokenized_data_indonli_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8dd4f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "with open(f'{OUTPUT_DIR}/output.txt', \"w\") as f:\n",
    "  f.write(str(predict_result))\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae2ecd7",
   "metadata": {},
   "source": [
    "# Melakukan evaluasi dari prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9bd8d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(eval_pred):\n",
    "    predictions = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(\n",
    "        predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b92b8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_result = compute_accuracy(predict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50ecea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(ACCURACY_DIR), exist_ok=True)\n",
    "with open(f'{ACCURACY_DIR}/accuracy.txt', \"w\") as f:\n",
    "  f.write(str(accuracy_result))\n",
    "  f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
