{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d0b244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this code if you don't have dev.jsonl & train.jsonl\n",
    "#!wget https://huggingface.co/datasets/muhammadravi251001/translated-indo-nli/raw/main/dev.jsonl\n",
    "#!wget https://huggingface.co/datasets/muhammadravi251001/translated-indo-nli/resolve/main/train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c80393",
   "metadata": {},
   "source": [
    "## Mendefinisikan hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0140dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "EPOCH = 1\n",
    "SAMPLE = 25\n",
    "# EPOCH = 16\n",
    "# SAMPLE = sys.maxsize\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_LENGTH = 400\n",
    "STRIDE = 100\n",
    "LOGGING_STEPS = 50\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0a8e6",
   "metadata": {},
   "source": [
    "## Instalasi setiap module yang digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c823cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nusacrowd@ git+https://github.com/IndoNLP/nusa-crowd.git@7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Cloning https://github.com/IndoNLP/nusa-crowd.git (to revision 7748513d20331e72f9969f94f5d43c7f2d4a59a5) to /tmp/pip-install-msdprj5v/nusacrowd_78f53029639247aab424310d9177e4ad\n",
      "  Running command git clone --filter=blob:none -q https://github.com/IndoNLP/nusa-crowd.git /tmp/pip-install-msdprj5v/nusacrowd_78f53029639247aab424310d9177e4ad\n",
      "  Running command git rev-parse -q --verify 'sha^7748513d20331e72f9969f94f5d43c7f2d4a59a5'\n",
      "  Running command git fetch -q https://github.com/IndoNLP/nusa-crowd.git 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Running command git checkout -q 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Resolved https://github.com/IndoNLP/nusa-crowd.git to commit 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: aiohttp==3.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (3.8.1)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: appdirs==1.4.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: async-timeout==4.0.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (4.0.2)\n",
      "Requirement already satisfied: attrs==22.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (22.2.0)\n",
      "Requirement already satisfied: audioread==3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: bioc==1.3.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (1.3.7)\n",
      "Requirement already satisfied: black==22.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (22.12.0)\n",
      "Requirement already satisfied: cachetools==5.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (5.3.0)\n",
      "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (2022.12.7)\n",
      "Requirement already satisfied: cffi==1.15.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (1.15.1)\n",
      "Requirement already satisfied: cfgv==3.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (3.3.1)\n",
      "Requirement already satisfied: charset-normalizer==2.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 14)) (2.1.1)\n",
      "Requirement already satisfied: click==8.1.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (8.1.3)\n",
      "Requirement already satisfied: colorama==0.4.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: conllu==4.5.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 17)) (4.5.2)\n",
      "Requirement already satisfied: datasets==2.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 18)) (2.2.0)\n",
      "Requirement already satisfied: decorator==5.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 19)) (5.1.1)\n",
      "Requirement already satisfied: dill==0.3.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 20)) (0.3.6)\n",
      "Requirement already satisfied: distlib==0.3.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 21)) (0.3.6)\n",
      "Requirement already satisfied: docutils==0.19 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 22)) (0.19)\n",
      "Requirement already satisfied: et-xmlfile==1.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 23)) (1.1.0)\n",
      "Requirement already satisfied: evaluate==0.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 24)) (0.4.0)\n",
      "Requirement already satisfied: ffmpeg==1.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 25)) (1.4)\n",
      "Requirement already satisfied: filelock==3.9.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 26)) (3.9.0)\n",
      "Requirement already satisfied: flake8==6.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 27)) (6.0.0)\n",
      "Requirement already satisfied: frozenlist==1.3.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 28)) (1.3.3)\n",
      "Requirement already satisfied: fsspec==2023.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 29)) (2023.1.0)\n",
      "Requirement already satisfied: git-lfs==1.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 30)) (1.6)\n",
      "Requirement already satisfied: google-auth==2.16.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 31)) (2.16.1)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 32)) (0.4.6)\n",
      "Requirement already satisfied: grpcio==1.51.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 33)) (1.51.1)\n",
      "Requirement already satisfied: huggingface-hub==0.12.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 34)) (0.12.1)\n",
      "Requirement already satisfied: identify==2.5.18 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 35)) (2.5.18)\n",
      "Requirement already satisfied: idna==3.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 36)) (3.4)\n",
      "Requirement already satisfied: importlib-metadata==6.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 37)) (6.0.0)\n",
      "Requirement already satisfied: isort==5.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 38)) (5.12.0)\n",
      "Requirement already satisfied: joblib==1.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 39)) (1.2.0)\n",
      "Requirement already satisfied: jsonlines==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 40)) (3.1.0)\n",
      "Requirement already satisfied: librosa==0.9.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 41)) (0.9.2)\n",
      "Requirement already satisfied: llvmlite==0.39.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 42)) (0.39.1)\n",
      "Requirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 43)) (0.5.3)\n",
      "Requirement already satisfied: lxml==4.9.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 44)) (4.9.2)\n",
      "Requirement already satisfied: Markdown==3.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 45)) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe==2.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 46)) (2.1.2)\n",
      "Requirement already satisfied: mccabe==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 47)) (0.7.0)\n",
      "Requirement already satisfied: multidict==6.0.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 48)) (6.0.4)\n",
      "Requirement already satisfied: multiprocess==0.70.14 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 49)) (0.70.14)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 50)) (1.0.0)\n",
      "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 51)) (3.8.1)\n",
      "Requirement already satisfied: nodeenv==1.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 52)) (1.7.0)\n",
      "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 53)) (0.56.4)\n",
      "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 54)) (1.23.5)\n",
      "Requirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 56)) (3.2.2)\n",
      "Requirement already satisfied: openpyxl==3.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 57)) (3.1.1)\n",
      "Requirement already satisfied: packaging==23.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 58)) (23.0)\n",
      "Requirement already satisfied: pandas==1.3.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 59)) (1.3.3)\n",
      "Requirement already satisfied: pathspec==0.11.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 60)) (0.11.0)\n",
      "Requirement already satisfied: platformdirs==3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 61)) (3.0.0)\n",
      "Requirement already satisfied: pooch==1.6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 62)) (1.6.0)\n",
      "Requirement already satisfied: pre-commit==2.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 63)) (2.19.0)\n",
      "Requirement already satisfied: protobuf==4.22.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 64)) (4.22.0)\n",
      "Requirement already satisfied: pyarrow==11.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 65)) (11.0.0)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 66)) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 67)) (0.2.8)\n",
      "Requirement already satisfied: pycodestyle==2.10.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 68)) (2.10.0)\n",
      "Requirement already satisfied: pycparser==2.21 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 69)) (2.21)\n",
      "Requirement already satisfied: pyflakes==3.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 70)) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 71)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2022.7.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 72)) (2022.7.1)\n",
      "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 73)) (6.0)\n",
      "Requirement already satisfied: regex==2022.10.31 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 74)) (2022.10.31)\n",
      "Requirement already satisfied: requests==2.28.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 75)) (2.28.2)\n",
      "Requirement already satisfied: requests-oauthlib==1.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 76)) (1.3.1)\n",
      "Requirement already satisfied: resampy==0.4.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 77)) (0.4.2)\n",
      "Requirement already satisfied: responses==0.18.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 78)) (0.18.0)\n",
      "Requirement already satisfied: rsa==4.9 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 79)) (4.9)\n",
      "Requirement already satisfied: scikit-learn==1.2.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 80)) (1.2.1)\n",
      "Requirement already satisfied: scipy==1.10.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 81)) (1.10.0)\n",
      "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 82)) (1.16.0)\n",
      "Requirement already satisfied: soundfile==0.12.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 83)) (0.12.1)\n",
      "Requirement already satisfied: tensorboard==2.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 84)) (2.12.0)\n",
      "Requirement already satisfied: tensorboard-data-server==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 85)) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 86)) (1.8.1)\n",
      "Requirement already satisfied: termcolor==2.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 87)) (2.2.0)\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 88)) (3.1.0)\n",
      "Requirement already satisfied: tokenizers==0.13.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 89)) (0.13.2)\n",
      "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 90)) (0.10.2)\n",
      "Requirement already satisfied: tomli==2.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 91)) (2.0.1)\n",
      "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 92)) (1.13.1)\n",
      "Requirement already satisfied: torchaudio==0.13.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 93)) (0.13.1)\n",
      "Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 94)) (4.64.1)\n",
      "Requirement already satisfied: transformers==4.26.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 95)) (4.26.1)\n",
      "Requirement already satisfied: translate-toolkit==3.8.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 96)) (3.8.4)\n",
      "Requirement already satisfied: typing_extensions==4.5.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 97)) (4.5.0)\n",
      "Requirement already satisfied: urllib3==1.26.14 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 98)) (1.26.14)\n",
      "Requirement already satisfied: virtualenv==20.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 99)) (20.19.0)\n",
      "Requirement already satisfied: Werkzeug==2.2.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 100)) (2.2.3)\n",
      "Requirement already satisfied: win32-setctime==1.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 101)) (1.1.0)\n",
      "Requirement already satisfied: xxhash==3.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 102)) (3.2.0)\n",
      "Requirement already satisfied: yarl==1.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 103)) (1.8.2)\n",
      "Requirement already satisfied: zipp==3.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 104)) (3.14.0)\n",
      "Requirement already satisfied: zstandard==0.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 105)) (0.19.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nodeenv==1.7.0->-r requirements.txt (line 52)) (45.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard==2.12.0->-r requirements.txt (line 84)) (0.34.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.10.3.66)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a75e2908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 23 16:02:23 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    58W / 300W |   1372MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    58W / 300W |   6278MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    45W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    44W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    60W / 300W |    898MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    58W / 300W |   8628MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    45W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    42W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad31c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c07fa7",
   "metadata": {},
   "source": [
    "## Import setiap library yang digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2243097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "import operator\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from evaluate import load\n",
    "from nusacrowd import NusantaraConfigHelper\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from datasets import (\n",
    "  load_dataset, \n",
    "  load_from_disk,\n",
    "  Dataset,\n",
    "  DatasetDict\n",
    ")\n",
    "from transformers import (\n",
    "  BigBirdTokenizerFast,\n",
    "  BigBirdForSequenceClassification,\n",
    "  DataCollatorWithPadding,\n",
    "  TrainingArguments,\n",
    "  Trainer,\n",
    "  BertForSequenceClassification,\n",
    "  BertForQuestionAnswering,\n",
    "  AutoModel, \n",
    "  BertTokenizerFast,\n",
    "  AutoTokenizer, \n",
    "  AutoModel, \n",
    "  BertTokenizer, \n",
    "  BertForPreTraining,\n",
    "  AutoModelForSequenceClassification,\n",
    "  AutoModelForQuestionAnswering\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab2f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73826639",
   "metadata": {},
   "source": [
    "## Gunakan tokenizer yang sudah pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0f6efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae5ffe",
   "metadata": {},
   "source": [
    "## Import dataset IndoNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fac19f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>secara konsep krim skimming memiliki dua dimen...</td>\n",
       "      <td>produk dan geografi adalah apa yang membuat cr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anda tahu selama musim dan saya kira di tingka...</td>\n",
       "      <td>anda kehilangan hal-hal ke tingkat berikut jik...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salah satu nomor kami akan menjalankan instruk...</td>\n",
       "      <td>seorang anggota timku akan melaksanakan perint...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bagaimana kau tahu, semua ini adalah informasi...</td>\n",
       "      <td>informasi ini milik mereka.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ya saya katakan bagaimana meskipun jika anda p...</td>\n",
       "      <td>sepatu tenis memiliki kisaran harga.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392697</th>\n",
       "      <td>jelas, california bisa - dan harus - melakukan...</td>\n",
       "      <td>california tidak bisa melakukan yang lebih baik.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392698</th>\n",
       "      <td>pernah dianggap sebagai jalan terindah di erop...</td>\n",
       "      <td>banyak bangunan asli yang telah digantikan ole...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392699</th>\n",
       "      <td>perahu rumah adalah tradisi yang terpelihara d...</td>\n",
       "      <td>tradisi perahu rumah berasal saat raj inggris ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392700</th>\n",
       "      <td>obituaries fondly recall his on-air debates an...</td>\n",
       "      <td>obituari-obituari itu indah dan ditulis dengan...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392701</th>\n",
       "      <td>dalam hal yang lain anda tahu eh bahwa saya ha...</td>\n",
       "      <td>suami saya telah terlalu bekerja akhir-akhir i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>392702 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  premise  \\\n",
       "0       secara konsep krim skimming memiliki dua dimen...   \n",
       "1       anda tahu selama musim dan saya kira di tingka...   \n",
       "2       salah satu nomor kami akan menjalankan instruk...   \n",
       "3       bagaimana kau tahu, semua ini adalah informasi...   \n",
       "4       ya saya katakan bagaimana meskipun jika anda p...   \n",
       "...                                                   ...   \n",
       "392697  jelas, california bisa - dan harus - melakukan...   \n",
       "392698  pernah dianggap sebagai jalan terindah di erop...   \n",
       "392699  perahu rumah adalah tradisi yang terpelihara d...   \n",
       "392700  obituaries fondly recall his on-air debates an...   \n",
       "392701  dalam hal yang lain anda tahu eh bahwa saya ha...   \n",
       "\n",
       "                                               hypothesis  label  \n",
       "0       produk dan geografi adalah apa yang membuat cr...      2  \n",
       "1       anda kehilangan hal-hal ke tingkat berikut jik...      0  \n",
       "2       seorang anggota timku akan melaksanakan perint...      0  \n",
       "3                             informasi ini milik mereka.      0  \n",
       "4                    sepatu tenis memiliki kisaran harga.      2  \n",
       "...                                                   ...    ...  \n",
       "392697   california tidak bisa melakukan yang lebih baik.      1  \n",
       "392698  banyak bangunan asli yang telah digantikan ole...      2  \n",
       "392699  tradisi perahu rumah berasal saat raj inggris ...      0  \n",
       "392700  obituari-obituari itu indah dan ditulis dengan...      2  \n",
       "392701  suami saya telah terlalu bekerja akhir-akhir i...      2  \n",
       "\n",
       "[392702 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_json(path_or_buf='train.jsonl', lines=True)\n",
    "data_train = data_train[['sentence1', 'sentence2', 'gold_label']]\n",
    "data_train = data_train.rename(columns={'sentence1': 'premise', 'sentence2': 'hypothesis', 'gold_label': 'label'})\n",
    "\n",
    "data_train['label'] = data_train['label'].replace(['entailment'], 0)\n",
    "data_train['label'] = data_train['label'].replace(['contradiction'], 1)\n",
    "data_train['label'] = data_train['label'].replace(['neutral'], 2)\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd812475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hak-hak baru cukup bagus</td>\n",
       "      <td>semua orang benar-benar menyukai manfaat terbaru</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>situs ini mencakup daftar semua pemenang pengh...</td>\n",
       "      <td>artikel eksekutif pemerintah yang disimpan di ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eh aku tidak tahu aku punya emosi campur aduk ...</td>\n",
       "      <td>aku menyukainya untuk sebagian besar, tapi mas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ya saya pikir restoran favorit saya selalu men...</td>\n",
       "      <td>restoran favorit saya selalu setidaknya seratu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aku tidak tahu um apakah anda melakukan banyak...</td>\n",
       "      <td>aku tahu persis.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>apakah anda menonton itu?</td>\n",
       "      <td>bisa kau lihat?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>bagi telinga barat, sifat-sifat bahasa yang pa...</td>\n",
       "      <td>bagi telinga barat, sifat bahasa yang paling t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>pencatat menangkap suara tiup keras, tabrakan,...</td>\n",
       "      <td>pencatat tidak menangkap suara apapun.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>itu sikap yang baik!</td>\n",
       "      <td>anda merasa baik tentang hal ini, bukan?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>bloomer (untuk `bunga'), mentega (untuk `ram')...</td>\n",
       "      <td>bloomer adalah kata lain untuk bunga, mentega ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "0                               hak-hak baru cukup bagus   \n",
       "1      situs ini mencakup daftar semua pemenang pengh...   \n",
       "2      eh aku tidak tahu aku punya emosi campur aduk ...   \n",
       "3      ya saya pikir restoran favorit saya selalu men...   \n",
       "4      aku tidak tahu um apakah anda melakukan banyak...   \n",
       "...                                                  ...   \n",
       "19995                          apakah anda menonton itu?   \n",
       "19996  bagi telinga barat, sifat-sifat bahasa yang pa...   \n",
       "19997  pencatat menangkap suara tiup keras, tabrakan,...   \n",
       "19998                               itu sikap yang baik!   \n",
       "19999  bloomer (untuk `bunga'), mentega (untuk `ram')...   \n",
       "\n",
       "                                              hypothesis label  \n",
       "0       semua orang benar-benar menyukai manfaat terbaru     2  \n",
       "1      artikel eksekutif pemerintah yang disimpan di ...     1  \n",
       "2      aku menyukainya untuk sebagian besar, tapi mas...     0  \n",
       "3      restoran favorit saya selalu setidaknya seratu...     1  \n",
       "4                                       aku tahu persis.     1  \n",
       "...                                                  ...   ...  \n",
       "19995                                    bisa kau lihat?     1  \n",
       "19996  bagi telinga barat, sifat bahasa yang paling t...     1  \n",
       "19997             pencatat tidak menangkap suara apapun.     1  \n",
       "19998           anda merasa baik tentang hal ini, bukan?     2  \n",
       "19999  bloomer adalah kata lain untuk bunga, mentega ...     0  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_validation = pd.read_json(path_or_buf='dev.jsonl', lines=True)\n",
    "data_validation = data_validation[['sentence1', 'sentence2', 'gold_label']]\n",
    "data_validation = data_validation.rename(columns={'sentence1': 'premise', 'sentence2': 'hypothesis', 'gold_label': 'label'})\n",
    "\n",
    "data_validation['label'] = data_validation['label'].replace(['entailment'], 0)\n",
    "data_validation['label'] = data_validation['label'].replace(['contradiction'], 1)\n",
    "data_validation['label'] = data_validation['label'].replace(['neutral'], 2)\n",
    "\n",
    "data_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eecd2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train[data_train.label != '-']\n",
    "data_validation = data_validation[data_validation.label != '-']\n",
    "\n",
    "train_dataset = Dataset.from_dict(data_train)\n",
    "validation_dataset = Dataset.from_dict(data_validation)\n",
    "\n",
    "data_indonli_translated = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d9eaf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_indonli = data_indonli_translated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a057b",
   "metadata": {},
   "source": [
    "## Fungsi utilitas untuk pre-process data IndoNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2375ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_indonli(examples, tokenizer, MAX_LENGTH):\n",
    "    return tokenizer(\n",
    "        examples['premise'], examples['hypothesis'],\n",
    "        truncation=True, return_token_type_ids=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930214c",
   "metadata": {},
   "source": [
    "## Melakukan tokenisasi data IndoNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2637960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function_indonli at 0x7fe767d98550> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae421d40cae45b8aeab7a9e49f75026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ee63cd0f2c49a18125f5ca5b71ade0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data_indonli = data_indonli.map(\n",
    "    preprocess_function_indonli,\n",
    "    batched=True,\n",
    "    load_from_cache_file=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=['premise', 'hypothesis'],\n",
    "    fn_kwargs={'tokenizer': tokenizer, 'MAX_LENGTH': MAX_LENGTH}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2740b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_indonli.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\"], output_all_columns=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a25916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_indonli_train = Dataset.from_dict(tokenized_data_indonli[\"train\"][:SAMPLE])\n",
    "tokenized_data_indonli_validation = Dataset.from_dict(tokenized_data_indonli[\"validation\"][:SAMPLE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00703fff",
   "metadata": {},
   "source": [
    "# Tahapan fine-tune IndoNLI diatas IndoBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac668c92",
   "metadata": {},
   "source": [
    "## Fungsi utilitas untuk komputasi metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ad2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(\n",
    "        predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8428c0",
   "metadata": {},
   "source": [
    "## Dictionary untuk mapping label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79c934e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'entailment', 1: 'neutral', \n",
    "            2: 'contradiction'}\n",
    "label2id = {'entailment': 0, 'neutral': \n",
    "            1, 'contradiction': 2}\n",
    "accuracy = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a2bbc",
   "metadata": {},
   "source": [
    "## Gunakan model Sequence Classification yang sudah pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a414ff02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing BertForSequenceClassification: ['roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'lm_head.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.9.output.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.query.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'classifier.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'classifier.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.self.key.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_sc = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=3, \n",
    "    id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faa3f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sc = model_sc.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d37f2d",
   "metadata": {},
   "source": [
    "## Melakukan pengumpulan data dengan padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57b6709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76747d21",
   "metadata": {},
   "source": [
    "## Mendefinisikan argumen (dataops) untuk training nanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67517378",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_NOW = str(datetime.now()).replace(\":\", \"-\").replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "NAME = 'IndoNLI-data_translated-with_XLMR'\n",
    "SC = f'./results/{NAME}-{TIME_NOW}'\n",
    "\n",
    "CHECKPOINT_DIR = f'{SC}/checkpoint/'\n",
    "MODEL_DIR = f'{SC}/model/'\n",
    "OUTPUT_DIR = f'{SC}/output/'\n",
    "ACCURACY_DIR = f'{SC}/accuracy/'\n",
    "\n",
    "REPO_NAME = f'fine-tuned-{NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4c1cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_sc = TrainingArguments(\n",
    "    \n",
    "    # Checkpoint\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=EPOCH,\n",
    "    \n",
    "    # Log\n",
    "    report_to='tensorboard',\n",
    "    logging_strategy='steps',\n",
    "    logging_first_step=True,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    \n",
    "    # Train\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    bf16=False,\n",
    "    dataloader_num_workers=cpu_count(),\n",
    "    \n",
    "    # Miscellaneous\n",
    "    evaluation_strategy='epoch',\n",
    "    seed=SEED,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=REPO_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad8ad5",
   "metadata": {},
   "source": [
    "## Mulai training untuk fine-tune IndoNLI diatas IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16e07363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/muhammadravi251001/fine-tuned-IndoNLI-data_translated-with_XLMR into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cb8fa711d842e2b3d71c9580813855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 1.40k/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2ab4bba45341199dfbae3cfbb20a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file training_args.bin: 100%|##########| 3.62k/3.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2535356d9434036a474c32e1f0d89ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Feb23_09-36-43_muhammad-ravi-tensorrt-pod/events.out.tfevents.1677145027.muhammad-ravi-tens…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84e4135dfc6483796fa5dcc1ce72a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Feb23_09-36-43_muhammad-ravi-tensorrt-pod/1677145027.9321065/events.out.tfevents.1677145027…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50555568831c48898732896c0bb049b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file training_args.bin:  28%|##7       | 1.00k/3.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaf97224f704961b55b4eba0ad4947c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Feb23_09-36-43_muhammad-ravi-tensorrt-pod/events.out.tfevents.1677145027.muhammad-ravi-tensorr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137c93bae17c42babcd9b0b8010b9ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Feb23_09-36-43_muhammad-ravi-tensorrt-pod/1677145027.9321065/events.out.tfevents.1677145027.mu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696eeab0656a47de9661c8eaa3730eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file tokenizer.json:   0%|          | 7.40k/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2c3b8fc34446f4b4d56f0e37df37a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file tokenizer.json:   0%|          | 1.00k/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cc6416217d4fbfaeb1ca19a2f40cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_sc = Trainer(\n",
    "    model=model_sc,\n",
    "    args=training_args_sc,\n",
    "    train_dataset=tokenized_data_indonli_train,\n",
    "    eval_dataset=tokenized_data_indonli_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16432588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 25\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 278045955\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 01:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.265557</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 25\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/checkpoint/checkpoint-1\n",
      "Configuration saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/checkpoint/checkpoint-1/config.json\n",
      "Model weights saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/checkpoint/checkpoint-1/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/checkpoint/checkpoint-1/tokenizer_config.json\n",
      "Special tokens file saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/checkpoint/checkpoint-1/special_tokens_map.json\n",
      "tokenizer config file saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/checkpoint/tokenizer_config.json\n",
      "Special tokens file saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/checkpoint/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=0.5625180006027222, metrics={'train_runtime': 108.2974, 'train_samples_per_second': 0.231, 'train_steps_per_second': 0.009, 'total_flos': 1279594551060.0, 'train_loss': 0.5625180006027222, 'epoch': 1.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_sc.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc4820",
   "metadata": {},
   "source": [
    "## Simpan model Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7806a1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/model/\n",
      "Configuration saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/model/config.json\n",
      "Model weights saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/model/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/model/tokenizer_config.json\n",
      "Special tokens file saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/model/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/checkpoint/\n",
      "Configuration saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/checkpoint/config.json\n",
      "Model weights saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/checkpoint/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/checkpoint/tokenizer_config.json\n",
      "Special tokens file saved in ./results/IndoNLI-data_translated-with_XLMR-2023-02-23_16-03-53_954131/checkpoint/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f7a0eee8b940459d8559d529b3549b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ecf90fca874327beed66b9775d780c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin: 100%|##########| 3.62k/3.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b24f230748a473fa51395a845ace4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Feb23_16-03-53_muhammad-ravi-tensorrt-pod/1677168351.3610508/events.out.tfevents.1677168351.m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c8e60c3bd341fb99c4942015772282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Feb23_16-03-53_muhammad-ravi-tensorrt-pod/events.out.tfevents.1677168351.muhammad-ravi-tensor…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/muhammadravi251001/fine-tuned-IndoNLI-data_translated-with_XLMR\n",
      "   75d07eb..0431969  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.24}]}\n",
      "To https://huggingface.co/muhammadravi251001/fine-tuned-IndoNLI-data_translated-with_XLMR\n",
      "   0431969..e3b5d1f  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_sc.save_model(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007fe77f",
   "metadata": {},
   "source": [
    "# Melakukan prediksi dari model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87595639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 25\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_result = trainer_sc.predict(tokenized_data_indonli_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dd4f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "with open(f'{OUTPUT_DIR}/output.txt', \"w\") as f:\n",
    "  f.write(str(predict_result))\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae2ecd7",
   "metadata": {},
   "source": [
    "# Melakukan evaluasi dari prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bd8d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(eval_pred):\n",
    "    predictions = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(\n",
    "        predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b92b8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_result = compute_accuracy(predict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50ecea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(ACCURACY_DIR), exist_ok=True)\n",
    "with open(f'{ACCURACY_DIR}/accuracy.txt', \"w\") as f:\n",
    "  f.write(str(accuracy_result))\n",
    "  f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
