if __name__ == '__main__':
  # -*- coding: utf-8 -*-
  """[MBZUAI] Menjalankan QA Tanpa Intermediate Task - Transfer Learning.ipynb

  Automatically generated by Colaboratory.

  Original file is located at
      https://colab.research.google.com/drive/1velenOaJpyNkbw70TtztuHPj1VKX4h5f

  # Menjalankan QA Tanpa Intermediate Task - Transfer Learning

  # Import semua module
  """
  from termcolor import colored, cprint
  cprint("Mulai kode QA TANPA Intermediate Task Transfer Learning", "yellow")
  cprint("Mulai import semua module...", "blue")
  import transformers
  import evaluate
  import torch
  import operator
  import ast
  import json
  import re
  import sys
  import os

  import numpy as np
  import pandas as pd
  import torch.nn as nn

  from multiprocessing import cpu_count
  from evaluate import load
  from nusacrowd import NusantaraConfigHelper
  from datetime import datetime

  from datasets import (
      load_dataset, 
      load_from_disk,
      Dataset
  )
  from transformers import (
      BigBirdTokenizerFast,
      BigBirdForSequenceClassification,
      DataCollatorWithPadding,
      TrainingArguments,
      Trainer,
      BertForSequenceClassification,
      BertForQuestionAnswering,
      AutoModel, 
      BertTokenizerFast,
      AutoTokenizer, 
      AutoModel, 
      BertTokenizer, 
      BertForPreTraining,
      AutoModelForSequenceClassification,
      AutoModelForQuestionAnswering
  )
  cprint("Selesai import semua module!", "green")

  """# Definisikan hyperparameter"""
  cprint("Mulai mendefinisikan hyperparameter...", "blue")
  MODEL_NAME = "indolem/indobert-base-uncased"
  SEED = 42
  EPOCH = 1
  BATCH_SIZE = 16
  GRADIENT_ACCUMULATION = 4
  LEARNING_RATE = 1e-5
  MAX_LENGTH = 400
  STRIDE = 100
  LOGGING_STEPS = 50
  WARMUP_RATIO = 0.06
  WEIGHT_DECAY = 0.01
  # Untuk mempercepat training, saya ubah SAMPLE menjadi 100.
  # Bila mau menggunakan keseluruhan data, gunakan: 
  SAMPLE = sys.maxsize
  # SAMPLE = 100
  cprint("Selesai mendefinisikan hyperparameter!", "green")

  """# Import data SQUAD-ID"""
  cprint("Mulai import dataset SQUAD-ID...", "blue")
  conhelps = NusantaraConfigHelper()
  data_squad_id = conhelps.filtered(lambda x: 'squad_id' in x.dataset_name)[0].load_dataset()
  cprint("Bentuk data SQUAD-ID", "cyan")
  print(data_squad_id)
  cprint("Selesai import dataset SQUAD-ID!", "green")

  """# Definisikan tokenizer"""
  cprint("Mulai mendefinisikan tokenizer...", "blue")
  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
  cprint("Selesai mendefinisikan tokenizer!", "green")

  """# Definisikan fungsi pre-processnya"""
  cprint("Mulai mendefinisikan fungsi pre-process...", "blue")
  def rindex(lst, value):
      return len(lst) - operator.indexOf(reversed(lst), value) - 1

  def preprocess_function_qa(examples, tokenizer):
    total_extreme_cases = 0
    examples["question"] = [q.lstrip() for q in examples["question"]]
    examples["context"] = [c.lstrip() for c in examples["context"]]

    tokenized_examples = tokenizer(
        examples['question'],
        examples['context'],
        truncation="only_second",
        max_length = MAX_LENGTH,
        stride=STRIDE,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length"
    )

    tokenized_examples['start_positions'] = []
    tokenized_examples['end_positions'] = []

    for seq_idx in range(len(tokenized_examples['input_ids'])):
      seq_ids = tokenized_examples.sequence_ids(seq_idx)
      offset_mappings = tokenized_examples['offset_mapping'][seq_idx]

      cur_example_idx = tokenized_examples['overflow_to_sample_mapping'][seq_idx]
      
      #answer = examples['answer'][seq_idx][0]
      answer = examples['answer'][cur_example_idx][0]
      answer = eval(answer)
      
      #answer_text = answer['text'][0]
      answer_start = answer['answer_start']
      #answer_end = answer_start + len(answer_text)
      answer_end = answer['answer_end']

      context_pos_start = seq_ids.index(1)
      context_pos_end = rindex(seq_ids, 1)

      s = e = 0
      if (offset_mappings[context_pos_start][0] <= answer_start and
          offset_mappings[context_pos_end][1] >= answer_end):
        i = context_pos_start
        while offset_mappings[i][0] < answer_start:
          i += 1
        if offset_mappings[i][0] == answer_start:
          s = i
        else:
          s = i - 1

        j = context_pos_end
        while offset_mappings[j][1] > answer_end:
          j -= 1      
        if offset_mappings[j][1] == answer_end:
          e = j
        else:
          e = j + 1

      tokenized_examples['start_positions'].append(s)
      tokenized_examples['end_positions'].append(e)
    return tokenized_examples
  cprint("Selesai mendefinisikan fungsi pre-process!", "green")

  """# Mulai tokenisasi dan pre-process"""
  cprint("Mulai tokenisasi dan pre-process...", "blue")
  tokenized_data_squad_id = data_squad_id.map(
    preprocess_function_qa,
    batched=True,
    remove_columns=data_squad_id["train"].column_names,
    num_proc=2,
    fn_kwargs={'tokenizer': tokenizer}
  )

  tokenized_data_squad_id = tokenized_data_squad_id.remove_columns(["offset_mapping", 
                                            "overflow_to_sample_mapping"])

  tokenized_data_squad_train = Dataset.from_dict(tokenized_data_squad_id["train"][:SAMPLE])
  tokenized_data_squad_validation = Dataset.from_dict(tokenized_data_squad_id["validation"][:SAMPLE])
  cprint("Selesai tokenisasi dan pre-process!", "green")

  """# Mendefinisikan argumen (dataops) untuk training nanti"""
  cprint("Mulai mendefinisikan argumen (dataops)...", "blue")
  TIME_NOW = str(datetime.now()).replace(":", "-").replace(" ", "_").replace(".", "_")
  CACHE_DIR = './dataset-qa'
  QA = 'mBERT-without-intermediate'
  DS_TRAIN_DIR = './dataset/qa_train'
  DS_VAL_DIR = './dataset/qa_val'
  CHECKPOINT_DIR = f'./checkpoint-{QA}'
  TENSORBOARD_DIR = './tensorboard-qa'
  MODEL_DIR = f'{QA}-{TIME_NOW}/model/'
  OUTPUT_DIR = f'{QA}-{TIME_NOW}/output/'
  ACCURACY_DIR = f'{QA}-{TIME_NOW}/accuracy/'
  cprint("Selesai mendefinisikan argumen (dataops)!", "green")

  """# Mendefinisikan Training Arguments untuk train"""
  cprint("Mulai mendefinisikan Training Arguments...", "blue")
  training_args_qa = TrainingArguments(
      
      # Checkpoint
      output_dir=CHECKPOINT_DIR,
      save_strategy='epoch',
      save_total_limit=EPOCH,
      
      # Log
      report_to=None,
      logging_dir=TENSORBOARD_DIR,
      logging_strategy='steps',
      logging_first_step=True,
      logging_steps=LOGGING_STEPS,
      
      # Train
      num_train_epochs=EPOCH,
      weight_decay=WEIGHT_DECAY,
      per_device_train_batch_size=BATCH_SIZE,
      gradient_accumulation_steps=GRADIENT_ACCUMULATION,
      learning_rate=LEARNING_RATE,
      warmup_ratio=WARMUP_RATIO,
      bf16=False,
      dataloader_num_workers=cpu_count(),
      
      # Miscellaneous
      evaluation_strategy='epoch',
      seed=SEED,
  )
  cprint("Selesai mendefinisikan Training Arguments!", "green")

  """# Pendefinisian model Question Answering"""
  cprint("Mulai mendefinisikan model Question Answering...", "blue")
  model_qa = BertForQuestionAnswering.from_pretrained(MODEL_NAME)
  cprint("Selesai mendefinisikan model Question Answering!", "green")

  """# Melakukan pengumpulan data dengan padding"""
  cprint("Mulai mengumpulkan data dengan padding...", "blue")
  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
  cprint("Selesai mengumpulkan data dengan padding!", "green")

  """# Mulai training untuk fine-tune SQUAD diatas IndoBERT"""
  cprint("Mulai training QA...", "blue")
  trainer_qa = Trainer(
      model=model_qa,
      args=training_args_qa,
      train_dataset=tokenized_data_squad_train,
      eval_dataset=tokenized_data_squad_validation,
      tokenizer=tokenizer,
      data_collator=data_collator,
  )

  trainer_qa.train()
  cprint("Selesai training QA!", "green")

  """# Menyimpan model Question Answering"""
  cprint("Mulai menyimpan model...", "blue")
  trainer_qa.save_model(MODEL_DIR)
  cprint("Selesai menyimpan model!", "green")

  """# Melakukan prediksi dari model"""
  cprint("Mulai memprediksi dari model...", "blue")
  predict_result = trainer_qa.predict(tokenized_data_squad_validation)

  os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)
  with open(f'{OUTPUT_DIR}/output.txt', "w") as f:
      f.write(predict_result)
      f.close()
  cprint("Selesai memprediksi dari model!", "green")

  """# Melakukan evaluasi dari prediksi"""
  cprint("Mulai mengevaluasi dari prediksi...", "blue")
  def compute_accuracy(predict_result):
    predictions_idx = np.argmax(
        predict_result.predictions, axis=2)
    total_correct = 0
    denominator = len(predictions_idx[0])
    label_array = np.asarray(predict_result.label_ids)

    for i in range(len(predict_result.predictions[0])):
      if predictions_idx[0][i] == label_array[0][i]:
        if predictions_idx[1][i] == label_array[1][i]:
          total_correct += 1

    accuracy = (total_correct / denominator)
    return accuracy

  accuracy_result = compute_accuracy(predict_result)

  os.makedirs(os.path.dirname(ACCURACY_DIR), exist_ok=True)
  with open(f'{ACCURACY_DIR}/accuracy.txt', "w") as f:
      f.write(accuracy_result)
      f.close()
  cprint("Selesai mengevaluasi dari prediksi!", "green")