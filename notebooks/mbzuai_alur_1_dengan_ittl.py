if __name__ == '__main__':    

  # -*- coding: utf-8 -*-
  """[MBZUAI] Alur 1 dengan ITTL.ipynb

  Automatically generated by Colaboratory.

  Original file is located at
      https://colab.research.google.com/drive/1wmB7bFuyFBctKkZHCRZX_7uRpG_VXIKO

  # Alur 1 -- Menjalankan QA Dengan Intermediate Task - Transfer Learning

  # Tahapan Preprocess data IndoNLI

  ## Instalasi setiap module yang digunakan
  """

  """## Import setiap library yang digunakan"""
  from termcolor import colored, cprint
  cprint("Mulai kode QA DENGAN Intermediate Task Transfer Learning", "yellow")
  cprint("Mulai import semua module...", "blue")
  import transformers
  import evaluate
  import torch
  import operator
  import ast
  import json
  import re
  import sys
  import os

  import numpy as np
  import pandas as pd
  import torch.nn as nn

  from multiprocessing import cpu_count
  from evaluate import load
  from nusacrowd import NusantaraConfigHelper
  from datetime import datetime
  from datasets import (
      load_dataset, 
      load_from_disk,
      Dataset
  )
  from transformers import (
      BigBirdTokenizerFast,
      BigBirdForSequenceClassification,
      DataCollatorWithPadding,
      TrainingArguments,
      Trainer,
      BertForSequenceClassification,
      BertForQuestionAnswering,
      AutoModel, 
      BertTokenizerFast,
      AutoTokenizer, 
      AutoModel, 
      BertTokenizer, 
      BertForPreTraining,
      AutoModelForSequenceClassification,
      AutoModelForQuestionAnswering
  )
  cprint("Selesai import semua module!", "green")

  """## Mendefinisikan hyperparameter"""
  cprint("Mulai mendefinisikan hyperparameter...", "blue")
  MODEL_NAME = "indolem/indobert-base-uncased"
  SEED = 42
  EPOCH = 1
  BATCH_SIZE = 16
  GRADIENT_ACCUMULATION = 4
  LEARNING_RATE = 1e-5
  MAX_LENGTH = 400
  STRIDE = 100
  LOGGING_STEPS = 50
  WARMUP_RATIO = 0.06
  WEIGHT_DECAY = 0.01
  # Untuk mempercepat training, saya ubah SAMPLE menjadi 100.
  # Bila mau menggunakan keseluruhan data, gunakan: 
  SAMPLE = sys.maxsize
  # SAMPLE = 100
  cprint("Selesai mendefinisikan hyperparameter!", "green")

  """## Gunakan tokenizer yang sudah pre-trained"""
  cprint("Mulai mendefinisikan tokenizer...", "blue")
  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
  cprint("Selesai mendefinisikan tokenizer!", "green")

  """## Import dataset IndoNLI"""
  cprint("Mulai import dataset IndoNLI...", "blue")
  data_indonli = load_dataset("indonli")
  cprint("Bentuk data IndoNLI", "cyan")
  print(data_indonli)
  cprint("Selesai import dataset IndoNLI!", "green")

  """## Fungsi utilitas untuk pre-process data IndoNLI"""
  cprint("Mulai mendefinisikan fungsi pre-process...", "blue")
  def preprocess_function_indonli(examples, tokenizer):
      return tokenizer(
          examples['premise'], examples['hypothesis'],
          truncation=True, return_token_type_ids=True
      )
  cprint("Selesai mendefinisikan fungsi pre-process!", "green")

  """## Melakukan tokenisasi data IndoNLI"""
  cprint("Mulai tokenisasi dan pre-process...", "blue")
  tokenized_data_indonli = data_indonli.map(
      preprocess_function_indonli,
      batched=True,
      load_from_cache_file=True,
      num_proc=1,
      remove_columns=['premise', 'hypothesis'],
      fn_kwargs={'tokenizer': tokenizer}
  )

  tokenized_data_indonli_train = Dataset.from_dict(tokenized_data_indonli["train"][:SAMPLE])
  tokenized_data_indonli_validation = Dataset.from_dict(tokenized_data_indonli["validation"][:SAMPLE])
  tokenized_data_indonli_test_lay = Dataset.from_dict(tokenized_data_indonli["test_lay"][:SAMPLE])
  tokenized_data_indonli_test_expert = Dataset.from_dict(tokenized_data_indonli["test_expert"][:SAMPLE])
  cprint("Selesai tokenisasi dan pre-process!", "green")

  """# Tahapan fine-tune IndoNLI diatas IndoBERT

  ## Fungsi utilitas untuk komputasi metrik
  """

  def compute_metrics(eval_pred):
      predictions, labels = eval_pred
      predictions = np.argmax(predictions, axis=1)
      return accuracy.compute(
          predictions=predictions, references=labels)

  """## Dictionary untuk mapping label"""

  id2label = {0: 'entailment', 1: 'neutral', 
              2: 'contradiction'}
  label2id = {'entailment': 0, 'neutral': 
              1, 'contradiction': 2}
  accuracy = evaluate.load('accuracy')

  """## Gunakan model Sequence Classification yang sudah pre-trained"""
  cprint("Mulai mendefinisikan model Sequence Classification...", "blue")
  model_sc = BertForSequenceClassification.from_pretrained(
      MODEL_NAME, num_labels=3, 
      id2label=id2label, label2id=label2id)
  cprint("Mulai mendefinisikan model Sequence Classification!", "green")

  """## Melakukan pengumpulan data dengan padding"""
  cprint("Mulai mengumpulkan data dengan padding...", "blue")
  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
  cprint("Selesai mengumpulkan data dengan padding!", "green")

  """## Mendefinisikan argumen (dataops) untuk training nanti"""
  cprint("Mulai mendefinisikan argumen (dataops)...", "blue")
  CACHE_DIR = './dataset-nli'
  NLI_MODEL = 'indobert-indonesian-nli'
  DS_TRAIN_DIR = './dataset/nli_train'
  DS_VAL_DIR = './dataset/nli_val'
  DS_TEST_LAY_DIR = './dataset/nli_test_lay'
  DS_TEST_EXP_DIR = './dataset/nli_test_expert'
  CHECKPOINT_DIR = f'./checkpoint-{NLI_MODEL}'
  TENSORBOARD_DIR = './tensorboard-nli'
  cprint("Selesai mendefinisikan argumen (dataops)!", "green")

  """# Mendefinisikan Training Arguments untuk train"""
  cprint("Mulai mendefinisikan Training Arguments...", "blue")
  training_args_sc = TrainingArguments(
      
      # Checkpoint
      output_dir=CHECKPOINT_DIR,
      save_strategy='epoch',
      save_total_limit=EPOCH,
      
      # Log
      report_to=None,
      logging_dir=TENSORBOARD_DIR,
      logging_strategy='steps',
      logging_first_step=True,
      logging_steps=50,
      
      # Train
      num_train_epochs=EPOCH,
      weight_decay=WEIGHT_DECAY,
      per_device_train_batch_size=BATCH_SIZE,
      gradient_accumulation_steps=GRADIENT_ACCUMULATION,
      learning_rate=LEARNING_RATE,
      warmup_ratio=0.06,
      bf16=False,
      dataloader_num_workers=cpu_count(),
      
      # Miscellaneous
      evaluation_strategy='epoch',
      seed=SEED,
  )
  cprint("Selesai mendefinisikan Training Arguments!", "green")

  """## Mulai training untuk fine-tune IndoNLI diatas IndoBERT"""
  cprint("Mulai training SC...", "blue")
  trainer_sc = Trainer(
      model=model_sc,
      args=training_args_sc,
      train_dataset=tokenized_data_indonli_train,
      eval_dataset=tokenized_data_indonli_validation,
      tokenizer=tokenizer,
      data_collator=data_collator,
      compute_metrics=compute_metrics,
  )

  trainer_sc.train()
  cprint("Selesai training SC!", "green")
  """Di atas, saya coba *epoch* yang sedikit terlebih dahulu karena masih menggunakan *free tier* Google Colab. Kalau mesin sudah mumpuni, baru *epoch*-nya akan ditingkatkan.

  ## Simpan model Sequence Classification
  """
  cprint("Mulai menyimpan model...", "blue")
  trainer_sc.save_model(NLI_MODEL)
  cprint("Selesai menyimpan model!", "green")
  """# Mulai Fine-Tuning SQUAD diatas IndoBERT dengan bobot hasil training Sequence Classification IndoNLI

  ## Gunakan model Question Answering yang sudah pre-trained
  """
  cprint("Mulai mendefinisikan model Question Answering...", "blue")
  model_qa = BertForQuestionAnswering.from_pretrained(MODEL_NAME)
  cprint("Selesai mendefinisikan model Question Answering!", "green")

  """## Gunakan bobot dari hasil training Sequence Classification ke model Question Answering"""
  cprint("Mulai transfer bobot SC ke QA...", "blue")
  filtered_dict = {k: v for k, v in model_sc.state_dict().items() if k in model_qa.state_dict()}
  model_qa.state_dict().update(filtered_dict)

  model_qa.load_state_dict(model_qa.state_dict())
  cprint("Selesai transfer bobot SC ke QA!", "blue")

  """# Import data SQUAD-ID"""
  cprint("Mulai import dataset SQUAD-ID...", "blue")
  conhelps = NusantaraConfigHelper()
  data_squad_id = conhelps.filtered(lambda x: 'squad_id' in x.dataset_name)[0].load_dataset()
  cprint("Bentuk data SQUAD-ID", "cyan")
  print(data_squad_id)
  cprint("Selesai import dataset SQUAD-ID!", "green")

  """## Fungsi utilitas untuk pre-process data SQUAD-ID"""
  cprint("Mulai mendefinisikan fungsi pre-process...", "blue")
  def rindex(lst, value):
      return len(lst) - operator.indexOf(reversed(lst), value) - 1

  def preprocess_function_qa(examples, tokenizer):
    total_extreme_cases = 0
    examples["question"] = [q.lstrip() for q in examples["question"]]
    examples["context"] = [c.lstrip() for c in examples["context"]]

    tokenized_examples = tokenizer(
        examples['question'],
        examples['context'],
        truncation="only_second",
        max_length = MAX_LENGTH,
        stride=STRIDE,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length"
    )

    tokenized_examples['start_positions'] = []
    tokenized_examples['end_positions'] = []

    for seq_idx in range(len(tokenized_examples['input_ids'])):
      seq_ids = tokenized_examples.sequence_ids(seq_idx)
      offset_mappings = tokenized_examples['offset_mapping'][seq_idx]

      cur_example_idx = tokenized_examples['overflow_to_sample_mapping'][seq_idx]
      
      #answer = examples['answer'][seq_idx][0]
      answer = examples['answer'][cur_example_idx][0]
      answer = eval(answer)
      
      #answer_text = answer['text'][0]
      answer_start = answer['answer_start']
      #answer_end = answer_start + len(answer_text)
      answer_end = answer['answer_end']

      context_pos_start = seq_ids.index(1)
      context_pos_end = rindex(seq_ids, 1)

      s = e = 0
      if (offset_mappings[context_pos_start][0] <= answer_start and
          offset_mappings[context_pos_end][1] >= answer_end):
        i = context_pos_start
        while offset_mappings[i][0] < answer_start:
          i += 1
        if offset_mappings[i][0] == answer_start:
          s = i
        else:
          s = i - 1

        j = context_pos_end
        while offset_mappings[j][1] > answer_end:
          j -= 1      
        if offset_mappings[j][1] == answer_end:
          e = j
        else:
          e = j + 1

      tokenized_examples['start_positions'].append(s)
      tokenized_examples['end_positions'].append(e)
    return tokenized_examples
  cprint("Selesai mendefinisikan fungsi pre-process!", "green")

  """## Melakukan tokenisasi data SQUAD-ID"""
  cprint("Mulai tokenisasi dan pre-process...", "blue")
  tokenized_data_squad_id = data_squad_id.map(
    preprocess_function_qa,
    batched=True,
    remove_columns=data_squad_id["train"].column_names,
    num_proc=2,
    fn_kwargs={'tokenizer': tokenizer}
  )

  tokenized_data_squad_id = tokenized_data_squad_id.remove_columns(["offset_mapping", 
                                            "overflow_to_sample_mapping"])

  tokenized_data_squad_train = Dataset.from_dict(tokenized_data_squad_id["train"][:SAMPLE])
  tokenized_data_squad_validation = Dataset.from_dict(tokenized_data_squad_id["validation"][:SAMPLE])
  cprint("Selesai tokenisasi dan pre-process!", "green")

  """## Mendefinisikan argumen (dataops) untuk training nanti"""
  cprint("Mulai mendefinisikan argumen (dataops)...", "blue")
  TIME_NOW = str(datetime.now()).replace(":", "-").replace(" ", "_").replace(".", "_")
  CACHE_DIR = './dataset-qa'
  QA = 'mBERT-with-intermediate'
  DS_TRAIN_DIR = './dataset/qa_train'
  DS_VAL_DIR = './dataset/qa_val'
  CHECKPOINT_DIR = f'./checkpoint-{QA}'
  TENSORBOARD_DIR = './tensorboard-qa'
  MODEL_DIR = f'{QA}-{TIME_NOW}/model/'
  OUTPUT_DIR = f'{QA}-{TIME_NOW}/output/'
  ACCURACY_DIR = f'{QA}-{TIME_NOW}/accuracy/'
  cprint("Selesai mendefinisikan argumen (dataops)!", "green")

  """## Mulai training untuk fine-tune SQUAD-ID diatas IndoBERT dengan bobot hasil training Sequence Classification IndoNLI"""
  cprint("Mulai mendefinisikan Training Arguments...", "blue")
  training_args_qa = TrainingArguments(
      
      # Checkpoint
      output_dir=CHECKPOINT_DIR,
      save_strategy='epoch',
      save_total_limit=EPOCH,
      
      # Log
      report_to=None,
      logging_dir=TENSORBOARD_DIR,
      logging_strategy='steps',
      logging_first_step=True,
      logging_steps=LOGGING_STEPS,
      
      # Train
      num_train_epochs=EPOCH,
      weight_decay=WEIGHT_DECAY,
      per_device_train_batch_size=BATCH_SIZE,
      gradient_accumulation_steps=GRADIENT_ACCUMULATION,
      learning_rate=LEARNING_RATE,
      warmup_ratio=WARMUP_RATIO,
      bf16=False,
      dataloader_num_workers=cpu_count(),
      
      # Miscellaneous
      evaluation_strategy='epoch',
      seed=SEED,
  )
  cprint("Selesai mendefinisikan Training Arguments!", "green")

  """# Mulai training untuk fine-tune SQUAD diatas IndoBERT dengan bobot dari Sequence Classification"""
  cprint("Mulai training QA...", "blue")
  trainer_qa = Trainer(
      model=model_qa,
      args=training_args_qa,
      train_dataset=tokenized_data_squad_train,
      eval_dataset=tokenized_data_squad_validation,
      tokenizer=tokenizer,
      data_collator=data_collator,
  )

  trainer_qa.train()
  cprint("Selesai training QA!", "green")

  """## Simpan model Question Answering"""
  cprint("Mulai menyimpan model...", "blue")
  trainer_qa.save_model(MODEL_DIR)
  cprint("Selesai menyimpan model!", "green")

  """# Memastikan bahwa model Question Answering memiliki keys yang sama dengan model Sequence Classification"""
  cprint("Keys SC:", "cyan")
  print(model_sc.state_dict().keys())
  cprint("Keys QA:", "cyan")
  print(model_qa.state_dict().keys())

  cprint("State SC:", "cyan")
  model_sc.state_dict()
  cprint("State QA:", "cyan")
  model_qa.state_dict()

  """Bobot kedua model di atas pasti berbeda, karena pada model Question Answering dilakukan training tambahan.

  # Melakukan prediksi dari model
  """
  cprint("Mulai memprediksi dari model...", "blue")
  predict_result = trainer_qa.predict(tokenized_data_squad_validation)

  os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)
  with open(f'{OUTPUT_DIR}/output.txt', "w") as f:
      f.write(predict_result)
      f.close()
  cprint("Selesai memprediksi dari model!", "green")

  """# Evaluasi model berdasarkan prediksi"""
  cprint("Mulai mengevaluasi dari prediksi...", "blue")
  def compute_accuracy(predict_result):
    predictions_idx = np.argmax(
        predict_result.predictions, axis=2)
    total_correct = 0
    denominator = len(predictions_idx[0])
    label_array = np.asarray(predict_result.label_ids)

    for i in range(len(predict_result.predictions[0])):
      if predictions_idx[0][i] == label_array[0][i]:
        if predictions_idx[1][i] == label_array[1][i]:
          total_correct += 1

    accuracy = (total_correct / denominator)
    return accuracy

  accuracy_result = compute_accuracy(predict_result)

  os.makedirs(os.path.dirname(ACCURACY_DIR), exist_ok=True)
  with open(f'{ACCURACY_DIR}/accuracy.txt', "w") as f:
      f.write(accuracy_result)
      f.close()
  cprint("Selesai mengevaluasi dari prediksi!", "green")