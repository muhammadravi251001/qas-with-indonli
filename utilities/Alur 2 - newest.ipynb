{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhsDF_5UdqLI"
   },
   "source": [
    "# Menjalankan QA Tanpa Intermediate Task - Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKm8qASDXqR-"
   },
   "source": [
    "# Import semua module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DH9DifnXp5_",
    "outputId": "94161dcf-e635-486d-c6d8-5747b267b64f"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install transformers\n",
    "#!pip install tensorboard\n",
    "#!pip install evaluate\n",
    "#!pip install git+https://github.com/IndoNLP/nusa-crowd.git@release_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nusacrowd@ git+https://github.com/IndoNLP/nusa-crowd.git@7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Cloning https://github.com/IndoNLP/nusa-crowd.git (to revision 7748513d20331e72f9969f94f5d43c7f2d4a59a5) to /tmp/pip-install-c5qwaofy/nusacrowd_724a7e3599f64750b631912b79a155b8\n",
      "  Running command git clone --filter=blob:none -q https://github.com/IndoNLP/nusa-crowd.git /tmp/pip-install-c5qwaofy/nusacrowd_724a7e3599f64750b631912b79a155b8\n",
      "  Running command git rev-parse -q --verify 'sha^7748513d20331e72f9969f94f5d43c7f2d4a59a5'\n",
      "  Running command git fetch -q https://github.com/IndoNLP/nusa-crowd.git 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Running command git checkout -q 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Resolved https://github.com/IndoNLP/nusa-crowd.git to commit 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: aiohttp==3.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (3.8.1)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: appdirs==1.4.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: async-timeout==4.0.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (4.0.2)\n",
      "Requirement already satisfied: attrs==22.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (22.2.0)\n",
      "Requirement already satisfied: audioread==3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: bioc==1.3.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (1.3.7)\n",
      "Requirement already satisfied: black==22.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (22.12.0)\n",
      "Requirement already satisfied: cachetools==5.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (5.3.0)\n",
      "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (2022.12.7)\n",
      "Requirement already satisfied: cffi==1.15.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (1.15.1)\n",
      "Requirement already satisfied: cfgv==3.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (3.3.1)\n",
      "Requirement already satisfied: charset-normalizer==2.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 14)) (2.1.1)\n",
      "Requirement already satisfied: click==8.1.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (8.1.3)\n",
      "Requirement already satisfied: colorama==0.4.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: conllu==4.5.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 17)) (4.5.2)\n",
      "Requirement already satisfied: datasets==2.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 18)) (2.2.0)\n",
      "Requirement already satisfied: decorator==5.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 19)) (5.1.1)\n",
      "Requirement already satisfied: dill==0.3.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 20)) (0.3.6)\n",
      "Requirement already satisfied: distlib==0.3.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 21)) (0.3.6)\n",
      "Requirement already satisfied: docutils==0.19 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 22)) (0.19)\n",
      "Requirement already satisfied: et-xmlfile==1.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 23)) (1.1.0)\n",
      "Requirement already satisfied: evaluate==0.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 24)) (0.4.0)\n",
      "Requirement already satisfied: ffmpeg==1.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 25)) (1.4)\n",
      "Requirement already satisfied: filelock==3.9.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 26)) (3.9.0)\n",
      "Requirement already satisfied: flake8==6.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 27)) (6.0.0)\n",
      "Requirement already satisfied: frozenlist==1.3.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 28)) (1.3.3)\n",
      "Requirement already satisfied: fsspec==2023.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 29)) (2023.1.0)\n",
      "Requirement already satisfied: git-lfs==1.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 30)) (1.6)\n",
      "Requirement already satisfied: google-auth==2.16.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 31)) (2.16.1)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 32)) (0.4.6)\n",
      "Requirement already satisfied: grpcio==1.51.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 33)) (1.51.1)\n",
      "Requirement already satisfied: huggingface-hub==0.12.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 34)) (0.12.1)\n",
      "Requirement already satisfied: identify==2.5.18 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 35)) (2.5.18)\n",
      "Requirement already satisfied: idna==3.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 36)) (3.4)\n",
      "Requirement already satisfied: importlib-metadata==6.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 37)) (6.0.0)\n",
      "Requirement already satisfied: isort==5.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 38)) (5.12.0)\n",
      "Requirement already satisfied: joblib==1.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 39)) (1.2.0)\n",
      "Requirement already satisfied: jsonlines==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 40)) (3.1.0)\n",
      "Requirement already satisfied: librosa==0.9.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 41)) (0.9.2)\n",
      "Requirement already satisfied: llvmlite==0.39.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 42)) (0.39.1)\n",
      "Requirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 43)) (0.5.3)\n",
      "Requirement already satisfied: lxml==4.9.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 44)) (4.9.2)\n",
      "Requirement already satisfied: Markdown==3.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 45)) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe==2.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 46)) (2.1.2)\n",
      "Requirement already satisfied: mccabe==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 47)) (0.7.0)\n",
      "Requirement already satisfied: multidict==6.0.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 48)) (6.0.4)\n",
      "Requirement already satisfied: multiprocess==0.70.14 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 49)) (0.70.14)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 50)) (1.0.0)\n",
      "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 51)) (3.8.1)\n",
      "Requirement already satisfied: nodeenv==1.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 52)) (1.7.0)\n",
      "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 53)) (0.56.4)\n",
      "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 54)) (1.23.5)\n",
      "Requirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 56)) (3.2.2)\n",
      "Requirement already satisfied: openpyxl==3.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 57)) (3.1.1)\n",
      "Requirement already satisfied: packaging==23.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 58)) (23.0)\n",
      "Requirement already satisfied: pandas==1.3.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 59)) (1.3.3)\n",
      "Requirement already satisfied: pathspec==0.11.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 60)) (0.11.0)\n",
      "Requirement already satisfied: platformdirs==3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 61)) (3.0.0)\n",
      "Requirement already satisfied: pooch==1.6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 62)) (1.6.0)\n",
      "Requirement already satisfied: pre-commit==2.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 63)) (2.19.0)\n",
      "Requirement already satisfied: protobuf==4.22.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 64)) (4.22.0)\n",
      "Requirement already satisfied: pyarrow==11.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 65)) (11.0.0)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 66)) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 67)) (0.2.8)\n",
      "Requirement already satisfied: pycodestyle==2.10.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 68)) (2.10.0)\n",
      "Requirement already satisfied: pycparser==2.21 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 69)) (2.21)\n",
      "Requirement already satisfied: pyflakes==3.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 70)) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 71)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2022.7.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 72)) (2022.7.1)\n",
      "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 73)) (6.0)\n",
      "Requirement already satisfied: regex==2022.10.31 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 74)) (2022.10.31)\n",
      "Requirement already satisfied: requests==2.28.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 75)) (2.28.2)\n",
      "Requirement already satisfied: requests-oauthlib==1.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 76)) (1.3.1)\n",
      "Requirement already satisfied: resampy==0.4.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 77)) (0.4.2)\n",
      "Requirement already satisfied: responses==0.18.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 78)) (0.18.0)\n",
      "Requirement already satisfied: rsa==4.9 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 79)) (4.9)\n",
      "Requirement already satisfied: scikit-learn==1.2.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 80)) (1.2.1)\n",
      "Requirement already satisfied: scipy==1.10.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 81)) (1.10.0)\n",
      "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 82)) (1.16.0)\n",
      "Requirement already satisfied: soundfile==0.12.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 83)) (0.12.1)\n",
      "Requirement already satisfied: tensorboard==2.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 84)) (2.12.0)\n",
      "Requirement already satisfied: tensorboard-data-server==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 85)) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 86)) (1.8.1)\n",
      "Requirement already satisfied: termcolor==2.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 87)) (2.2.0)\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 88)) (3.1.0)\n",
      "Requirement already satisfied: tokenizers==0.13.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 89)) (0.13.2)\n",
      "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 90)) (0.10.2)\n",
      "Requirement already satisfied: tomli==2.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 91)) (2.0.1)\n",
      "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 92)) (1.13.1)\n",
      "Requirement already satisfied: torchaudio==0.13.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 93)) (0.13.1)\n",
      "Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 94)) (4.64.1)\n",
      "Requirement already satisfied: transformers==4.26.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 95)) (4.26.1)\n",
      "Requirement already satisfied: translate-toolkit==3.8.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 96)) (3.8.4)\n",
      "Requirement already satisfied: typing_extensions==4.5.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 97)) (4.5.0)\n",
      "Requirement already satisfied: urllib3==1.26.14 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 98)) (1.26.14)\n",
      "Requirement already satisfied: virtualenv==20.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 99)) (20.19.0)\n",
      "Requirement already satisfied: Werkzeug==2.2.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 100)) (2.2.3)\n",
      "Requirement already satisfied: win32-setctime==1.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 101)) (1.1.0)\n",
      "Requirement already satisfied: xxhash==3.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 102)) (3.2.0)\n",
      "Requirement already satisfied: yarl==1.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 103)) (1.8.2)\n",
      "Requirement already satisfied: zipp==3.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 104)) (3.14.0)\n",
      "Requirement already satisfied: zstandard==0.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 105)) (0.19.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nodeenv==1.7.0->-r requirements.txt (line 52)) (45.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard==2.12.0->-r requirements.txt (line 84)) (0.34.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.7.99)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 10 12:53:56 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    44W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    44W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    45W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    43W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    60W / 300W |  10310MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    58W / 300W |   8628MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    44W / 300W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    56W / 300W |  12914MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Melihat GPU yang tersedia dan penggunaannya.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memilih GPU yang akan digunakan (contohnya: GPU #7)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pgY_FL4lXuEu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "import operator\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from evaluate import load\n",
    "from nusacrowd import NusantaraConfigHelper\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from huggingface_hub import notebook_login\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    load_from_disk,\n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "from transformers import (\n",
    "    BigBirdTokenizerFast,\n",
    "    BigBirdForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BertForSequenceClassification,\n",
    "    BertForQuestionAnswering,\n",
    "    AutoModel, \n",
    "    BertTokenizerFast,\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    BertTokenizer, \n",
    "    BertForPreTraining,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    EvalPrediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtu0yFqMYsX9"
   },
   "source": [
    "# Definisikan hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gjHJwpxeYugs"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"indolem/indobert-base-uncased\"\n",
    "#MODEL_NAME = \"afaji/fine-tuned-IndoNLI-Translated-with-indobert-base-uncased\"\n",
    "SEED = 42\n",
    "EPOCH = 1\n",
    "BATCH_SIZE = 32\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_LENGTH = 400\n",
    "STRIDE = 100\n",
    "LOGGING_STEPS = 50\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01\n",
    "# Untuk mempercepat training, saya ubah SAMPLE menjadi 100.\n",
    "# Bila mau menggunakan keseluruhan data, gunakan: \n",
    "SAMPLE = sys.maxsize\n",
    "# SAMPLE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyH7dlPGYOSk"
   },
   "source": [
    "# Import dataset QAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/IndoNLP/nusa-crowd.git\n",
      "  Cloning https://github.com/IndoNLP/nusa-crowd.git to /tmp/pip-req-build-t8nklg5v\n",
      "  Running command git clone --filter=blob:none -q https://github.com/IndoNLP/nusa-crowd.git /tmp/pip-req-build-t8nklg5v\n",
      "  Resolved https://github.com/IndoNLP/nusa-crowd.git to commit bea0aedb653c65d2dbc65a5f7b6950bd2cad274d\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: aiohttp==3.8.1 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (3.8.1)\n",
      "Requirement already satisfied: bioc==1.3.7 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (1.3.7)\n",
      "Requirement already satisfied: black~=22.0 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (22.12.0)\n",
      "Requirement already satisfied: conllu in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (4.5.2)\n",
      "Requirement already satisfied: datasets==2.2.0 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (2.2.0)\n",
      "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (1.4)\n",
      "Requirement already satisfied: flake8>=3.8.3 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (6.0.0)\n",
      "Requirement already satisfied: isort>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (5.12.0)\n",
      "Requirement already satisfied: jsonlines>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (3.1.0)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (0.9.2)\n",
      "Requirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (0.5.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (1.23.5)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (3.1.1)\n",
      "Requirement already satisfied: pandas==1.3.3 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (1.3.3)\n",
      "Requirement already satisfied: pre-commit==2.19.0 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (2.19.0)\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (0.12.1)\n",
      "Requirement already satisfied: torchaudio>=0.11 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (0.13.1)\n",
      "Requirement already satisfied: translate-toolkit>=3.7.3 in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (3.8.4)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (4.5.0)\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.8/dist-packages (from nusacrowd==0.0.1) (0.19.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.8.1->nusacrowd==0.0.1) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.8.1->nusacrowd==0.0.1) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.8.1->nusacrowd==0.0.1) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.8.1->nusacrowd==0.0.1) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.8.1->nusacrowd==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.8.1->nusacrowd==0.0.1) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.8.1->nusacrowd==0.0.1) (6.0.4)\n",
      "Requirement already satisfied: docutils>=0.15.2 in /usr/local/lib/python3.8/dist-packages (from bioc==1.3.7->nusacrowd==0.0.1) (0.19)\n",
      "Requirement already satisfied: lxml>=4.6.3 in /usr/local/lib/python3.8/dist-packages (from bioc==1.3.7->nusacrowd==0.0.1) (4.9.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.2.0->nusacrowd==0.0.1) (0.12.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets==2.2.0->nusacrowd==0.0.1) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.2.0->nusacrowd==0.0.1) (2023.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.2.0->nusacrowd==0.0.1) (2.28.2)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets==2.2.0->nusacrowd==0.0.1) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.2.0->nusacrowd==0.0.1) (11.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.2.0->nusacrowd==0.0.1) (4.64.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from datasets==2.2.0->nusacrowd==0.0.1) (0.3.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets==2.2.0->nusacrowd==0.0.1) (23.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets==2.2.0->nusacrowd==0.0.1) (0.70.14)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.3->nusacrowd==0.0.1) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.3->nusacrowd==0.0.1) (2.8.2)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from pre-commit==2.19.0->nusacrowd==0.0.1) (0.10.2)\n",
      "Requirement already satisfied: virtualenv>=20.0.8 in /usr/local/lib/python3.8/dist-packages (from pre-commit==2.19.0->nusacrowd==0.0.1) (20.19.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pre-commit==2.19.0->nusacrowd==0.0.1) (2.5.18)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from pre-commit==2.19.0->nusacrowd==0.0.1) (1.7.0)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from pre-commit==2.19.0->nusacrowd==0.0.1) (3.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from pre-commit==2.19.0->nusacrowd==0.0.1) (6.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from black~=22.0->nusacrowd==0.0.1) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from black~=22.0->nusacrowd==0.0.1) (0.11.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from black~=22.0->nusacrowd==0.0.1) (8.1.3)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from black~=22.0->nusacrowd==0.0.1) (2.0.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.8/dist-packages (from black~=22.0->nusacrowd==0.0.1) (3.0.0)\n",
      "Requirement already satisfied: pyflakes<3.1.0,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from flake8>=3.8.3->nusacrowd==0.0.1) (3.0.1)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from flake8>=3.8.3->nusacrowd==0.0.1) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.11.0,>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from flake8>=3.8.3->nusacrowd==0.0.1) (2.10.0)\n",
      "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchaudio>=0.11->nusacrowd==0.0.1) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchaudio>=0.11->nusacrowd==0.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchaudio>=0.11->nusacrowd==0.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchaudio>=0.11->nusacrowd==0.0.1) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchaudio>=0.11->nusacrowd==0.0.1) (11.10.3.66)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchaudio>=0.11->nusacrowd==0.0.1) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchaudio>=0.11->nusacrowd==0.0.1) (45.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from librosa->nusacrowd==0.0.1) (1.2.1)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.8/dist-packages (from librosa->nusacrowd==0.0.1) (5.1.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.8/dist-packages (from librosa->nusacrowd==0.0.1) (1.2.0)\n",
      "Requirement already satisfied: numba>=0.45.1 in /usr/local/lib/python3.8/dist-packages (from librosa->nusacrowd==0.0.1) (0.56.4)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from librosa->nusacrowd==0.0.1) (0.4.2)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa->nusacrowd==0.0.1) (1.6.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.8/dist-packages (from librosa->nusacrowd==0.0.1) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from librosa->nusacrowd==0.0.1) (1.10.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile->nusacrowd==0.0.1) (1.15.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->nusacrowd==0.0.1) (2022.10.31)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl->nusacrowd==0.0.1) (1.1.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile->nusacrowd==0.0.1) (2.21)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.2.0->nusacrowd==0.0.1) (3.9.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.45.1->librosa->nusacrowd==0.0.1) (0.39.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.45.1->librosa->nusacrowd==0.0.1) (6.0.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa->nusacrowd==0.0.1) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.3->nusacrowd==0.0.1) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.2.0->nusacrowd==0.0.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.2.0->nusacrowd==0.0.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.2.0->nusacrowd==0.0.1) (2022.12.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.19.1->librosa->nusacrowd==0.0.1) (3.1.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from virtualenv>=20.0.8->pre-commit==2.19.0->nusacrowd==0.0.1) (0.3.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.45.1->librosa->nusacrowd==0.0.1) (3.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/IndoNLP/nusa-crowd.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlNhPlmDY2tk"
   },
   "source": [
    "# Definisikan tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "z3uXWOkUY4GD"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYYbFXzWYTr3"
   },
   "source": [
    "# Definisikan fungsi pre-processnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset ty_di_qa_id_dataset (/root/.cache/huggingface/datasets/ty_di_qa_id_dataset/tydiqa_id_source/1.0.0/77be91de4a88147f2b8ff9e6c1d376d01d26f9b51dd36f775a5607518c034111)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798f802056f6484a93f99f37a7a69d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conhelps = NusantaraConfigHelper()\n",
    "data_qas_id = conhelps.filtered(lambda x: 'tydiqa_id' in x.dataset_name)[0].load_dataset()\n",
    "\n",
    "df_train = pd.DataFrame(data_qas_id['train'])\n",
    "df_validation = pd.DataFrame(data_qas_id['validation'])\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_train = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in range(len(df_train['context'])):\n",
    "    answer_start = df_train['context'][i].index(df_train['label'][i])\n",
    "    answer_end = answer_start + len(df_train['label'][i])\n",
    "    new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                        'question': df_train[\"question\"][i], \n",
    "                                        'answer': {\"text\": df_train[\"label\"][i], \n",
    "                                                   \"answer_start\": answer_start, \n",
    "                                                   \"answer_end\": answer_end}}, \n",
    "                                                   ignore_index=True)\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_val = pd.DataFrame(columns=cols)    \n",
    "\n",
    "for i in range(len(df_validation['context'])):\n",
    "    answer_start = df_validation['context'][i].index(df_validation['label'][i])\n",
    "    answer_end = answer_start + len(df_validation['label'][i])\n",
    "    new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                    'question': df_validation[\"question\"][i], \n",
    "                                    'answer': {\"text\": df_validation[\"label\"][i], \n",
    "                                               \"answer_start\": answer_start, \n",
    "                                               \"answer_end\": answer_end}}, \n",
    "                                               ignore_index=True)    \n",
    "\n",
    "train_dataset = Dataset.from_dict(new_df_train)\n",
    "validation_dataset = Dataset.from_dict(new_df_val)\n",
    "\n",
    "data_qas_id = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EuSjYelvXJ9x"
   },
   "outputs": [],
   "source": [
    "def rindex(lst, value, operator=operator):\n",
    "      return len(lst) - operator.indexOf(reversed(lst), value) - 1\n",
    "\n",
    "def preprocess_function_qa(examples, tokenizer, MAX_LENGTH=MAX_LENGTH, STRIDE=STRIDE, rindex=rindex, operator=operator):\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    examples[\"context\"] = [c.lstrip() for c in examples[\"context\"]]\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "      examples['question'],\n",
    "      examples['context'],\n",
    "      truncation=True,\n",
    "      max_length = MAX_LENGTH,\n",
    "      stride=STRIDE,\n",
    "      return_overflowing_tokens=True,\n",
    "      return_offsets_mapping=True,\n",
    "      padding=\"max_length\",\n",
    "      return_tensors='np'\n",
    "    )\n",
    "\n",
    "    tokenized_examples['start_positions'] = []\n",
    "    tokenized_examples['end_positions'] = []\n",
    "\n",
    "    for seq_idx in range(len(tokenized_examples['input_ids'])):\n",
    "        seq_ids = tokenized_examples.sequence_ids(seq_idx)\n",
    "        offset_mappings = tokenized_examples['offset_mapping'][seq_idx]\n",
    "\n",
    "        cur_example_idx = tokenized_examples['overflow_to_sample_mapping'][seq_idx]\n",
    "\n",
    "        #answer = examples['answer'][seq_idx][0]\n",
    "        answer = examples['answer'][cur_example_idx]\n",
    "        answer = eval(str(answer))\n",
    "        #answer_text = answer['text'][0]\n",
    "        answer_start = answer['answer_start']\n",
    "        #answer_end = answer_start + len(answer_text)\n",
    "        answer_end = answer['answer_end']\n",
    "\n",
    "        context_pos_start = seq_ids.index(1)\n",
    "        context_pos_end = rindex(seq_ids, 1, operator)\n",
    "\n",
    "        s = e = 0\n",
    "        if (offset_mappings[context_pos_start][0] <= answer_start and\n",
    "            offset_mappings[context_pos_end][1] >= answer_end):\n",
    "          i = context_pos_start\n",
    "          while offset_mappings[i][0] < answer_start:\n",
    "            i += 1\n",
    "          if offset_mappings[i][0] == answer_start:\n",
    "            s = i\n",
    "          else:\n",
    "            s = i - 1\n",
    "\n",
    "          j = context_pos_end\n",
    "          while offset_mappings[j][1] > answer_end:\n",
    "            j -= 1      \n",
    "          if offset_mappings[j][1] == answer_end:\n",
    "            e = j\n",
    "          else:\n",
    "            e = j + 1\n",
    "\n",
    "        tokenized_examples['start_positions'].append(s)\n",
    "        tokenized_examples['end_positions'].append(e)\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0o9vkQ1YaO2"
   },
   "source": [
    "# Mulai tokenisasi dan pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "2721b15477d741519a3041a8f256575b",
      "6a373f9cb48a481d85803f0792ff27b3",
      "54a2637e8c8e4ca0917e35f4b58b48f3",
      "7dc3dc97044c48afb06a14575fc8e91b",
      "6c3c7e4da5af40b088f3046d7698edff",
      "a040ab99cb964c32a0a968d97dd55a16",
      "9ef8c0b24cb5490b8fa7e8ea8ad619fe",
      "dce089e985634a67a381bc85cbaca017",
      "bd9cb96d5a3c4400bb7d145590540a54",
      "f940438113734ccdb198efd12b8a3936",
      "c5b567cd34b340dea77cefd40e9aaec9",
      "6516c5b02aa24db2bc42cdd4d1f2c260",
      "cbed771e77314789bf8817404e5d6f67",
      "b0be384a887847e981868df7c551d3de",
      "10fb50db270449b48382c815904245a8",
      "b23174d38e3940718252770bd39177f8",
      "7b80b681b46b4671a14a7cf82fd50991",
      "bce02c83fbc142c5bb2d8c1f905a478d",
      "12c3fd408f914038acb511c4ef57f965",
      "540c7fcbf3b14e538cba18329bc1fc20",
      "fc7073fb093543fc990abc620348c1ca",
      "acac4d071be4488fbb272df63b625213",
      "29b13cbb638947d5a17071d299e2420f",
      "113ae282ca5a499a8a2ad36a796e763f",
      "0ce874e529474f478a7acd45b4ffd649",
      "935752d8733c41fbb25ed436606352ca",
      "cf2b519cc5ff446dae45afa2439c840e",
      "f10632530a1b499ba3d20f0286446bab",
      "94da3d6dfccc4a289c659b94bcc90658",
      "902bd6c12ea54039b8c913a7c1782ff4",
      "1d86de7f1fbf452892fe6d4b177558f9",
      "3f6183fb8ca348e0ac35b495681c705a",
      "bdf2cb9d10cc4b2aab466a9dfe7be5bc",
      "c069967001a542f5b0c3b88ff1841eba",
      "f46e4c24b9294ffdb25e1e64bc594eee",
      "14a450c5594148c3a4ca340521d167ba",
      "9c9b27a0b2ac4be3b8dcbb65e23b84a2",
      "c99472b37d644d63a471b65ecbe3bfbd",
      "e9ca9f63982744aba53e883882373160",
      "4f80ae88d884407ab8f9e08aa58632a4",
      "13ea541bb13b42738e74885a2c1db8df",
      "68818d7eea5849afa82c3003a57a5b8d",
      "065d96b02c7840019af97e3c135693a8",
      "45f8bfb31705441c80a22dbb955dd933"
     ]
    },
    "id": "uVGfobd8XNIF",
    "outputId": "9b911f15-dce5-456f-a0fd-446f9615be1c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'fn_kwargs'={'tokenizer': BertTokenizerFast(name_or_path='indolem/indobert-base-uncased', vocab_size=31923, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), 'MAX_LENGTH': 400, 'STRIDE': 100, 'rindex': <function rindex at 0x7f60739b6ee0>, 'operator': <module 'operator' from '/usr/lib/python3.8/operator.py'>} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3759c68f1b564d60af9296d0720b32b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bead01449afe45cea609cdb2815933fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data_qas_id = data_qas_id.map(\n",
    "    preprocess_function_qa,\n",
    "    batched=True,\n",
    "    remove_columns=data_qas_id['train'].column_names,\n",
    "    num_proc=1,\n",
    "    fn_kwargs={'tokenizer': tokenizer, 'MAX_LENGTH': MAX_LENGTH, 'STRIDE': STRIDE, 'rindex': rindex, 'operator': operator}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_qas_id = tokenized_data_qas_id.remove_columns([\"offset_mapping\", \n",
    "                                            \"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Fk1RbGEeldvi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_data_qas_id.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\"], output_all_columns=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SLfcGWQEjd36"
   },
   "outputs": [],
   "source": [
    "tokenized_data_qas_id_train = Dataset.from_dict(tokenized_data_qas_id[\"train\"][:SAMPLE])\n",
    "tokenized_data_qas_id_validation = Dataset.from_dict(tokenized_data_qas_id[\"validation\"][:SAMPLE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06_Y5Xw9bLj3"
   },
   "source": [
    "# Mendefinisikan argumen (dataops) untuk training nanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "foTQgHuujf46"
   },
   "outputs": [],
   "source": [
    "TIME_NOW = str(datetime.now()).replace(\":\", \"-\").replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "QA = './results/alur2-idk-mrc'\n",
    "CHECKPOINT_DIR = f'{QA}-{TIME_NOW}/checkpoint/'\n",
    "MODEL_DIR = f'{QA}-{TIME_NOW}/model/'\n",
    "OUTPUT_DIR = f'{QA}-{TIME_NOW}/output/'\n",
    "ACCURACY_DIR = f'{QA}-{TIME_NOW}/accuracy/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9YdVKNRbPHk"
   },
   "source": [
    "# Mendefinisikan Training Arguments untuk train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3VMs4pBjgXE",
    "outputId": "577270d3-fe73-44be-f1c2-e710bb5e1d1b"
   },
   "outputs": [],
   "source": [
    "training_args_qa = TrainingArguments(\n",
    "    \n",
    "    # Checkpoint\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=EPOCH,\n",
    "    \n",
    "    # Log\n",
    "    report_to='tensorboard',\n",
    "    logging_strategy='steps',\n",
    "    logging_first_step=True,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    \n",
    "    # Train\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    bf16=False,\n",
    "    dataloader_num_workers=cpu_count(),\n",
    "    \n",
    "    # Miscellaneous\n",
    "    evaluation_strategy='epoch',\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INibCqT5bR3d"
   },
   "source": [
    "# Pendefinisian model Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqTEhdqqtL4z",
    "outputId": "5b4394f3-63f5-4610-e72d-d383d9b8940a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_qa = BertForQuestionAnswering.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qa = model_qa.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyJvhxZ6bbbF"
   },
   "source": [
    "# Melakukan pengumpulan data dengan padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rGeJNonStSXT"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nF_E_CHbb8m"
   },
   "source": [
    "# Mulai training untuk fine-tune SQUAD diatas IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import collections\n",
    "\n",
    "# # Melakukan evaluasi dari prediksi\n",
    "def normalize_text(s):\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_f1_prec_rec(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split() # True positive + False positive = Untuk precision\n",
    "    gold_tokens = normalize_text(gold).split() # True positive + False negatives = Untuk recall\n",
    "    common = collections.Counter(pred_tokens) & collections.Counter(gold_tokens)\n",
    "    num_same = sum(common.values()) # True positive\n",
    "\n",
    "    if len(gold_tokens) == 0 or len(pred_tokens) == 0: \n",
    "        var = int(gold_tokens == pred_tokens)\n",
    "        return var, var, var\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(pred_tokens)\n",
    "    recall = 1.0 * num_same / len(gold_tokens)\n",
    "    f1 = (2.0 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1, precision, recall\n",
    "\n",
    "def compute_metrics(predict_result):\n",
    "    predictions_idx = np.argmax(predict_result.predictions, axis=2)\n",
    "    denominator = len(predictions_idx[0])\n",
    "    label_array = np.asarray(predict_result.label_ids)\n",
    "    total_correct = 0\n",
    "    f1_array = []\n",
    "    precision_array = []\n",
    "    recall_array = []\n",
    "\n",
    "    for i in range(len(predict_result.predictions[0])):\n",
    "        start_pred_idx = predictions_idx[0][i]\n",
    "        end_pred_idx = predictions_idx[1][i] + 1\n",
    "        start_gold_idx = label_array[0][i]\n",
    "        end_gold_idx = label_array[1][i] + 1\n",
    "\n",
    "        pred_text = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                    [start_pred_idx: end_pred_idx])\n",
    "        gold_text = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                    [start_gold_idx: end_gold_idx])\n",
    "\n",
    "        if pred_text == gold_text:\n",
    "            total_correct += 1\n",
    "\n",
    "        f1, precision, recall = compute_f1_prec_rec(pred=pred_text, gold=gold_text)\n",
    "\n",
    "        f1_array.append(f1)\n",
    "        precision_array.append(precision)\n",
    "        recall_array.append(recall)\n",
    "\n",
    "    exact_match = ((total_correct / denominator) * 100.0)\n",
    "    final_f1 = np.mean(f1_array) * 100.0\n",
    "    final_precision = np.mean(precision_array) * 100.0\n",
    "    final_recall = np.mean(recall_array) * 100.0\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': final_f1, 'precision': final_precision, 'recall': final_recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "G6pLmNzCjhra"
   },
   "outputs": [],
   "source": [
    "trainer_qa = Trainer(\n",
    "    model=model_qa.to(device),\n",
    "    args=training_args_qa,\n",
    "    train_dataset=tokenized_data_qas_id_train,\n",
    "    eval_dataset=tokenized_data_qas_id_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qp96lB8vjtXG",
    "outputId": "b118d7ac-a90d-43c2-bd8b-ee0a0c4ab8de",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4910\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 38\n",
      "  Number of trainable parameters = 109969154\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 01:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.127000</td>\n",
       "      <td>3.779974</td>\n",
       "      <td>9.424084</td>\n",
       "      <td>20.391822</td>\n",
       "      <td>22.270491</td>\n",
       "      <td>32.261107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 573\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/alur2-idk-mrc-2023-03-10_12-54-33_531594/checkpoint/checkpoint-38\n",
      "Configuration saved in ./results/alur2-idk-mrc-2023-03-10_12-54-33_531594/checkpoint/checkpoint-38/config.json\n",
      "Model weights saved in ./results/alur2-idk-mrc-2023-03-10_12-54-33_531594/checkpoint/checkpoint-38/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/alur2-idk-mrc-2023-03-10_12-54-33_531594/checkpoint/checkpoint-38/tokenizer_config.json\n",
      "Special tokens file saved in ./results/alur2-idk-mrc-2023-03-10_12-54-33_531594/checkpoint/checkpoint-38/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=38, training_loss=4.7019881449247665, metrics={'train_runtime': 120.536, 'train_samples_per_second': 40.735, 'train_steps_per_second': 0.315, 'total_flos': 992927675596800.0, 'train_loss': 4.7019881449247665, 'epoch': 0.99})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_qa.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DreIeglDbrHY"
   },
   "source": [
    "# Menyimpan model Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KJick7YDjuoq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/alur2-idk-mrc-2023-03-10_12-54-33_531594/model/\n",
      "Configuration saved in ./results/alur2-idk-mrc-2023-03-10_12-54-33_531594/model/config.json\n",
      "Model weights saved in ./results/alur2-idk-mrc-2023-03-10_12-54-33_531594/model/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/alur2-idk-mrc-2023-03-10_12-54-33_531594/model/tokenizer_config.json\n",
      "Special tokens file saved in ./results/alur2-idk-mrc-2023-03-10_12-54-33_531594/model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer_qa.save_model(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX-y8EbobugF"
   },
   "source": [
    "# Melakukan prediksi dari model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6aE7w2yMkWxj",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 573\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=(array([[-3.8467932, -1.2784417, -2.4225533, ..., -2.2612486, -2.7080827,\n",
       "        -3.6240296],\n",
       "       [-3.8943279, -0.9453649, -1.4320273, ..., -3.5967166, -2.8518643,\n",
       "        -2.3382006],\n",
       "       [-4.392624 , -2.2868369, -1.7495764, ..., -4.4628854, -4.2939157,\n",
       "        -4.334184 ],\n",
       "       ...,\n",
       "       [-4.1014056, -2.1758704, -1.1351863, ..., -3.4700184, -2.972008 ,\n",
       "        -2.7946863],\n",
       "       [-3.7165089, -1.8033885, -1.3102052, ..., -3.1343584, -3.0160666,\n",
       "        -3.2191167],\n",
       "       [-4.2181516, -1.2046374, -1.8890511, ..., -3.9477227, -3.885199 ,\n",
       "        -3.2214365]], dtype=float32), array([[-3.873902  , -1.258944  , -2.1837177 , ..., -2.433065  ,\n",
       "        -2.3998175 , -3.9508228 ],\n",
       "       [-3.2369387 , -1.4576739 , -1.3374946 , ..., -3.0182939 ,\n",
       "        -2.5944395 , -2.5743082 ],\n",
       "       [-3.7886198 , -1.3743143 , -1.4331965 , ..., -3.6360903 ,\n",
       "        -3.9793787 , -3.94315   ],\n",
       "       ...,\n",
       "       [-3.410995  , -2.028439  , -1.1142989 , ..., -3.6565647 ,\n",
       "        -3.5840342 , -3.5937684 ],\n",
       "       [-3.1131666 , -1.2874823 , -0.12368764, ..., -3.3176003 ,\n",
       "        -3.168652  , -3.2878966 ],\n",
       "       [-3.8237817 , -1.2075639 , -1.262999  , ..., -3.5346425 ,\n",
       "        -3.3653069 , -3.6361005 ]], dtype=float32)), label_ids=(array([ 43,  21,  18,  26,  43,  22,  11,  30,  73,  54,  14,  17,  12,\n",
       "        39,  48, 178,  47,  14,  38,  25,   8,  42,  56,  20,  21,  23,\n",
       "        46, 101,  11,  21,  78,  29,  15,  15,  11,  36,  19,  21,  12,\n",
       "        78,  13,  37,  11,  43,  21,  34,  38,   9,  11,   8,  53,  24,\n",
       "        35,  15,  14,  15,   8, 222,  20,  24, 125,  25,  13,  29,  24,\n",
       "        28, 110,  89,  16,  23,  38,  17,  10, 230,  19, 223,  26,  72,\n",
       "        44,  31,  33,  19,  41,  27,  91,  26,  30,  13, 151,  18,  12,\n",
       "        50,  43,  27,  56,  58,  13,  18,  59, 222, 223,  42,  77,  30,\n",
       "        76,  83,  88,  35,  34,  45,  18,  41,  39,  59,  73,  10,  17,\n",
       "        76,  94,   6,  22,   7,  25,  12,  16,  71, 102,  66,  50,  10,\n",
       "       179,  66,  17, 126,  44,  40,  16,  13,  35,  57,  36,  32,   8,\n",
       "       106,   8,  12,  91,  55,  55,  22,  16,   8,  87,  20,  49,  10,\n",
       "        74,  15,  54,  21,  57,  13,  23,   9,  14,  14,   9, 150,  46,\n",
       "        31,  35,  25,  89,  45,  24,  12,  29, 110,  64,  59,  11,  18,\n",
       "        12,  80,  10,  32,  33,  11,  51, 106,  15,  16,  10,  10,  10,\n",
       "         8,  86,   8,  34,  14,  12,  71,  95,  33,  10,  32,  10,  23,\n",
       "        93,  12,  34,  42,  15,  18,  32,   8,  48,  76,  23,   9,  89,\n",
       "        15, 106, 162,  20,   0, 116,  28,  49,  13, 187,  11,  54,  41,\n",
       "        16,  23,  23,  78,  34,  21,  33,  13,  73,  35,  19,  55,   9,\n",
       "        12,  15,  74,  13,  41,  36,  42,  16,   9,  58,   9, 165,  27,\n",
       "        27, 168,  24,  42,  45,  67,  65,  24,  16,  39,  61,  10,  19,\n",
       "        76,  42,   9,  55,  89,  11,  40,  51,  23,  24,  15,  11,   9,\n",
       "        18,  26,  12,  13,  82,  26,  13,  69,  10,  39,  14,  18,  49,\n",
       "        40,  51,  49,  13, 144,  78,  82,  35,  57, 106,  59,  10,  63,\n",
       "        73,  24,   8,  23, 113,  45,  21,  10,  26,  14,   8,  68,  22,\n",
       "        13,  27,   0, 179,   0,   0,  41,  26,  22,  45,  23, 116,  29,\n",
       "        55,  14,  11,  20,  34,  14,  17,  54,  34,  14,  10,  75,  15,\n",
       "        24,  89,  78,  41, 112,  51,  23,  73,  43,  13,  39,  16,  17,\n",
       "        18,  65,  52,  15,  17,  46,  84,  74,  76,   9,  13,  39,  87,\n",
       "        25,  22,  53, 108,  18,  74,  70,  43,  35,  15,  35,  31,  51,\n",
       "        28,  10,  51, 127,  78, 203,  25,  24,  51,  45,  64,  12, 106,\n",
       "        14,  10,  11,  49,  28,  12,  23,  10,  21, 112,  16,  72,   9,\n",
       "       114,  13,  20,  61,   9,  20,  60,  13,  47, 113,  16,   9,  36,\n",
       "        13,  72,  62,  56, 108,  19,  52,  22,  27,  11,  41,  42,  51,\n",
       "        35, 109,  11,  11,   9,  10,  32,  10,  16,  20, 158,  81,   9,\n",
       "        12,  23,   9,  59,  16,  29,  23,  74,  12,  20,  30,  18,  37,\n",
       "         8,  22, 133,  90,  11,  52,   8,  10,  93,  17,  52,  15,  43,\n",
       "        14,   0,  20,  24,  12,  10,  58,  31,  13,  28,  10,  14,  10,\n",
       "         8,  13,  43,  35,  32,  61,  37,  23,  12,  23,   9,  14,  25,\n",
       "        12,  30,  48,  28,  54,  15,  36,  52,  44,  29,   8,  27,  25,\n",
       "        27,  17,  18,  19,   9,  14,  45,  48,  15,  16,   7,  11,  48,\n",
       "       139,   9,  39,  44,  12,  92,  50,  24, 110,   0,  28,  10,  50,\n",
       "        21,  90,  20, 197,  16,  15,  18,  61,  26,  53,   0, 120,  16,\n",
       "        27,  50,  60,  24, 104,  25,  18,  18,   0,  47,  33,  42, 203,\n",
       "        78]), array([ 49,  25,  22,  28,  47,  30,  13,  38,  76,  58,  15,  47,  32,\n",
       "        41,  58, 202,  48,  14,  42,  31,  10,  46,  59,  21,  24,  27,\n",
       "        49, 105,  18,  24,  86,  38,  21,  20,  19,  37,  19,  22,  28,\n",
       "        83,  16,  38,  12,  65,  22,  34,  39,  12,  11,   9,  55,  25,\n",
       "        39,  27,  15,  37,  11, 223,  20,  31, 125,  25,  28,  30,  32,\n",
       "        56, 113,  93,  18,  25,  43,  24,  11, 233,  36, 232,  39,  74,\n",
       "        53,  34,  56,  19,  41,  30,  92,  36,  39,  15, 155,  25,  18,\n",
       "        52,  48,  43,  60,  58,  31,  20,  61, 225, 224,  44,  77,  35,\n",
       "        89,  85,  91,  37,  36,  50,  20,  41,  51,  71,  75,  12,  24,\n",
       "        81, 105,  10,  25,  12,  52,  32,  17,  72, 105,  66,  52,  36,\n",
       "       188,  71,  18, 130,  44,  42,  18,  16,  38,  61,  45,  34,   9,\n",
       "       106,   8,  43,  92,  62,  56,  23,  16,  12,  89,  22,  57,  21,\n",
       "        82,  16,  55,  24,  69,  13,  47,  21,  19,  15,  13, 173,  61,\n",
       "        31,  40,  27,  94,  55,  24,  14,  38, 111,  67,  85,  16,  19,\n",
       "        21,  80,  10,  35,  33,  11,  52, 106,  15,  23,  16,  17,  11,\n",
       "         9,  89,   8,  56,  18,  12,  72,  96,  33,  11,  37,  12,  31,\n",
       "        95,  16,  39,  50,  34,  20,  67,  12,  49,  77,  23,  15,  90,\n",
       "        44, 107, 165,  23,   0, 119,  29,  58,  17, 190,  12,  63,  41,\n",
       "        16,  29,  42,  89,  39,  37,  36,  33,  73,  36,  24,  57,  12,\n",
       "        15,  15,  75,  14,  44,  36,  52,  18,  10,  60,  23, 195,  69,\n",
       "        27, 172,  24,  75,  45,  69,  77,  26,  18,  58,  63,  30,  19,\n",
       "        76,  47,  36,  56,  90,  12,  43,  51,  25,  26,  15,  13,  13,\n",
       "        43,  26,  19,  15,  82,  54,  13,  91,  25,  46,  20,  18,  51,\n",
       "        42,  51,  51,  35, 144,  78,  83,  38,  62, 109,  62,  11,  87,\n",
       "        99,  24,  12,  23, 114,  53,  24,  42,  81,  34,   9,  77,  23,\n",
       "        14,  54,   0, 180,   0,   0,  49,  29,  25,  47,  30, 129,  29,\n",
       "        59,  15,  25,  21,  39,  33,  35,  59,  34,  15,  10,  76,  16,\n",
       "        25,  91,  90,  41, 113,  52,  23,  85,  47,  15,  39,  22,  17,\n",
       "        18,  67,  54,  35,  24,  75,  91,  74,  81,  18,  15,  42,  88,\n",
       "        25,  22,  55, 112,  48,  74,  74,  45,  51,  17,  43,  33,  52,\n",
       "        61,  11,  53, 134,  80, 206,  27,  24,  53,  49,  64,  20, 132,\n",
       "        15,  11,  12,  55,  41,  13,  28,  18,  23, 133,  22,  78,  11,\n",
       "       116,  36,  45,  61,   9,  42,  61,  21,  56, 116,  19,  11,  61,\n",
       "        15,  72,  68,  82, 128,  21,  57,  24,  28,  13,  46,  46,  52,\n",
       "        50, 110,  34,  12,  13,  24,  32,  22,  21,  31, 162,  85,  11,\n",
       "        13,  26,  20,  59,  18,  29,  25,  78,  38,  25,  33,  31,  60,\n",
       "        16,  22, 140,  92,  31,  59,  20,  31,  96,  20,  53,  17,  46,\n",
       "        18,   0,  20,  37,  12,  15,  59,  33,  15,  29,  14,  27,  24,\n",
       "         9,  15,  56,  38,  34,  69,  51,  25,  14,  23,   9,  16,  26,\n",
       "        16,  33,  50,  32,  91,  28,  37,  54,  49,  31,   9,  29,  69,\n",
       "        32,  24,  18,  27,  12,  35,  73,  48,  33,  22,   8,  18,  53,\n",
       "       141,  26,  39,  46,  16,  97,  51,  24, 112,   0,  42,  12,  52,\n",
       "        23,  95,  20, 199,  18,  15,  19,  85,  48,  55,   0, 120,  18,\n",
       "        29,  54,  78,  29, 116,  28,  48,  22,   0,  49,  33,  42, 206,\n",
       "        79])), metrics={'test_loss': 3.7799735069274902, 'test_exact_match': 9.424083769633508, 'test_f1': 20.391822200951516, 'test_precision': 22.270490982704246, 'test_recall': 32.26110659945819, 'test_runtime': 9.6723, 'test_samples_per_second': 59.242, 'test_steps_per_second': 7.444})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result = trainer_qa.predict(tokenized_data_qas_id_validation)\n",
    "predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "with open(f'{OUTPUT_DIR}/output.txt', \"w\") as f:\n",
    "  f.write(str(predict_result))\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2BS4XC5byxE"
   },
   "source": [
    "# Melakukan evaluasi dari prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9J-zKl_zkUze"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 9.424083769633508,\n",
       " 'f1': 20.391822200951516,\n",
       " 'precision': 22.270490982704246,\n",
       " 'recall': 32.26110659945819}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_result = compute_metrics(predict_result)\n",
    "accuracy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(ACCURACY_DIR), exist_ok=True)\n",
    "with open(f'{ACCURACY_DIR}/accuracy.txt', \"w\") as f:\n",
    "  f.write(str(accuracy_result))\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coba Alur 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19]],\n",
       "\n",
       "       [[20, 21, 22, 23, 24],\n",
       "        [25, 26, 27, 28, 29],\n",
       "        [30, 31, 32, 33, 34],\n",
       "        [35, 36, 37, 38, 39]],\n",
       "\n",
       "       [[40, 41, 42, 43, 44],\n",
       "        [45, 46, 47, 48, 49],\n",
       "        [50, 51, 52, 53, 54],\n",
       "        [55, 56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(60).reshape(3, 4, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  8, 13, 18],\n",
       "       [23, 28, 33, 38],\n",
       "       [43, 48, 53, 58]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3, 0:4, -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_result.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 3], [2, 2], [4, 2]])\n",
    "print(x.shape)\n",
    "print(np.argsort(x)[-1][-1])\n",
    "print(np.argmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 573)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9, 43, 73, ..., 42, 16, 36],\n",
       "       [54, 48, 74, ..., 42, 57, 21]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.argmax(predict_result.predictions, axis=2)\n",
    "print(x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 573)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[250, 298, 357, ..., 294, 291, 181],\n",
       "       [349, 365, 383, ..., 294, 247, 112]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.argsort(predict_result.predictions, axis=2)[:,:, -0]\n",
    "print(x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_qas_dataframe(predict_result=predict_result, index_largest=1):\n",
    "    predictions_idx = np.argsort(predict_result.predictions, axis=2)[:, :, index_largest * -1]\n",
    "    #predictions_idx = np.argmax(predict_result.predictions, axis=2)\n",
    "    label_array = np.asarray(predict_result.label_ids)\n",
    "    question_decoded = []\n",
    "    context_decoded = []\n",
    "    pred_answer_decoded = []\n",
    "    gold_answer_decoded = []\n",
    "    \n",
    "    for i in tqdm(range(len(predict_result.predictions[0]))):\n",
    "        start_pred_idx = predictions_idx[0][i]\n",
    "        end_pred_idx = predictions_idx[1][i] + 1\n",
    "        pred_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_pred_idx: end_pred_idx], skip_special_tokens=True)\n",
    "        pred_answer_decoded.append(pred_answer)\n",
    "        \n",
    "        start_gold_idx = label_array[0][i]\n",
    "        end_gold_idx = label_array[1][i] + 1\n",
    "        gold_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_gold_idx: end_gold_idx], skip_special_tokens=True)\n",
    "        gold_answer_decoded.append(gold_answer)\n",
    "        \n",
    "        question = []\n",
    "        context = []\n",
    "         \n",
    "        for j in range(len(tokenized_data_qas_id_validation[i]['token_type_ids'])):\n",
    "            if tokenized_data_qas_id_validation[i]['token_type_ids'][j] == 0:\n",
    "                question.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "            else:\n",
    "                context.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "\n",
    "        question_decoded.append(tokenizer.decode(question, skip_special_tokens=True))\n",
    "        context_decoded.append(tokenizer.decode(context, skip_special_tokens=True))\n",
    "    \n",
    "    qas_df = pd.DataFrame({'Context': context_decoded, \n",
    "                           'Question': question_decoded, \n",
    "                           'Prediction Answer': pred_answer_decoded,\n",
    "                          'Gold Answer': gold_answer_decoded})\n",
    "                      \n",
    "    return qas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 573/573 [04:04<00:00,  2.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Question</th>\n",
       "      <th>Prediction Answer</th>\n",
       "      <th>Gold Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kolumbus bukanlah orang pertama yang tiba di a...</td>\n",
       "      <td>siapakah yang menemuka benua amerika?</td>\n",
       "      <td>kolumbus bukanlah orang pertama yang tiba di a...</td>\n",
       "      <td>orang - orang viking dari eropa utara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kabupaten donggala ( english : donggala regenc...</td>\n",
       "      <td>dimanakah letak donggala?</td>\n",
       "      <td>4275, 08km²</td>\n",
       "      <td>provinsi sulawesi tengah, indonesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awal mula teknik industri dapat ditelusuri dar...</td>\n",
       "      <td>siapa bapak teknik industri?</td>\n",
       "      <td>1798</td>\n",
       "      <td>frederick winslow taylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>penghulu rasyid ( lahir di desa telaga itar ta...</td>\n",
       "      <td>kapan penghulu rasyid meninggal?</td>\n",
       "      <td>1815 – meninggal di desa banua lawas, 15 desem...</td>\n",
       "      <td>15 desember 1861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>samudra pasifik atau lautan teduh ( dari bahas...</td>\n",
       "      <td>seberapa luas kah samudera pasifik?</td>\n",
       "      <td>69, 4 juta mi² ). panjangnya sekitar 15. 500</td>\n",
       "      <td>179, 7 juta km²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>pt scooter victory inter part atau yang lebih ...</td>\n",
       "      <td>dimana kantor pusat pt scooter victory inter p...</td>\n",
       "      <td>8 september 2011</td>\n",
       "      <td>bekasi, indonesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>studi hubungan internasional sebagai teori sud...</td>\n",
       "      <td>kapan teori hubungan internasional diciptakan?</td>\n",
       "      <td>1939</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>musik hip hop atau hip - hop [ 1 ] [ 2 ] atau ...</td>\n",
       "      <td>kapan musik hip hop pertama kali muncul?</td>\n",
       "      <td>1970an</td>\n",
       "      <td>1970an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>dalam melaksanakan kegiatan belajar - mengajar...</td>\n",
       "      <td>berapa luas smk negeri 1 cikampek?</td>\n",
       "      <td>smk negeri 1 cikampek menempati 3 buah kampus ...</td>\n",
       "      <td>29095m²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>ikan pari manta ( manta birostris ) adalah sal...</td>\n",
       "      <td>berapakah berat ikan pari manta yag terbesar?</td>\n",
       "      <td></td>\n",
       "      <td>3 ton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Context  \\\n",
       "0    kolumbus bukanlah orang pertama yang tiba di a...   \n",
       "1    kabupaten donggala ( english : donggala regenc...   \n",
       "2    awal mula teknik industri dapat ditelusuri dar...   \n",
       "3    penghulu rasyid ( lahir di desa telaga itar ta...   \n",
       "4    samudra pasifik atau lautan teduh ( dari bahas...   \n",
       "..                                                 ...   \n",
       "568  pt scooter victory inter part atau yang lebih ...   \n",
       "569  studi hubungan internasional sebagai teori sud...   \n",
       "570  musik hip hop atau hip - hop [ 1 ] [ 2 ] atau ...   \n",
       "571  dalam melaksanakan kegiatan belajar - mengajar...   \n",
       "572  ikan pari manta ( manta birostris ) adalah sal...   \n",
       "\n",
       "                                              Question  \\\n",
       "0                siapakah yang menemuka benua amerika?   \n",
       "1                            dimanakah letak donggala?   \n",
       "2                         siapa bapak teknik industri?   \n",
       "3                     kapan penghulu rasyid meninggal?   \n",
       "4                  seberapa luas kah samudera pasifik?   \n",
       "..                                                 ...   \n",
       "568  dimana kantor pusat pt scooter victory inter p...   \n",
       "569     kapan teori hubungan internasional diciptakan?   \n",
       "570           kapan musik hip hop pertama kali muncul?   \n",
       "571                 berapa luas smk negeri 1 cikampek?   \n",
       "572      berapakah berat ikan pari manta yag terbesar?   \n",
       "\n",
       "                                     Prediction Answer  \\\n",
       "0    kolumbus bukanlah orang pertama yang tiba di a...   \n",
       "1                                          4275, 08km²   \n",
       "2                                                 1798   \n",
       "3    1815 – meninggal di desa banua lawas, 15 desem...   \n",
       "4         69, 4 juta mi² ). panjangnya sekitar 15. 500   \n",
       "..                                                 ...   \n",
       "568                                   8 september 2011   \n",
       "569                                               1939   \n",
       "570                                             1970an   \n",
       "571  smk negeri 1 cikampek menempati 3 buah kampus ...   \n",
       "572                                                      \n",
       "\n",
       "                               Gold Answer  \n",
       "0    orang - orang viking dari eropa utara  \n",
       "1      provinsi sulawesi tengah, indonesia  \n",
       "2                 frederick winslow taylor  \n",
       "3                         15 desember 1861  \n",
       "4                          179, 7 juta km²  \n",
       "..                                     ...  \n",
       "568                      bekasi, indonesia  \n",
       "569                                   1939  \n",
       "570                                 1970an  \n",
       "571                                29095m²  \n",
       "572                                  3 ton  \n",
       "\n",
       "[573 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qas_df = create_qas_dataframe(predict_result)\n",
    "qas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_mark = ['siapa', 'siapakah',\n",
    "                    'apa', 'apakah', 'adakah',\n",
    "                    'dimana', 'dimanakah', 'darimanakah',\n",
    "                    'kapan', 'kapankah',\n",
    "                    'bagaimana', 'bagaimanakah',\n",
    "                    'kenapa', 'mengapa',\n",
    "                    'berapa', 'berapakah', 'seberapa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kah\n"
     ]
    }
   ],
   "source": [
    "hipo = \"kapankah musik hip hop kah pertama kali muncul?\"\n",
    "\n",
    "for i in hipo.split():\n",
    "    if (i == \"kah\") and (i not in question_mark):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nli_dataframe(df, type='replace first'):\n",
    "    \n",
    "    nli_df = pd.DataFrame()\n",
    "    \n",
    "    question_mark = ['siapa', 'siapakah',\n",
    "                    'apa', 'apakah', 'adakah',\n",
    "                    'dimana', 'dimanakah', 'darimanakah',\n",
    "                    'kapan', 'kapankah',\n",
    "                    'bagaimana', 'bagaimanakah',\n",
    "                    'kenapa', 'mengapa',\n",
    "                    'berapa', 'berapakah', 'seberapa']\n",
    "    \n",
    "    if type == 'rule based':\n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            premise = df['Context'][i]\n",
    "            hypothesis = df['Question'][i]\n",
    "            \n",
    "            hypothesis = hypothesis.replace('kah', '')\n",
    "            \n",
    "            for j in hypothesis.split():\n",
    "                if j in question_mark:\n",
    "                    if j == 'siapa' or j == 'siapakah':\n",
    "                        pred_hypothesis = hypothesis.replace('?', '')\n",
    "                        pred_hypothesis = pred_hypothesis.replace(j, '')\n",
    "                        pred_hypothesis = f\"{pred_hypothesis} adalah {df['Prediction Answer'][i]}\"\n",
    "\n",
    "                        gold_hypothesis = hypothesis.replace('?', '')\n",
    "                        gold_hypothesis = gold_hypothesis.replace(j, '')\n",
    "                        gold_hypothesis = f\"{gold_hypothesis} adalah {df['Gold Answer'][i]}\"\n",
    "                    \n",
    "                    elif j == 'apa' or j == 'apakah' or j == 'adakah':\n",
    "                        pred_hypothesis = hypothesis.replace('?', '')\n",
    "                        pred_hypothesis = pred_hypothesis.replace(j, '')\n",
    "                        pred_hypothesis = f\"{pred_hypothesis} adalah {df['Prediction Answer'][i]}\"\n",
    "\n",
    "                        gold_hypothesis = hypothesis.replace('?', '')\n",
    "                        gold_hypothesis = gold_hypothesis.replace(j, '')\n",
    "                        gold_hypothesis = f\"{gold_hypothesis} adalah {df['Gold Answer'][i]}\"\n",
    "                        \n",
    "                    elif j == 'dimana' or j == 'dimanakah':\n",
    "                        pred_hypothesis = hypothesis.replace('?', '')\n",
    "                        pred_hypothesis = pred_hypothesis.replace(j, '')\n",
    "                        pred_hypothesis = f\"{pred_hypothesis} di {df['Prediction Answer'][i]}\"\n",
    "\n",
    "                        gold_hypothesis = hypothesis.replace('?', '')\n",
    "                        gold_hypothesis = gold_hypothesis.replace(j, '')\n",
    "                        gold_hypothesis = f\"{gold_hypothesis} di {df['Gold Answer'][i]}\"\n",
    "                    \n",
    "                    elif j == 'darimanakah':\n",
    "                        pred_hypothesis = hypothesis.replace('?', '')\n",
    "                        pred_hypothesis = pred_hypothesis.replace(j, '')\n",
    "                        pred_hypothesis = f\"{pred_hypothesis} dari {df['Prediction Answer'][i]}\"\n",
    "\n",
    "                        gold_hypothesis = hypothesis.replace('?', '')\n",
    "                        gold_hypothesis = gold_hypothesis.replace(j, '')\n",
    "                        gold_hypothesis = f\"{gold_hypothesis} dari {df['Gold Answer'][i]}\"\n",
    "                    \n",
    "                    elif j == 'kapan' or j == 'kapankah':\n",
    "                        pred_hypothesis = hypothesis.replace('?', '')\n",
    "                        pred_hypothesis = pred_hypothesis.replace(j, '')\n",
    "                        pred_hypothesis = f\"{pred_hypothesis} pada {df['Prediction Answer'][i]}\"\n",
    "\n",
    "                        gold_hypothesis = hypothesis.replace('?', '')\n",
    "                        gold_hypothesis = gold_hypothesis.replace(j, '')\n",
    "                        gold_hypothesis = f\"{gold_hypothesis} pada {df['Gold Answer'][i]}\"\n",
    "\n",
    "                    elif j == 'bagaimana' or j == 'bagaimanakah':\n",
    "                        pred_hypothesis = hypothesis.replace('?', '')\n",
    "                        pred_hypothesis = pred_hypothesis.replace(j, '')\n",
    "                        pred_hypothesis = f\"{pred_hypothesis} adalah {df['Prediction Answer'][i]}\"\n",
    "\n",
    "                        gold_hypothesis = hypothesis.replace('?', '')\n",
    "                        gold_hypothesis = gold_hypothesis.replace(j, '')\n",
    "                        gold_hypothesis = f\"{gold_hypothesis} adalah {df['Gold Answer'][i]}\"\n",
    "                    \n",
    "                    elif j == 'kenapa' or j == 'mengapa':\n",
    "                        pred_hypothesis = hypothesis.replace('?', '')\n",
    "                        pred_hypothesis = pred_hypothesis.replace(j, 'alasan')\n",
    "                        pred_hypothesis = f\"{pred_hypothesis} adalah karena {df['Prediction Answer'][i]}\"\n",
    "\n",
    "                        gold_hypothesis = hypothesis.replace('?', '')\n",
    "                        gold_hypothesis = gold_hypothesis.replace(j, 'alasan')\n",
    "                        gold_hypothesis = f\"{gold_hypothesis} adalah karena {df['Gold Answer'][i]}\"\n",
    "                    \n",
    "                    elif j == 'berapa' or j == 'berapakah' or j == 'seberapa': \n",
    "                        pred_hypothesis = hypothesis.replace('?', '')\n",
    "                        pred_hypothesis = pred_hypothesis.replace(j, '')\n",
    "                        pred_hypothesis = f\"{pred_hypothesis} adalah {df['Prediction Answer'][i]}\"\n",
    "\n",
    "                        gold_hypothesis = hypothesis.replace('?', '')\n",
    "                        gold_hypothesis = gold_hypothesis.replace(j, '')\n",
    "                        gold_hypothesis = f\"{gold_hypothesis} adalah {df['Gold Answer'][i]}\"\n",
    "                    \n",
    "            nli_df = nli_df.append({'Premise': premise,\n",
    "                           'Prediction Hypothesis': pred_hypothesis,\n",
    "                            'Gold Hypothesis': gold_hypothesis\n",
    "                           }, ignore_index=True)\n",
    "\n",
    "    elif type == 'replace first':\n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            premise = df['Context'][i]\n",
    "            hypothesis = df['Question'][i]\n",
    "            pred_hypothesis = hypothesis.replace('?', '')\n",
    "            pred_hypothesis = pred_hypothesis.replace(hypothesis.split()[0], \n",
    "                                            df['Prediction Answer'][i])\n",
    "\n",
    "            gold_hypothesis = hypothesis.replace('?', '')\n",
    "            gold_hypothesis = gold_hypothesis.replace(hypothesis.split()[0], \n",
    "                                            df['Gold Answer'][i])\n",
    "            \n",
    "            nli_df = nli_df.append({'Premise': premise,\n",
    "                               'Prediction Hypothesis': pred_hypothesis,\n",
    "                                'Gold Hypothesis': gold_hypothesis\n",
    "                               }, ignore_index=True)\n",
    "    \n",
    "    elif type == 'replace question mark':\n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            premise = df['Context'][i]\n",
    "            hypothesis = df['Question'][i]\n",
    "            for j in hypothesis.split():\n",
    "                if j in question_mark:\n",
    "                    pred_hypothesis = hypothesis.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, df['Prediction Answer'][i])\n",
    "                    \n",
    "                    gold_hypothesis = hypothesis.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, df['Gold Answer'][i])\n",
    "                \n",
    "            nli_df = nli_df.append({'Premise': premise,\n",
    "                                   'Prediction Hypothesis': pred_hypothesis,\n",
    "                                    'Gold Hypothesis': gold_hypothesis\n",
    "                                   }, ignore_index=True)\n",
    "    \n",
    "    elif type == 'machine generation': pass # TODO\n",
    "    \n",
    "    elif type == 'add adalah': \n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            premise = df['Context'][i]\n",
    "            hypothesis = df['Question'][i]\n",
    "            pred_hypothesis = hypothesis.replace('?', '')\n",
    "            pred_hypothesis = pred_hypothesis.replace(hypothesis.split()[0], '')\n",
    "            pred_hypothesis = f\"{pred_hypothesis} adalah {df['Prediction Answer'][i]}\"\n",
    "\n",
    "            gold_hypothesis = hypothesis.replace('?', '')\n",
    "            gold_hypothesis = gold_hypothesis.replace(hypothesis.split()[0], '')\n",
    "            gold_hypothesis = f\"{gold_hypothesis} adalah {df['Gold Answer'][i]}\"\n",
    "            \n",
    "            nli_df = nli_df.append({'Premise': premise,\n",
    "                               'Prediction Hypothesis': pred_hypothesis,\n",
    "                                'Gold Hypothesis': gold_hypothesis\n",
    "                               }, ignore_index=True)\n",
    "    \n",
    "    elif type == 'just concat answer and question':\n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            premise = df['Context'][i]\n",
    "            hypothesis = df['Question'][i]\n",
    "\n",
    "            pred_hypothesis = f\"{hypothesis} {df['Prediction Answer'][i]}\"         \n",
    "            gold_hypothesis = f\"{hypothesis} {df['Gold Answer'][i]}\"\n",
    "                \n",
    "            nli_df = nli_df.append({'Premise': premise,\n",
    "                                   'Prediction Hypothesis': pred_hypothesis,\n",
    "                                    'Gold Hypothesis': gold_hypothesis\n",
    "                                   }, ignore_index=True)\n",
    "    '''\n",
    "    kata_tanya = []\n",
    "    for i in tqdm(range(df.shape[0])): \n",
    "        hypothesis = df['Question'][i]\n",
    "        kata_tanya.append(hypothesis.split()[0])\n",
    "    unik = set(np.unique(kata_tanya))\n",
    "    temp3 = [x for x in unik if x not in question_mark]\n",
    "    print(temp3)\n",
    "    '''\n",
    "        \n",
    "    return nli_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 573/573 [00:00<00:00, 611.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Premise</th>\n",
       "      <th>Prediction Hypothesis</th>\n",
       "      <th>Gold Hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kolumbus bukanlah orang pertama yang tiba di a...</td>\n",
       "      <td>yang menemuka benua amerika adalah kolumbus b...</td>\n",
       "      <td>yang menemuka benua amerika adalah orang - or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kabupaten donggala ( english : donggala regenc...</td>\n",
       "      <td>letak donggala di 4275, 08km²</td>\n",
       "      <td>letak donggala di provinsi sulawesi tengah, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awal mula teknik industri dapat ditelusuri dar...</td>\n",
       "      <td>bapak teknik industri adalah 1798</td>\n",
       "      <td>bapak teknik industri adalah frederick winslo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>penghulu rasyid ( lahir di desa telaga itar ta...</td>\n",
       "      <td>penghulu rasyid meninggal pada 1815 – meningg...</td>\n",
       "      <td>penghulu rasyid meninggal pada 15 desember 1861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>samudra pasifik atau lautan teduh ( dari bahas...</td>\n",
       "      <td>luas  samudera pasifik adalah 69, 4 juta mi² ...</td>\n",
       "      <td>luas  samudera pasifik adalah 179, 7 juta km²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>pt scooter victory inter part atau yang lebih ...</td>\n",
       "      <td>kantor pusat pt scooter victory inter part di...</td>\n",
       "      <td>kantor pusat pt scooter victory inter part di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>studi hubungan internasional sebagai teori sud...</td>\n",
       "      <td>teori hubungan internasional diciptakan pada ...</td>\n",
       "      <td>teori hubungan internasional diciptakan pada ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>musik hip hop atau hip - hop [ 1 ] [ 2 ] atau ...</td>\n",
       "      <td>musik hip hop pertama kali muncul pada 1970an</td>\n",
       "      <td>musik hip hop pertama kali muncul pada 1970an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>dalam melaksanakan kegiatan belajar - mengajar...</td>\n",
       "      <td>luas smk negeri 1 cikampek adalah smk negeri ...</td>\n",
       "      <td>luas smk negeri 1 cikampek adalah 29095m²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>ikan pari manta ( manta birostris ) adalah sal...</td>\n",
       "      <td>berat ikan pari manta yag terbesar adalah</td>\n",
       "      <td>berat ikan pari manta yag terbesar adalah 3 ton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Premise  \\\n",
       "0    kolumbus bukanlah orang pertama yang tiba di a...   \n",
       "1    kabupaten donggala ( english : donggala regenc...   \n",
       "2    awal mula teknik industri dapat ditelusuri dar...   \n",
       "3    penghulu rasyid ( lahir di desa telaga itar ta...   \n",
       "4    samudra pasifik atau lautan teduh ( dari bahas...   \n",
       "..                                                 ...   \n",
       "568  pt scooter victory inter part atau yang lebih ...   \n",
       "569  studi hubungan internasional sebagai teori sud...   \n",
       "570  musik hip hop atau hip - hop [ 1 ] [ 2 ] atau ...   \n",
       "571  dalam melaksanakan kegiatan belajar - mengajar...   \n",
       "572  ikan pari manta ( manta birostris ) adalah sal...   \n",
       "\n",
       "                                 Prediction Hypothesis  \\\n",
       "0     yang menemuka benua amerika adalah kolumbus b...   \n",
       "1                        letak donggala di 4275, 08km²   \n",
       "2                    bapak teknik industri adalah 1798   \n",
       "3     penghulu rasyid meninggal pada 1815 – meningg...   \n",
       "4     luas  samudera pasifik adalah 69, 4 juta mi² ...   \n",
       "..                                                 ...   \n",
       "568   kantor pusat pt scooter victory inter part di...   \n",
       "569   teori hubungan internasional diciptakan pada ...   \n",
       "570      musik hip hop pertama kali muncul pada 1970an   \n",
       "571   luas smk negeri 1 cikampek adalah smk negeri ...   \n",
       "572         berat ikan pari manta yag terbesar adalah    \n",
       "\n",
       "                                       Gold Hypothesis  \n",
       "0     yang menemuka benua amerika adalah orang - or...  \n",
       "1     letak donggala di provinsi sulawesi tengah, i...  \n",
       "2     bapak teknik industri adalah frederick winslo...  \n",
       "3      penghulu rasyid meninggal pada 15 desember 1861  \n",
       "4        luas  samudera pasifik adalah 179, 7 juta km²  \n",
       "..                                                 ...  \n",
       "568   kantor pusat pt scooter victory inter part di...  \n",
       "569   teori hubungan internasional diciptakan pada ...  \n",
       "570      musik hip hop pertama kali muncul pada 1970an  \n",
       "571          luas smk negeri 1 cikampek adalah 29095m²  \n",
       "572    berat ikan pari manta yag terbesar adalah 3 ton  \n",
       "\n",
       "[573 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_df = create_nli_dataframe(qas_df, type='rule based')\n",
    "nli_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mencoba cara retrieve model dari HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pretrained_name_sc = \"afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased\"\n",
    "pretrained_name_qa = \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\"\n",
    "tokenizer_kwargs = {'padding': True, 'truncation': True, 'max_length': MAX_LENGTH}\n",
    "\n",
    "nlp_sc = pipeline(task=\"text-classification\", model=pretrained_name_sc, tokenizer=pretrained_name_sc)\n",
    "nlp_qa = pipeline(task=\"question-answering\", model=pretrained_name_qa, tokenizer=pretrained_name_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'entailment', 'score': 0.987500011920929},\n",
       " {'label': 'contradiction', 'score': 0.011974925175309181},\n",
       " {'label': 'neutral', 'score': 0.0005250379908829927}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nlp_sc({'text': nli_df['Premise'][0], 'text_pair': nli_df['Prediction Hypothesis'][0]}, top_k=3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.31116893887519836,\n",
       " 'start': 78,\n",
       " 'end': 113,\n",
       " 'answer': 'provinsi sulawesi tengah, indonesia'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_qa(question=qas_df['Question'][1], context=qas_df['Context'][1], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coba buat method evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_after_postprocess(df=nli_df):\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        predicted = nlp_sc({'text': df['Premise'][i], 'text_pair': df['Prediction Hypothesis'][i]}, \n",
    "                           top_k=1, **tokenizer_kwargs)[0]['label']\n",
    "        print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9, 43, 73, ..., 42, 16, 36],\n",
       "       [54, 48, 74, ..., 42, 57, 21]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_idx = np.argsort(predict_result.predictions, axis=2)[:, :, 1 * -1]\n",
    "predictions_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXIMUM_SEARCH_ITER =  3\n",
    "\n",
    "def evaluation(predict_result, type_smoothing=\"replace first\", type_qas=\"TODO\", MAXIMUM_SEARCH_ITER=MAXIMUM_SEARCH_ITER):\n",
    "    \n",
    "    # Ekstrak dari PredictionOutput QAS\n",
    "    predictions_idx = np.argsort(predict_result.predictions, axis=2)[:, :, 1 * -1]\n",
    "    label_array = np.asarray(predict_result.label_ids)\n",
    "    \n",
    "    question_decoded = []\n",
    "    context_decoded = []\n",
    "    pred_answer_decoded = []\n",
    "    gold_answer_decoded = []\n",
    "    \n",
    "    question_d = None\n",
    "    context_d = None\n",
    "    \n",
    "    # Iterasi ini ditujukan untuk retrieve answer\n",
    "    for i in tqdm(range(len(predict_result.predictions[0]))):\n",
    "        \n",
    "        start_pred_idx = predictions_idx[0][i]\n",
    "        end_pred_idx = predictions_idx[1][i] + 1\n",
    "        \n",
    "        start_gold_idx = label_array[0][i]\n",
    "        end_gold_idx = label_array[1][i] + 1\n",
    "        \n",
    "        # Retrieve answer prediksi\n",
    "        pred_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_pred_idx: end_pred_idx], skip_special_tokens=True)\n",
    "        \n",
    "        # Retrieve answer gold\n",
    "        gold_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_gold_idx: end_gold_idx], skip_special_tokens=True)\n",
    "        \n",
    "        gold_answer_decoded.append(gold_answer)\n",
    "        \n",
    "        question = []\n",
    "        context = []\n",
    "        \n",
    "        # Iterasi ini untuk retrieve question dan context index yang bersangkutan\n",
    "        for j in range(len(tokenized_data_qas_id_validation[i]['token_type_ids'])):\n",
    "            \n",
    "            # Bila token_type_ids-nya 0, maka itu question (sesuai dengan urutan tokenisasi)\n",
    "            if tokenized_data_qas_id_validation[i]['token_type_ids'][j] == 0:\n",
    "                question.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "                question_d = tokenizer.decode(question, skip_special_tokens=True)\n",
    "            \n",
    "            # Bila token_type_ids-nya 1, maka itu context (sesuai dengan urutan tokenisasi)\n",
    "            else:\n",
    "                context.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "                context_d = tokenizer.decode(context, skip_special_tokens=True)\n",
    "            \n",
    "            # Bagian untuk Smoothing (On-Progress, sudah dikerjakan diatas, belum \"ditempel\" kesini)\n",
    "            \n",
    "            # Cek label dari answer prediksi dan context\n",
    "            predicted_label = nlp_sc({'text': pred_answer, 'text_pair': context_d}, **tokenizer_kwargs)['label'] \n",
    "            \n",
    "            # Cek label dari answer prediksi dan context, bila labelnya entailment, maka answernya jadi hasil akhir\n",
    "            if predicted_label == 'entailment':\n",
    "                question_decoded.append(tokenizer.decode(question, skip_special_tokens=True))\n",
    "                context_decoded.append(tokenizer.decode(context, skip_special_tokens=True))\n",
    "                pred_answer_decoded.append(pred_answer)\n",
    "            \n",
    "            # Cek label dari answer prediksi dan context, bila labelnya bukan entailment, maka masuk ke for-loop untuk\n",
    "            # -- iterasi ke argmax selanjutnya, dengan menggunakan argsort\n",
    "            else:\n",
    "                \n",
    "                # Bila bukan entailment, loop sebanyak MAXIMUM_SEARCH_ITER kali.\n",
    "                for index_largest in range(MAXIMUM_SEARCH_ITER):\n",
    "                    \n",
    "                    # Cari di index kedua, ketiga, keempat, dan seterusnya\n",
    "                    predictions_idx_inside_loop = np.argsort(predict_result.predictions, \n",
    "                                                             axis=2)[:, :, (index_largest + 2) * -1]\n",
    "                    \n",
    "                    start_pred_idx = predictions_idx_inside_loop[0][i]\n",
    "                    end_pred_idx = predictions_idx_inside_loop[1][i] + 1\n",
    "                    \n",
    "                    # Retrieve answer prediksi\n",
    "                    pred_answer_inside_loop = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                                   [start_pred_idx: end_pred_idx], skip_special_tokens=True)\n",
    "                    \n",
    "                    # Cek label dari answer prediksi dan context\n",
    "                    predicted_label_inside_loop = nlp_sc({'text': pred_answer_inside_loop, 'text_pair': context_d}\n",
    "                                       , **tokenizer_kwargs)['label']\n",
    "                    \n",
    "                    # Bila label-nya sudah entailment, maka answernya jadi hasil akhir, dan break\n",
    "                    if predicted_label_inside_loop == 'entailment':\n",
    "                        question_decoded.append(tokenizer.decode(question, skip_special_tokens=True))\n",
    "                        context_decoded.append(tokenizer.decode(context, skip_special_tokens=True))\n",
    "                        pred_answer_decoded.append(pred_answer_inside_loop)\n",
    "                        break\n",
    "                    \n",
    "                    # Bila sampai iterasi terakhir, belum entailment juga, maka append saja jawaban kosong\n",
    "                    if index_largest == (MAXIMUM_SEARCH_ITER - 1):\n",
    "                        question_decoded.append(tokenizer.decode(question, skip_special_tokens=True))\n",
    "                        context_decoded.append(tokenizer.decode(context, skip_special_tokens=True))\n",
    "                        pred_answer_decoded.append(\"\") # Disini, jawaban kosong\n",
    "                        break\n",
    "                    \n",
    "                    # Bila label-nya belum entailment hingga iterasi terakhir, continue sampai MAXIMUM_SEARCH_ITER terakhir\n",
    "                    else: continue\n",
    "    \n",
    "    # Buat DataFrame QAS\n",
    "    qas_df = pd.DataFrame({'Context': context_decoded, \n",
    "                           'Question': question_decoded, \n",
    "                           'Prediction Answer': pred_answer_decoded,\n",
    "                          'Gold Answer': gold_answer_decoded})\n",
    "    \n",
    "    # Return DataFrame QAS\n",
    "    return qas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                               | 2/573 [02:58<14:10:16, 89.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_result\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 42\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(predict_result, type_smoothing, type_qas)\u001b[0m\n\u001b[1;32m     37\u001b[0m     context_d \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(context, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Smoothing (OTW)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Cek entails\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_sc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_pair\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_d\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentailment\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     44\u001b[0m     question_decoded\u001b[38;5;241m.\u001b[39mappend(tokenizer\u001b[38;5;241m.\u001b[39mdecode(question, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py:155\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:1084\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1077\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1078\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m     )\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1084\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:1091\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1090\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1091\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:992\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m    991\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 992\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    993\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py:182\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_inputs):\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:1563\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1563\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1577\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:1012\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1012\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1020\u001b[0m     embedding_output,\n\u001b[1;32m   1021\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1030\u001b[0m )\n\u001b[1;32m   1031\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:235\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    233\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 235\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    237\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluation(predict_result)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05c4878c74ad419f81e6abc2b9f9fae1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0633918b582f45c38356130338ccfb58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8aa4db7376e3445089af950c446aab70",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d7c6178f42ed4b9f8c5169c21783ce2d",
      "value": 2
     }
    },
    "065d96b02c7840019af97e3c135693a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ce874e529474f478a7acd45b4ffd649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_902bd6c12ea54039b8c913a7c1782ff4",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d86de7f1fbf452892fe6d4b177558f9",
      "value": 6
     }
    },
    "10fb50db270449b48382c815904245a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc7073fb093543fc990abc620348c1ca",
      "placeholder": "​",
      "style": "IPY_MODEL_acac4d071be4488fbb272df63b625213",
      "value": " 66/66 [23:11&lt;00:00, 14.47s/ba]"
     }
    },
    "113ae282ca5a499a8a2ad36a796e763f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f10632530a1b499ba3d20f0286446bab",
      "placeholder": "​",
      "style": "IPY_MODEL_94da3d6dfccc4a289c659b94bcc90658",
      "value": "#0: 100%"
     }
    },
    "12c3fd408f914038acb511c4ef57f965": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13ea541bb13b42738e74885a2c1db8df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14a450c5594148c3a4ca340521d167ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13ea541bb13b42738e74885a2c1db8df",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_68818d7eea5849afa82c3003a57a5b8d",
      "value": 6
     }
    },
    "1d86de7f1fbf452892fe6d4b177558f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23ce34e28a524ad9ab44a3715c8be175": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05c4878c74ad419f81e6abc2b9f9fae1",
      "placeholder": "​",
      "style": "IPY_MODEL_7fc9c87c6cc44cdf82ea4f74c0bfe7b4",
      "value": "100%"
     }
    },
    "2721b15477d741519a3041a8f256575b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a373f9cb48a481d85803f0792ff27b3",
       "IPY_MODEL_54a2637e8c8e4ca0917e35f4b58b48f3",
       "IPY_MODEL_7dc3dc97044c48afb06a14575fc8e91b"
      ],
      "layout": "IPY_MODEL_6c3c7e4da5af40b088f3046d7698edff"
     }
    },
    "29b13cbb638947d5a17071d299e2420f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_113ae282ca5a499a8a2ad36a796e763f",
       "IPY_MODEL_0ce874e529474f478a7acd45b4ffd649",
       "IPY_MODEL_935752d8733c41fbb25ed436606352ca"
      ],
      "layout": "IPY_MODEL_cf2b519cc5ff446dae45afa2439c840e"
     }
    },
    "2e0e9b7ee63b4c748d6294d63cc7b088": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd4d3c6663db46c9934c0d1f7a128d4c",
      "placeholder": "​",
      "style": "IPY_MODEL_3db65eb148504de6a4207708554874f0",
      "value": " 2/2 [00:00&lt;00:00,  8.75it/s]"
     }
    },
    "3db65eb148504de6a4207708554874f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f3c8a647bb24840ae71ce6e3219f4dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23ce34e28a524ad9ab44a3715c8be175",
       "IPY_MODEL_87c26a64111847f3a6cdb2f0797ae1f5",
       "IPY_MODEL_2e0e9b7ee63b4c748d6294d63cc7b088"
      ],
      "layout": "IPY_MODEL_750e7a8018e6415f8cfc15288d80f13f"
     }
    },
    "3f6183fb8ca348e0ac35b495681c705a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45f8bfb31705441c80a22dbb955dd933": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4836cff910304df894759890642ac333": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f2a3658a0f14f66a59c14d36558c7e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86b094a2e5564e4d8af8693dab674961",
      "placeholder": "​",
      "style": "IPY_MODEL_7fc619affab644758cfea5857adb7c93",
      "value": " 2/2 [00:00&lt;00:00,  4.72it/s]"
     }
    },
    "4f80ae88d884407ab8f9e08aa58632a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "540c7fcbf3b14e538cba18329bc1fc20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54a2637e8c8e4ca0917e35f4b58b48f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dce089e985634a67a381bc85cbaca017",
      "max": 66,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd9cb96d5a3c4400bb7d145590540a54",
      "value": 66
     }
    },
    "6516c5b02aa24db2bc42cdd4d1f2c260": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cbed771e77314789bf8817404e5d6f67",
       "IPY_MODEL_b0be384a887847e981868df7c551d3de",
       "IPY_MODEL_10fb50db270449b48382c815904245a8"
      ],
      "layout": "IPY_MODEL_b23174d38e3940718252770bd39177f8"
     }
    },
    "68818d7eea5849afa82c3003a57a5b8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a373f9cb48a481d85803f0792ff27b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a040ab99cb964c32a0a968d97dd55a16",
      "placeholder": "​",
      "style": "IPY_MODEL_9ef8c0b24cb5490b8fa7e8ea8ad619fe",
      "value": "#0: 100%"
     }
    },
    "6c3c7e4da5af40b088f3046d7698edff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "750e7a8018e6415f8cfc15288d80f13f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b80b681b46b4671a14a7cf82fd50991": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dc3dc97044c48afb06a14575fc8e91b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f940438113734ccdb198efd12b8a3936",
      "placeholder": "​",
      "style": "IPY_MODEL_c5b567cd34b340dea77cefd40e9aaec9",
      "value": " 66/66 [23:07&lt;00:00, 15.32s/ba]"
     }
    },
    "7fc619affab644758cfea5857adb7c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fc9c87c6cc44cdf82ea4f74c0bfe7b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8104b020ba8b498591e14a51af34306d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83abc2e663e447e7af2f826eacd155c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86b094a2e5564e4d8af8693dab674961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87c26a64111847f3a6cdb2f0797ae1f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb1fb197045a44ab8c204a8bdc3c4584",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4836cff910304df894759890642ac333",
      "value": 2
     }
    },
    "8aa4db7376e3445089af950c446aab70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ce95fb7cd2f4cf49b54dfa319f1e835": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83abc2e663e447e7af2f826eacd155c7",
      "placeholder": "​",
      "style": "IPY_MODEL_8104b020ba8b498591e14a51af34306d",
      "value": "100%"
     }
    },
    "902bd6c12ea54039b8c913a7c1782ff4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "935752d8733c41fbb25ed436606352ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f6183fb8ca348e0ac35b495681c705a",
      "placeholder": "​",
      "style": "IPY_MODEL_bdf2cb9d10cc4b2aab466a9dfe7be5bc",
      "value": " 6/6 [02:04&lt;00:00, 20.17s/ba]"
     }
    },
    "94da3d6dfccc4a289c659b94bcc90658": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c9b27a0b2ac4be3b8dcbb65e23b84a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_065d96b02c7840019af97e3c135693a8",
      "placeholder": "​",
      "style": "IPY_MODEL_45f8bfb31705441c80a22dbb955dd933",
      "value": " 6/6 [02:03&lt;00:00, 20.04s/ba]"
     }
    },
    "9ef8c0b24cb5490b8fa7e8ea8ad619fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a040ab99cb964c32a0a968d97dd55a16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acac4d071be4488fbb272df63b625213": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0be384a887847e981868df7c551d3de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12c3fd408f914038acb511c4ef57f965",
      "max": 66,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_540c7fcbf3b14e538cba18329bc1fc20",
      "value": 66
     }
    },
    "b23174d38e3940718252770bd39177f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bce02c83fbc142c5bb2d8c1f905a478d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd9cb96d5a3c4400bb7d145590540a54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bdf2cb9d10cc4b2aab466a9dfe7be5bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c069967001a542f5b0c3b88ff1841eba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f46e4c24b9294ffdb25e1e64bc594eee",
       "IPY_MODEL_14a450c5594148c3a4ca340521d167ba",
       "IPY_MODEL_9c9b27a0b2ac4be3b8dcbb65e23b84a2"
      ],
      "layout": "IPY_MODEL_c99472b37d644d63a471b65ecbe3bfbd"
     }
    },
    "c5b567cd34b340dea77cefd40e9aaec9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c99472b37d644d63a471b65ecbe3bfbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb1fb197045a44ab8c204a8bdc3c4584": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbed771e77314789bf8817404e5d6f67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b80b681b46b4671a14a7cf82fd50991",
      "placeholder": "​",
      "style": "IPY_MODEL_bce02c83fbc142c5bb2d8c1f905a478d",
      "value": "#1: 100%"
     }
    },
    "cf2b519cc5ff446dae45afa2439c840e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7c6178f42ed4b9f8c5169c21783ce2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dce089e985634a67a381bc85cbaca017": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd4d3c6663db46c9934c0d1f7a128d4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8eebd19f57348669da4353e690a64eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ce95fb7cd2f4cf49b54dfa319f1e835",
       "IPY_MODEL_0633918b582f45c38356130338ccfb58",
       "IPY_MODEL_4f2a3658a0f14f66a59c14d36558c7e6"
      ],
      "layout": "IPY_MODEL_f2758827907544acaa282673eebff9d0"
     }
    },
    "e9ca9f63982744aba53e883882373160": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f10632530a1b499ba3d20f0286446bab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2758827907544acaa282673eebff9d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f46e4c24b9294ffdb25e1e64bc594eee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9ca9f63982744aba53e883882373160",
      "placeholder": "​",
      "style": "IPY_MODEL_4f80ae88d884407ab8f9e08aa58632a4",
      "value": "#1: 100%"
     }
    },
    "f940438113734ccdb198efd12b8a3936": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc7073fb093543fc990abc620348c1ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
