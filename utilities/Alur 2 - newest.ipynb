{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhsDF_5UdqLI"
   },
   "source": [
    "# Menjalankan QA Tanpa Intermediate Task - Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKm8qASDXqR-"
   },
   "source": [
    "# Import semua module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DH9DifnXp5_",
    "outputId": "94161dcf-e635-486d-c6d8-5747b267b64f"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install transformers\n",
    "#!pip install tensorboard\n",
    "#!pip install evaluate\n",
    "#!pip install git+https://github.com/IndoNLP/nusa-crowd.git@release_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (23.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting nusacrowd@ git+https://github.com/IndoNLP/nusa-crowd.git@7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Cloning https://github.com/IndoNLP/nusa-crowd.git (to revision 7748513d20331e72f9969f94f5d43c7f2d4a59a5) to /tmp/pip-install-rvvers_w/nusacrowd_14e056bd964e463490bec2746ed6da9d\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/IndoNLP/nusa-crowd.git /tmp/pip-install-rvvers_w/nusacrowd_14e056bd964e463490bec2746ed6da9d\n",
      "  Running command git rev-parse -q --verify 'sha^7748513d20331e72f9969f94f5d43c7f2d4a59a5'\n",
      "  Running command git fetch -q https://github.com/IndoNLP/nusa-crowd.git 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Running command git checkout -q 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Resolved https://github.com/IndoNLP/nusa-crowd.git to commit 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: aiohttp==3.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (3.8.1)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: appdirs==1.4.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: async-timeout==4.0.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (4.0.2)\n",
      "Requirement already satisfied: attrs==22.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (22.2.0)\n",
      "Requirement already satisfied: audioread==3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: bioc==1.3.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (1.3.7)\n",
      "Requirement already satisfied: black==22.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (22.12.0)\n",
      "Requirement already satisfied: cachetools==5.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (5.3.0)\n",
      "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (2022.12.7)\n",
      "Requirement already satisfied: cffi==1.15.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (1.15.1)\n",
      "Requirement already satisfied: cfgv==3.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (3.3.1)\n",
      "Requirement already satisfied: charset-normalizer==2.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 14)) (2.1.1)\n",
      "Requirement already satisfied: click==8.1.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (8.1.3)\n",
      "Requirement already satisfied: colorama==0.4.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: conllu==4.5.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 17)) (4.5.2)\n",
      "Requirement already satisfied: datasets==2.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 18)) (2.2.0)\n",
      "Requirement already satisfied: decorator==5.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 19)) (5.1.1)\n",
      "Requirement already satisfied: dill==0.3.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 20)) (0.3.6)\n",
      "Requirement already satisfied: distlib==0.3.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 21)) (0.3.6)\n",
      "Requirement already satisfied: docutils==0.19 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 22)) (0.19)\n",
      "Requirement already satisfied: et-xmlfile==1.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 23)) (1.1.0)\n",
      "Requirement already satisfied: evaluate==0.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 24)) (0.4.0)\n",
      "Requirement already satisfied: ffmpeg==1.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 25)) (1.4)\n",
      "Requirement already satisfied: filelock==3.9.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 26)) (3.9.0)\n",
      "Requirement already satisfied: flake8==6.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 27)) (6.0.0)\n",
      "Requirement already satisfied: frozenlist==1.3.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 28)) (1.3.3)\n",
      "Requirement already satisfied: fsspec==2023.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 29)) (2023.1.0)\n",
      "Requirement already satisfied: git-lfs==1.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 30)) (1.6)\n",
      "Requirement already satisfied: google-auth==2.16.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 31)) (2.16.1)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 32)) (0.4.6)\n",
      "Requirement already satisfied: grpcio==1.51.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 33)) (1.51.1)\n",
      "Requirement already satisfied: huggingface-hub==0.12.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 34)) (0.12.1)\n",
      "Requirement already satisfied: identify==2.5.18 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 35)) (2.5.18)\n",
      "Requirement already satisfied: idna==3.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 36)) (3.4)\n",
      "Requirement already satisfied: importlib-metadata==6.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 37)) (6.0.0)\n",
      "Requirement already satisfied: isort==5.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 38)) (5.12.0)\n",
      "Requirement already satisfied: joblib==1.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 39)) (1.2.0)\n",
      "Requirement already satisfied: jsonlines==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 40)) (3.1.0)\n",
      "Requirement already satisfied: librosa==0.9.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 41)) (0.9.2)\n",
      "Requirement already satisfied: llvmlite==0.39.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 42)) (0.39.1)\n",
      "Requirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 43)) (0.5.3)\n",
      "Requirement already satisfied: lxml==4.9.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 44)) (4.9.2)\n",
      "Requirement already satisfied: Markdown==3.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 45)) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe==2.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 46)) (2.1.2)\n",
      "Requirement already satisfied: mccabe==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 47)) (0.7.0)\n",
      "Requirement already satisfied: multidict==6.0.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 48)) (6.0.4)\n",
      "Requirement already satisfied: multiprocess==0.70.14 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 49)) (0.70.14)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 50)) (1.0.0)\n",
      "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 51)) (3.8.1)\n",
      "Requirement already satisfied: nodeenv==1.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 52)) (1.7.0)\n",
      "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 53)) (0.56.4)\n",
      "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 54)) (1.23.5)\n",
      "Requirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 56)) (3.2.2)\n",
      "Requirement already satisfied: openpyxl==3.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 57)) (3.1.1)\n",
      "Requirement already satisfied: packaging==23.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 58)) (23.0)\n",
      "Requirement already satisfied: pandas==1.3.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 59)) (1.3.3)\n",
      "Requirement already satisfied: pathspec==0.11.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 60)) (0.11.0)\n",
      "Requirement already satisfied: platformdirs==3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 61)) (3.0.0)\n",
      "Requirement already satisfied: pooch==1.6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 62)) (1.6.0)\n",
      "Requirement already satisfied: pre-commit==2.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 63)) (2.19.0)\n",
      "Requirement already satisfied: protobuf==4.22.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 64)) (4.22.0)\n",
      "Requirement already satisfied: pyarrow==11.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 65)) (11.0.0)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 66)) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 67)) (0.2.8)\n",
      "Requirement already satisfied: pycodestyle==2.10.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 68)) (2.10.0)\n",
      "Requirement already satisfied: pycparser==2.21 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 69)) (2.21)\n",
      "Requirement already satisfied: pyflakes==3.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 70)) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 71)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2022.7.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 72)) (2022.7.1)\n",
      "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 73)) (6.0)\n",
      "Requirement already satisfied: regex==2022.10.31 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 74)) (2022.10.31)\n",
      "Requirement already satisfied: requests==2.28.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 75)) (2.28.2)\n",
      "Requirement already satisfied: requests-oauthlib==1.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 76)) (1.3.1)\n",
      "Requirement already satisfied: resampy==0.4.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 77)) (0.4.2)\n",
      "Requirement already satisfied: responses==0.18.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 78)) (0.18.0)\n",
      "Requirement already satisfied: rsa==4.9 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 79)) (4.9)\n",
      "Requirement already satisfied: scikit-learn==1.2.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 80)) (1.2.1)\n",
      "Requirement already satisfied: scipy==1.10.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 81)) (1.10.0)\n",
      "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 82)) (1.16.0)\n",
      "Requirement already satisfied: soundfile==0.12.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 83)) (0.12.1)\n",
      "Requirement already satisfied: tensorboard==2.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 84)) (2.12.0)\n",
      "Requirement already satisfied: tensorboard-data-server==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 85)) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 86)) (1.8.1)\n",
      "Requirement already satisfied: termcolor==2.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 87)) (2.2.0)\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 88)) (3.1.0)\n",
      "Requirement already satisfied: tokenizers==0.13.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 89)) (0.13.2)\n",
      "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 90)) (0.10.2)\n",
      "Requirement already satisfied: tomli==2.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 91)) (2.0.1)\n",
      "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 92)) (1.13.1)\n",
      "Requirement already satisfied: torchaudio==0.13.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 93)) (0.13.1)\n",
      "Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 94)) (4.64.1)\n",
      "Requirement already satisfied: transformers==4.26.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 95)) (4.26.1)\n",
      "Requirement already satisfied: translate-toolkit==3.8.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 96)) (3.8.4)\n",
      "Requirement already satisfied: typing_extensions==4.5.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 97)) (4.5.0)\n",
      "Requirement already satisfied: urllib3==1.26.14 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 98)) (1.26.14)\n",
      "Requirement already satisfied: virtualenv==20.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 99)) (20.19.0)\n",
      "Requirement already satisfied: Werkzeug==2.2.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 100)) (2.2.3)\n",
      "Requirement already satisfied: wget==3.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 101)) (3.2)\n",
      "Requirement already satisfied: win32-setctime==1.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 102)) (1.1.0)\n",
      "Requirement already satisfied: xxhash==3.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 103)) (3.2.0)\n",
      "Requirement already satisfied: yarl==1.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 104)) (1.8.2)\n",
      "Requirement already satisfied: zipp==3.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 105)) (3.14.0)\n",
      "Requirement already satisfied: zstandard==0.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 106)) (0.19.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nodeenv==1.7.0->-r requirements.txt (line 52)) (45.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard==2.12.0->-r requirements.txt (line 84)) (0.34.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (8.5.0.96)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 26 08:55:53 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    56W / 300W |  32476MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   58C    P0   187W / 300W |  32292MiB / 32480MiB |     98%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   54C    P0   259W / 300W |  12012MiB / 32480MiB |     97%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    56W / 300W |   9210MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    58W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    57W / 300W |   9364MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    56W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    56W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Melihat GPU yang tersedia dan penggunaannya.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memilih GPU yang akan digunakan (contohnya: GPU #7)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pgY_FL4lXuEu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "import operator\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from evaluate import load\n",
    "from nusacrowd import NusantaraConfigHelper\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from huggingface_hub import notebook_login\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    load_from_disk,\n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "from transformers import (\n",
    "    BigBirdTokenizerFast,\n",
    "    BigBirdForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BertForSequenceClassification,\n",
    "    BertForQuestionAnswering,\n",
    "    AutoModel, \n",
    "    BertTokenizerFast,\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    BertTokenizer, \n",
    "    BertForPreTraining,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    EvalPrediction,\n",
    "    AutoModel,\n",
    "    BertModel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtu0yFqMYsX9"
   },
   "source": [
    "# Definisikan hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gjHJwpxeYugs"
   },
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"indolem/indobert-base-uncased\"\n",
    "#MODEL_NAME = \"afaji/fine-tuned-IndoNLI-Translated-with-indobert-base-uncased\"\n",
    "MODEL_NAME = \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\"\n",
    "#MODEL_NAME = \"indobenchmark/indobert-large-p2\"\n",
    "SEED = 42\n",
    "EPOCH = 1\n",
    "BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_LENGTH = 400\n",
    "STRIDE = 100\n",
    "LOGGING_STEPS = 50\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAXIMUM_SEARCH_ITER =  2\n",
    "# Untuk mempercepat training, saya ubah SAMPLE menjadi 100.\n",
    "# Bila mau menggunakan keseluruhan data, gunakan: \n",
    "SAMPLE = sys.maxsize\n",
    "# SAMPLE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyH7dlPGYOSk"
   },
   "source": [
    "# Import dataset QAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/IndoNLP/nusa-crowd.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==1.7.0 torchvision==0.8.1 -f https://download.pytorch.org/whl/cu101/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#from tensorflow.python.platform import build_info as build\n",
    "#print(f\"tensorflow version: {tf.__version__}\")\n",
    "#print(f\"Cuda Version: {build.build_info['cuda_version']}\")\n",
    "#print(f\"Cudnn version: {build.build_info['cudnn_version']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\r\n",
      "Built on Mon_Sep_13_19:13:29_PDT_2021\r\n",
      "Cuda compilation tools, release 11.5, V11.5.50\r\n",
      "Build cuda_11.5.r11.5/compiler.30411180_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall torch\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 26 08:55:57 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    57W / 300W |  32476MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   58C    P0   222W / 300W |  32248MiB / 32480MiB |     99%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   52C    P0    79W / 300W |  12012MiB / 32480MiB |     95%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    56W / 300W |   9210MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    58W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    57W / 300W |   9364MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    56W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    56W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cudatoolkit==11.5 (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cudatoolkit==11.5\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install cudatoolkit==11.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conda in /usr/local/lib/python3.8/dist-packages (4.3.16)\n",
      "Requirement already satisfied: pycosat>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from conda) (0.6.3)\n",
      "Requirement already satisfied: requests>=2.12.4 in /usr/local/lib/python3.8/dist-packages (from conda) (2.28.2)\n",
      "Requirement already satisfied: ruamel.yaml>=0.11.14 in /usr/local/lib/python3.8/dist-packages (from conda) (0.17.21)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.12.4->conda) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.12.4->conda) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.12.4->conda) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.12.4->conda) (1.26.14)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/local/lib/python3.8/dist-packages (from ruamel.yaml>=0.11.14->conda) (0.2.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: The install method you used for conda--probably either `pip install conda`\r\n",
      "or `easy_install conda`--is not compatible with using conda as an application.\r\n",
      "If your intention is to install conda as a standalone application, currently\r\n",
      "supported install methods include the Anaconda installer and the miniconda\r\n",
      "installer.  You can download the miniconda installer from\r\n",
      "https://conda.io/miniconda.html.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda install cudatoolkit=11.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch._C._cuda_getDeviceCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlNhPlmDY2tk"
   },
   "source": [
    "# Definisikan tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "z3uXWOkUY4GD"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYYbFXzWYTr3"
   },
   "source": [
    "# Definisikan fungsi pre-processnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset ty_di_qa_id_dataset (/root/.cache/huggingface/datasets/ty_di_qa_id_dataset/tydiqa_id_source/1.0.0/77be91de4a88147f2b8ff9e6c1d376d01d26f9b51dd36f775a5607518c034111)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c6b24deb004c7586f69dd1e86e8d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conhelps = NusantaraConfigHelper()\n",
    "data_qas_id = conhelps.filtered(lambda x: 'tydiqa_id' in x.dataset_name)[0].load_dataset()\n",
    "\n",
    "df_train = pd.DataFrame(data_qas_id['train'])\n",
    "df_validation = pd.DataFrame(data_qas_id['validation'])\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_train = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in range(len(df_train['context'])):\n",
    "    answer_start = df_train['context'][i].index(df_train['label'][i])\n",
    "    answer_end = answer_start + len(df_train['label'][i])\n",
    "    new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                        'question': df_train[\"question\"][i], \n",
    "                                        'answer': {\"text\": df_train[\"label\"][i], \n",
    "                                                   \"answer_start\": answer_start, \n",
    "                                                   \"answer_end\": answer_end}}, \n",
    "                                                   ignore_index=True)\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_val = pd.DataFrame(columns=cols)    \n",
    "\n",
    "for i in range(len(df_validation['context'])):\n",
    "    answer_start = df_validation['context'][i].index(df_validation['label'][i])\n",
    "    answer_end = answer_start + len(df_validation['label'][i])\n",
    "    new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                    'question': df_validation[\"question\"][i], \n",
    "                                    'answer': {\"text\": df_validation[\"label\"][i], \n",
    "                                               \"answer_start\": answer_start, \n",
    "                                               \"answer_end\": answer_end}}, \n",
    "                                               ignore_index=True)    \n",
    "\n",
    "train_dataset = Dataset.from_dict(new_df_train)\n",
    "validation_dataset = Dataset.from_dict(new_df_val)\n",
    "\n",
    "data_qas_id = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "EuSjYelvXJ9x"
   },
   "outputs": [],
   "source": [
    "def rindex(lst, value, operator=operator):\n",
    "      return len(lst) - operator.indexOf(reversed(lst), value) - 1\n",
    "\n",
    "def preprocess_function_qa(examples, tokenizer, MAX_LENGTH=MAX_LENGTH, STRIDE=STRIDE, rindex=rindex, operator=operator):\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    examples[\"context\"] = [c.lstrip() for c in examples[\"context\"]]\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "      examples['question'],\n",
    "      examples['context'],\n",
    "      truncation=True,\n",
    "      max_length = MAX_LENGTH,\n",
    "      stride=STRIDE,\n",
    "      return_overflowing_tokens=True,\n",
    "      return_offsets_mapping=True,\n",
    "      padding=\"max_length\",\n",
    "      return_tensors='np'\n",
    "    )\n",
    "\n",
    "    tokenized_examples['start_positions'] = []\n",
    "    tokenized_examples['end_positions'] = []\n",
    "\n",
    "    for seq_idx in range(len(tokenized_examples['input_ids'])):\n",
    "        seq_ids = tokenized_examples.sequence_ids(seq_idx)\n",
    "        offset_mappings = tokenized_examples['offset_mapping'][seq_idx]\n",
    "\n",
    "        cur_example_idx = tokenized_examples['overflow_to_sample_mapping'][seq_idx]\n",
    "\n",
    "        #answer = examples['answer'][seq_idx][0]\n",
    "        answer = examples['answer'][cur_example_idx]\n",
    "        answer = eval(str(answer))\n",
    "        #answer_text = answer['text'][0]\n",
    "        answer_start = answer['answer_start']\n",
    "        #answer_end = answer_start + len(answer_text)\n",
    "        answer_end = answer['answer_end']\n",
    "\n",
    "        context_pos_start = seq_ids.index(1)\n",
    "        context_pos_end = rindex(seq_ids, 1, operator)\n",
    "\n",
    "        s = e = 0\n",
    "        if (offset_mappings[context_pos_start][0] <= answer_start and\n",
    "            offset_mappings[context_pos_end][1] >= answer_end):\n",
    "          i = context_pos_start\n",
    "          while offset_mappings[i][0] < answer_start:\n",
    "            i += 1\n",
    "          if offset_mappings[i][0] == answer_start:\n",
    "            s = i\n",
    "          else:\n",
    "            s = i - 1\n",
    "\n",
    "          j = context_pos_end\n",
    "          while offset_mappings[j][1] > answer_end:\n",
    "            j -= 1      \n",
    "          if offset_mappings[j][1] == answer_end:\n",
    "            e = j\n",
    "          else:\n",
    "            e = j + 1\n",
    "\n",
    "        tokenized_examples['start_positions'].append(s)\n",
    "        tokenized_examples['end_positions'].append(e)\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0o9vkQ1YaO2"
   },
   "source": [
    "# Mulai tokenisasi dan pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "2721b15477d741519a3041a8f256575b",
      "6a373f9cb48a481d85803f0792ff27b3",
      "54a2637e8c8e4ca0917e35f4b58b48f3",
      "7dc3dc97044c48afb06a14575fc8e91b",
      "6c3c7e4da5af40b088f3046d7698edff",
      "a040ab99cb964c32a0a968d97dd55a16",
      "9ef8c0b24cb5490b8fa7e8ea8ad619fe",
      "dce089e985634a67a381bc85cbaca017",
      "bd9cb96d5a3c4400bb7d145590540a54",
      "f940438113734ccdb198efd12b8a3936",
      "c5b567cd34b340dea77cefd40e9aaec9",
      "6516c5b02aa24db2bc42cdd4d1f2c260",
      "cbed771e77314789bf8817404e5d6f67",
      "b0be384a887847e981868df7c551d3de",
      "10fb50db270449b48382c815904245a8",
      "b23174d38e3940718252770bd39177f8",
      "7b80b681b46b4671a14a7cf82fd50991",
      "bce02c83fbc142c5bb2d8c1f905a478d",
      "12c3fd408f914038acb511c4ef57f965",
      "540c7fcbf3b14e538cba18329bc1fc20",
      "fc7073fb093543fc990abc620348c1ca",
      "acac4d071be4488fbb272df63b625213",
      "29b13cbb638947d5a17071d299e2420f",
      "113ae282ca5a499a8a2ad36a796e763f",
      "0ce874e529474f478a7acd45b4ffd649",
      "935752d8733c41fbb25ed436606352ca",
      "cf2b519cc5ff446dae45afa2439c840e",
      "f10632530a1b499ba3d20f0286446bab",
      "94da3d6dfccc4a289c659b94bcc90658",
      "902bd6c12ea54039b8c913a7c1782ff4",
      "1d86de7f1fbf452892fe6d4b177558f9",
      "3f6183fb8ca348e0ac35b495681c705a",
      "bdf2cb9d10cc4b2aab466a9dfe7be5bc",
      "c069967001a542f5b0c3b88ff1841eba",
      "f46e4c24b9294ffdb25e1e64bc594eee",
      "14a450c5594148c3a4ca340521d167ba",
      "9c9b27a0b2ac4be3b8dcbb65e23b84a2",
      "c99472b37d644d63a471b65ecbe3bfbd",
      "e9ca9f63982744aba53e883882373160",
      "4f80ae88d884407ab8f9e08aa58632a4",
      "13ea541bb13b42738e74885a2c1db8df",
      "68818d7eea5849afa82c3003a57a5b8d",
      "065d96b02c7840019af97e3c135693a8",
      "45f8bfb31705441c80a22dbb955dd933"
     ]
    },
    "id": "uVGfobd8XNIF",
    "outputId": "9b911f15-dce5-456f-a0fd-446f9615be1c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'fn_kwargs'={'tokenizer': BertTokenizerFast(name_or_path='afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05', vocab_size=31923, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), 'MAX_LENGTH': 400, 'STRIDE': 100, 'rindex': <function rindex at 0x7f29f97fdb80>, 'operator': <module 'operator' from '/usr/lib/python3.8/operator.py'>} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691abc917f1d4e8183af872de865b5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0419077ed86240f3ab2ca4f698ef8419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data_qas_id = data_qas_id.map(\n",
    "    preprocess_function_qa,\n",
    "    batched=True,\n",
    "    remove_columns=data_qas_id['train'].column_names,\n",
    "    num_proc=1,\n",
    "    fn_kwargs={'tokenizer': tokenizer, 'MAX_LENGTH': MAX_LENGTH, 'STRIDE': STRIDE, 'rindex': rindex, 'operator': operator}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_qas_id = tokenized_data_qas_id.remove_columns([\"offset_mapping\", \n",
    "                                            \"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Fk1RbGEeldvi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_data_qas_id.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "SLfcGWQEjd36"
   },
   "outputs": [],
   "source": [
    "tokenized_data_qas_id_train = Dataset.from_dict(tokenized_data_qas_id[\"train\"][:SAMPLE])\n",
    "tokenized_data_qas_id_validation = Dataset.from_dict(tokenized_data_qas_id[\"validation\"][:SAMPLE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06_Y5Xw9bLj3"
   },
   "source": [
    "# Mendefinisikan argumen (dataops) untuk training nanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "foTQgHuujf46"
   },
   "outputs": [],
   "source": [
    "TIME_NOW = str(datetime.now()).replace(\":\", \"-\").replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "QA = './results/alur2-idk-mrc'\n",
    "CHECKPOINT_DIR = f'{QA}-{TIME_NOW}/checkpoint/'\n",
    "MODEL_DIR = f'{QA}-{TIME_NOW}/model/'\n",
    "OUTPUT_DIR = f'{QA}-{TIME_NOW}/output/'\n",
    "ACCURACY_DIR = f'{QA}-{TIME_NOW}/accuracy/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9YdVKNRbPHk"
   },
   "source": [
    "# Mendefinisikan Training Arguments untuk train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3VMs4pBjgXE",
    "outputId": "577270d3-fe73-44be-f1c2-e710bb5e1d1b"
   },
   "outputs": [],
   "source": [
    "training_args_qa = TrainingArguments(\n",
    "    \n",
    "    # Checkpoint\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=EPOCH,\n",
    "    \n",
    "    # Log\n",
    "    report_to='tensorboard',\n",
    "    logging_strategy='steps',\n",
    "    logging_first_step=True,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    \n",
    "    # Train\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    bf16=False,\n",
    "    dataloader_num_workers=cpu_count(),\n",
    "    \n",
    "    # Miscellaneous\n",
    "    evaluation_strategy='epoch',\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INibCqT5bR3d"
   },
   "source": [
    "# Pendefinisian model Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqTEhdqqtL4z",
    "outputId": "5b4394f3-63f5-4610-e72d-d383d9b8940a"
   },
   "outputs": [],
   "source": [
    "model_qa = BertForQuestionAnswering.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qa = model_qa.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyJvhxZ6bbbF"
   },
   "source": [
    "# Melakukan pengumpulan data dengan padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "rGeJNonStSXT"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nF_E_CHbb8m"
   },
   "source": [
    "# Mulai training untuk fine-tune SQUAD diatas IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import collections\n",
    "\n",
    "# # Melakukan evaluasi dari prediksi\n",
    "def normalize_text(s):\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_f1_prec_rec(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split() # True positive + False positive = Untuk precision\n",
    "    gold_tokens = normalize_text(gold).split() # True positive + False negatives = Untuk recall\n",
    "    common = collections.Counter(pred_tokens) & collections.Counter(gold_tokens)\n",
    "    num_same = sum(common.values()) # True positive\n",
    "\n",
    "    if len(gold_tokens) == 0 or len(pred_tokens) == 0: \n",
    "        return int(gold_tokens == pred_tokens)\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(pred_tokens)\n",
    "    recall = 1.0 * num_same / len(gold_tokens)\n",
    "    f1 = (2.0 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1\n",
    "\n",
    "def compute_metrics(predict_result):\n",
    "    predictions_idx = np.argmax(predict_result.predictions, axis=2)\n",
    "    denominator = len(predictions_idx[0])\n",
    "    label_array = np.asarray(predict_result.label_ids)\n",
    "    total_correct = 0\n",
    "    f1_array = []\n",
    "    precision_array = []\n",
    "    recall_array = []\n",
    "\n",
    "    for i in range(len(predict_result.predictions[0])):\n",
    "        start_pred_idx = predictions_idx[0][i]\n",
    "        end_pred_idx = predictions_idx[1][i] + 1\n",
    "        start_gold_idx = label_array[0][i]\n",
    "        end_gold_idx = label_array[1][i] + 1\n",
    "\n",
    "        pred_text = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                    [start_pred_idx: end_pred_idx])\n",
    "        gold_text = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                    [start_gold_idx: end_gold_idx])\n",
    "\n",
    "        if pred_text == gold_text:\n",
    "            total_correct += 1\n",
    "\n",
    "        f1 = compute_f1_prec_rec(pred=pred_text, gold=gold_text)\n",
    "\n",
    "        f1_array.append(f1)\n",
    "\n",
    "    exact_match = ((total_correct / denominator) * 100.0)\n",
    "    final_f1 = np.mean(f1_array) * 100.0\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': final_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "G6pLmNzCjhra"
   },
   "outputs": [],
   "source": [
    "trainer_qa = Trainer(\n",
    "    model=model_qa,\n",
    "    args=training_args_qa,\n",
    "    #train_dataset=tokenized_data_qas_id_train,\n",
    "    #eval_dataset=tokenized_data_qas_id_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    #data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qp96lB8vjtXG",
    "outputId": "b118d7ac-a90d-43c2-bd8b-ee0a0c4ab8de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainer_qa.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DreIeglDbrHY"
   },
   "source": [
    "# Menyimpan model Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "KJick7YDjuoq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/alur2-idk-mrc-2023-03-26_08-56-55_690028/model/\n",
      "Configuration saved in ./results/alur2-idk-mrc-2023-03-26_08-56-55_690028/model/config.json\n",
      "Model weights saved in ./results/alur2-idk-mrc-2023-03-26_08-56-55_690028/model/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/alur2-idk-mrc-2023-03-26_08-56-55_690028/model/tokenizer_config.json\n",
      "Special tokens file saved in ./results/alur2-idk-mrc-2023-03-26_08-56-55_690028/model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer_qa.save_model(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX-y8EbobugF"
   },
   "source": [
    "# Melakukan prediksi dari model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "6aE7w2yMkWxj",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 573\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=(array([[-8.351709 , -8.076598 , -8.503143 , ..., -9.282152 , -8.230424 ,\n",
       "        -8.230472 ],\n",
       "       [-8.80134  , -7.8416004, -8.878553 , ..., -8.203246 , -8.023994 ,\n",
       "        -7.9551816],\n",
       "       [-8.406947 , -7.8792405, -7.5575123, ..., -8.06922  , -8.540083 ,\n",
       "        -8.542109 ],\n",
       "       ...,\n",
       "       [-8.366324 , -7.5886245, -7.8696113, ..., -9.02594  , -8.910944 ,\n",
       "        -8.83718  ],\n",
       "       [-7.473398 , -8.006924 , -8.180971 , ..., -8.620615 , -8.630538 ,\n",
       "        -8.637638 ],\n",
       "       [-8.5171175, -7.361905 , -8.587641 , ..., -9.035261 , -9.054317 ,\n",
       "        -9.0132675]], dtype=float32), array([[-8.092256 , -8.070768 , -8.353189 , ..., -8.548285 , -7.212416 ,\n",
       "        -7.212634 ],\n",
       "       [-8.535385 , -8.40173  , -8.309284 , ..., -7.9303446, -7.900365 ,\n",
       "        -7.886168 ],\n",
       "       [-7.968836 , -7.8870463, -7.9419675, ..., -7.099452 , -8.311144 ,\n",
       "        -8.295948 ],\n",
       "       ...,\n",
       "       [-8.283798 , -7.889635 , -8.555798 , ..., -8.739616 , -8.661089 ,\n",
       "        -8.633037 ],\n",
       "       [-7.4877176, -7.848751 , -7.7136855, ..., -7.8016624, -7.7560544,\n",
       "        -7.7050633],\n",
       "       [-8.070554 , -7.40412  , -7.3328032, ..., -8.413584 , -8.402795 ,\n",
       "        -8.31439  ]], dtype=float32)), label_ids=(array([ 43,  21,  18,  26,  43,  22,  11,  30,  73,  54,  14,  17,  12,\n",
       "        39,  48, 178,  47,  14,  38,  25,   8,  42,  56,  20,  21,  23,\n",
       "        46, 101,  11,  21,  78,  29,  15,  15,  11,  36,  19,  21,  12,\n",
       "        78,  13,  37,  11,  43,  21,  34,  38,   9,  11,   8,  53,  24,\n",
       "        35,  15,  14,  15,   8, 222,  20,  24, 125,  25,  13,  29,  24,\n",
       "        28, 110,  89,  16,  23,  38,  17,  10, 230,  19, 223,  26,  72,\n",
       "        44,  31,  33,  19,  41,  27,  91,  26,  30,  13, 151,  18,  12,\n",
       "        50,  43,  27,  56,  58,  13,  18,  59, 222, 223,  42,  77,  30,\n",
       "        76,  83,  88,  35,  34,  45,  18,  41,  39,  59,  73,  10,  17,\n",
       "        76,  94,   6,  22,   7,  25,  12,  16,  71, 102,  66,  50,  10,\n",
       "       179,  66,  17, 126,  44,  40,  16,  13,  35,  57,  36,  32,   8,\n",
       "       106,   8,  12,  91,  55,  55,  22,  16,   8,  87,  20,  49,  10,\n",
       "        74,  15,  54,  21,  57,  13,  23,   9,  14,  14,   9, 150,  46,\n",
       "        31,  35,  25,  89,  45,  24,  12,  29, 110,  64,  59,  11,  18,\n",
       "        12,  80,  10,  32,  33,  11,  51, 106,  15,  16,  10,  10,  10,\n",
       "         8,  86,   8,  34,  14,  12,  71,  95,  33,  10,  32,  10,  23,\n",
       "        93,  12,  34,  42,  15,  18,  32,   8,  48,  76,  23,   9,  89,\n",
       "        15, 106, 162,  20,   0, 116,  28,  49,  13, 187,  11,  54,  41,\n",
       "        16,  23,  23,  78,  34,  21,  33,  13,  73,  35,  19,  55,   9,\n",
       "        12,  15,  74,  13,  41,  36,  42,  16,   9,  58,   9, 165,  27,\n",
       "        27, 168,  24,  42,  45,  67,  65,  24,  16,  39,  61,  10,  19,\n",
       "        76,  42,   9,  55,  89,  11,  40,  51,  23,  24,  15,  11,   9,\n",
       "        18,  26,  12,  13,  82,  26,  13,  69,  10,  39,  14,  18,  49,\n",
       "        40,  51,  49,  13, 144,  78,  82,  35,  57, 106,  59,  10,  63,\n",
       "        73,  24,   8,  23, 113,  45,  21,  10,  26,  14,   8,  68,  22,\n",
       "        13,  27,   0, 179,   0,   0,  41,  26,  22,  45,  23, 116,  29,\n",
       "        55,  14,  11,  20,  34,  14,  17,  54,  34,  14,  10,  75,  15,\n",
       "        24,  89,  78,  41, 112,  51,  23,  73,  43,  13,  39,  16,  17,\n",
       "        18,  65,  52,  15,  17,  46,  84,  74,  76,   9,  13,  39,  87,\n",
       "        25,  22,  53, 108,  18,  74,  70,  43,  35,  15,  35,  31,  51,\n",
       "        28,  10,  51, 127,  78, 203,  25,  24,  51,  45,  64,  12, 106,\n",
       "        14,  10,  11,  49,  28,  12,  23,  10,  21, 112,  16,  72,   9,\n",
       "       114,  13,  20,  61,   9,  20,  60,  13,  47, 113,  16,   9,  36,\n",
       "        13,  72,  62,  56, 108,  19,  52,  22,  27,  11,  41,  42,  51,\n",
       "        35, 109,  11,  11,   9,  10,  32,  10,  16,  20, 158,  81,   9,\n",
       "        12,  23,   9,  59,  16,  29,  23,  74,  12,  20,  30,  18,  37,\n",
       "         8,  22, 133,  90,  11,  52,   8,  10,  93,  17,  52,  15,  43,\n",
       "        14,   0,  20,  24,  12,  10,  58,  31,  13,  28,  10,  14,  10,\n",
       "         8,  13,  43,  35,  32,  61,  37,  23,  12,  23,   9,  14,  25,\n",
       "        12,  30,  48,  28,  54,  15,  36,  52,  44,  29,   8,  27,  25,\n",
       "        27,  17,  18,  19,   9,  14,  45,  48,  15,  16,   7,  11,  48,\n",
       "       139,   9,  39,  44,  12,  92,  50,  24, 110,   0,  28,  10,  50,\n",
       "        21,  90,  20, 197,  16,  15,  18,  61,  26,  53,   0, 120,  16,\n",
       "        27,  50,  60,  24, 104,  25,  18,  18,   0,  47,  33,  42, 203,\n",
       "        78]), array([ 49,  25,  22,  28,  47,  30,  13,  38,  76,  58,  15,  47,  32,\n",
       "        41,  58, 202,  48,  14,  42,  31,  10,  46,  59,  21,  24,  27,\n",
       "        49, 105,  18,  24,  86,  38,  21,  20,  19,  37,  19,  22,  28,\n",
       "        83,  16,  38,  12,  65,  22,  34,  39,  12,  11,   9,  55,  25,\n",
       "        39,  27,  15,  37,  11, 223,  20,  31, 125,  25,  28,  30,  32,\n",
       "        56, 113,  93,  18,  25,  43,  24,  11, 233,  36, 232,  39,  74,\n",
       "        53,  34,  56,  19,  41,  30,  92,  36,  39,  15, 155,  25,  18,\n",
       "        52,  48,  43,  60,  58,  31,  20,  61, 225, 224,  44,  77,  35,\n",
       "        89,  85,  91,  37,  36,  50,  20,  41,  51,  71,  75,  12,  24,\n",
       "        81, 105,  10,  25,  12,  52,  32,  17,  72, 105,  66,  52,  36,\n",
       "       188,  71,  18, 130,  44,  42,  18,  16,  38,  61,  45,  34,   9,\n",
       "       106,   8,  43,  92,  62,  56,  23,  16,  12,  89,  22,  57,  21,\n",
       "        82,  16,  55,  24,  69,  13,  47,  21,  19,  15,  13, 173,  61,\n",
       "        31,  40,  27,  94,  55,  24,  14,  38, 111,  67,  85,  16,  19,\n",
       "        21,  80,  10,  35,  33,  11,  52, 106,  15,  23,  16,  17,  11,\n",
       "         9,  89,   8,  56,  18,  12,  72,  96,  33,  11,  37,  12,  31,\n",
       "        95,  16,  39,  50,  34,  20,  67,  12,  49,  77,  23,  15,  90,\n",
       "        44, 107, 165,  23,   0, 119,  29,  58,  17, 190,  12,  63,  41,\n",
       "        16,  29,  42,  89,  39,  37,  36,  33,  73,  36,  24,  57,  12,\n",
       "        15,  15,  75,  14,  44,  36,  52,  18,  10,  60,  23, 195,  69,\n",
       "        27, 172,  24,  75,  45,  69,  77,  26,  18,  58,  63,  30,  19,\n",
       "        76,  47,  36,  56,  90,  12,  43,  51,  25,  26,  15,  13,  13,\n",
       "        43,  26,  19,  15,  82,  54,  13,  91,  25,  46,  20,  18,  51,\n",
       "        42,  51,  51,  35, 144,  78,  83,  38,  62, 109,  62,  11,  87,\n",
       "        99,  24,  12,  23, 114,  53,  24,  42,  81,  34,   9,  77,  23,\n",
       "        14,  54,   0, 180,   0,   0,  49,  29,  25,  47,  30, 129,  29,\n",
       "        59,  15,  25,  21,  39,  33,  35,  59,  34,  15,  10,  76,  16,\n",
       "        25,  91,  90,  41, 113,  52,  23,  85,  47,  15,  39,  22,  17,\n",
       "        18,  67,  54,  35,  24,  75,  91,  74,  81,  18,  15,  42,  88,\n",
       "        25,  22,  55, 112,  48,  74,  74,  45,  51,  17,  43,  33,  52,\n",
       "        61,  11,  53, 134,  80, 206,  27,  24,  53,  49,  64,  20, 132,\n",
       "        15,  11,  12,  55,  41,  13,  28,  18,  23, 133,  22,  78,  11,\n",
       "       116,  36,  45,  61,   9,  42,  61,  21,  56, 116,  19,  11,  61,\n",
       "        15,  72,  68,  82, 128,  21,  57,  24,  28,  13,  46,  46,  52,\n",
       "        50, 110,  34,  12,  13,  24,  32,  22,  21,  31, 162,  85,  11,\n",
       "        13,  26,  20,  59,  18,  29,  25,  78,  38,  25,  33,  31,  60,\n",
       "        16,  22, 140,  92,  31,  59,  20,  31,  96,  20,  53,  17,  46,\n",
       "        18,   0,  20,  37,  12,  15,  59,  33,  15,  29,  14,  27,  24,\n",
       "         9,  15,  56,  38,  34,  69,  51,  25,  14,  23,   9,  16,  26,\n",
       "        16,  33,  50,  32,  91,  28,  37,  54,  49,  31,   9,  29,  69,\n",
       "        32,  24,  18,  27,  12,  35,  73,  48,  33,  22,   8,  18,  53,\n",
       "       141,  26,  39,  46,  16,  97,  51,  24, 112,   0,  42,  12,  52,\n",
       "        23,  95,  20, 199,  18,  15,  19,  85,  48,  55,   0, 120,  18,\n",
       "        29,  54,  78,  29, 116,  28,  48,  22,   0,  49,  33,  42, 206,\n",
       "        79])), metrics={'test_loss': 1.1361852884292603, 'test_runtime': 66.2082, 'test_samples_per_second': 8.655, 'test_steps_per_second': 1.087})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result = trainer_qa.predict(tokenized_data_qas_id_validation)\n",
    "predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "with open(f'{OUTPUT_DIR}/output.txt', \"w\") as f:\n",
    "  f.write(str(predict_result))\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2BS4XC5byxE"
   },
   "source": [
    "# Melakukan evaluasi dari prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "9J-zKl_zkUze"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 62.129144851657934, 'f1': 76.50970983610755}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_result_before_filtering = compute_metrics(predict_result)\n",
    "metric_result_before_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(ACCURACY_DIR), exist_ok=True)\n",
    "with open(f'{ACCURACY_DIR}/accuracy.txt', \"w\") as f:\n",
    "  f.write(str(metric_result_before_filtering))\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coba Alur 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_qas_dataframe(predict_result=predict_result, index_largest=1):\n",
    "    predictions_idx = np.argsort(predict_result.predictions, axis=2)[:, :, index_largest * -1]\n",
    "    #predictions_idx = np.argmax(predict_result.predictions, axis=2)\n",
    "    label_array = np.asarray(predict_result.label_ids)\n",
    "    question_decoded = []\n",
    "    context_decoded = []\n",
    "    pred_answer_decoded = []\n",
    "    gold_answer_decoded = []\n",
    "    \n",
    "    for i in tqdm(range(len(predict_result.predictions[0]))):\n",
    "        start_pred_idx = predictions_idx[0][i]\n",
    "        end_pred_idx = predictions_idx[1][i] + 1\n",
    "        pred_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_pred_idx: end_pred_idx], skip_special_tokens=True)\n",
    "        pred_answer_decoded.append(pred_answer)\n",
    "        \n",
    "        start_gold_idx = label_array[0][i]\n",
    "        end_gold_idx = label_array[1][i] + 1\n",
    "        gold_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_gold_idx: end_gold_idx], skip_special_tokens=True)\n",
    "        gold_answer_decoded.append(gold_answer)\n",
    "        \n",
    "        question = []\n",
    "        context = []\n",
    "         \n",
    "        for j in range(len(tokenized_data_qas_id_validation[i]['token_type_ids'])):\n",
    "            if tokenized_data_qas_id_validation[i]['token_type_ids'][j] == 0:\n",
    "                question.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "            else:\n",
    "                context.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "\n",
    "        question_decoded.append(tokenizer.decode(question, skip_special_tokens=True))\n",
    "        context_decoded.append(tokenizer.decode(context, skip_special_tokens=True))\n",
    "    \n",
    "    qas_df = pd.DataFrame({'Context': context_decoded, \n",
    "                           'Question': question_decoded, \n",
    "                           'Prediction Answer': pred_answer_decoded,\n",
    "                          'Gold Answer': gold_answer_decoded})\n",
    "                      \n",
    "    return qas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 573/573 [03:48<00:00,  2.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Question</th>\n",
       "      <th>Prediction Answer</th>\n",
       "      <th>Gold Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kolumbus bukanlah orang pertama yang tiba di a...</td>\n",
       "      <td>siapakah yang menemuka benua amerika?</td>\n",
       "      <td>kolumbus</td>\n",
       "      <td>orang - orang viking dari eropa utara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kabupaten donggala ( english : donggala regenc...</td>\n",
       "      <td>dimanakah letak donggala?</td>\n",
       "      <td>provinsi sulawesi tengah, indonesia</td>\n",
       "      <td>provinsi sulawesi tengah, indonesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awal mula teknik industri dapat ditelusuri dar...</td>\n",
       "      <td>siapa bapak teknik industri?</td>\n",
       "      <td>frederick winslow taylor</td>\n",
       "      <td>frederick winslow taylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>penghulu rasyid ( lahir di desa telaga itar ta...</td>\n",
       "      <td>kapan penghulu rasyid meninggal?</td>\n",
       "      <td>15 desember 1861</td>\n",
       "      <td>15 desember 1861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>samudra pasifik atau lautan teduh ( dari bahas...</td>\n",
       "      <td>seberapa luas kah samudera pasifik?</td>\n",
       "      <td>179, 7 juta km</td>\n",
       "      <td>179, 7 juta km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>pt scooter victory inter part atau yang lebih ...</td>\n",
       "      <td>dimana kantor pusat pt scooter victory inter p...</td>\n",
       "      <td>bekasi, indonesia</td>\n",
       "      <td>bekasi, indonesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>studi hubungan internasional sebagai teori sud...</td>\n",
       "      <td>kapan teori hubungan internasional diciptakan?</td>\n",
       "      <td>1939</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>musik hip hop atau hip - hop [ 1 ] [ 2 ] atau ...</td>\n",
       "      <td>kapan musik hip hop pertama kali muncul?</td>\n",
       "      <td>1970an</td>\n",
       "      <td>1970an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>dalam melaksanakan kegiatan belajar - mengajar...</td>\n",
       "      <td>berapa luas smk negeri 1 cikampek?</td>\n",
       "      <td>28997m, dan jika di tambah dengan luas kampus...</td>\n",
       "      <td>29095m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>ikan pari manta ( manta birostris ) adalah sal...</td>\n",
       "      <td>berapakah berat ikan pari manta yag terbesar?</td>\n",
       "      <td>3 ton</td>\n",
       "      <td>3 ton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Context  \\\n",
       "0    kolumbus bukanlah orang pertama yang tiba di a...   \n",
       "1    kabupaten donggala ( english : donggala regenc...   \n",
       "2    awal mula teknik industri dapat ditelusuri dar...   \n",
       "3    penghulu rasyid ( lahir di desa telaga itar ta...   \n",
       "4    samudra pasifik atau lautan teduh ( dari bahas...   \n",
       "..                                                 ...   \n",
       "568  pt scooter victory inter part atau yang lebih ...   \n",
       "569  studi hubungan internasional sebagai teori sud...   \n",
       "570  musik hip hop atau hip - hop [ 1 ] [ 2 ] atau ...   \n",
       "571  dalam melaksanakan kegiatan belajar - mengajar...   \n",
       "572  ikan pari manta ( manta birostris ) adalah sal...   \n",
       "\n",
       "                                              Question  \\\n",
       "0                siapakah yang menemuka benua amerika?   \n",
       "1                            dimanakah letak donggala?   \n",
       "2                         siapa bapak teknik industri?   \n",
       "3                     kapan penghulu rasyid meninggal?   \n",
       "4                  seberapa luas kah samudera pasifik?   \n",
       "..                                                 ...   \n",
       "568  dimana kantor pusat pt scooter victory inter p...   \n",
       "569     kapan teori hubungan internasional diciptakan?   \n",
       "570           kapan musik hip hop pertama kali muncul?   \n",
       "571                 berapa luas smk negeri 1 cikampek?   \n",
       "572      berapakah berat ikan pari manta yag terbesar?   \n",
       "\n",
       "                                     Prediction Answer  \\\n",
       "0                                             kolumbus   \n",
       "1                  provinsi sulawesi tengah, indonesia   \n",
       "2                             frederick winslow taylor   \n",
       "3                                     15 desember 1861   \n",
       "4                                      179, 7 juta km   \n",
       "..                                                 ...   \n",
       "568                                  bekasi, indonesia   \n",
       "569                                               1939   \n",
       "570                                             1970an   \n",
       "571  28997m, dan jika di tambah dengan luas kampus...   \n",
       "572                                              3 ton   \n",
       "\n",
       "                               Gold Answer  \n",
       "0    orang - orang viking dari eropa utara  \n",
       "1      provinsi sulawesi tengah, indonesia  \n",
       "2                 frederick winslow taylor  \n",
       "3                         15 desember 1861  \n",
       "4                          179, 7 juta km  \n",
       "..                                     ...  \n",
       "568                      bekasi, indonesia  \n",
       "569                                   1939  \n",
       "570                                 1970an  \n",
       "571                                29095m  \n",
       "572                                  3 ton  \n",
       "\n",
       "[573 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qas_df = create_qas_dataframe(predict_result)\n",
    "qas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_mark = ['siapa', 'siapakah',\n",
    "                    'apa', 'apakah', 'adakah',\n",
    "                    'dimana', 'dimanakah', 'darimanakah',\n",
    "                    'kapan', 'kapankah',\n",
    "                    'bagaimana', 'bagaimanakah',\n",
    "                    'kenapa', 'mengapa',\n",
    "                    'berapa', 'berapakah', 'seberapa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mencoba cara retrieve model dari HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.15.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (45.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.23.5)\n",
      "Collecting torch\n",
      "  Using cached torch-2.0.0-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/dist-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/dist-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/dist-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch) (3.26.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.0.1-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torch, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.13.1\n",
      "    Uninstalling torchaudio-0.13.1:\n",
      "      Successfully uninstalled torchaudio-0.13.1\n",
      "Successfully installed torch-2.0.0 torchaudio-2.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 26 09:02:28 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    56W / 300W |  32476MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   59C    P0   245W / 300W |  32292MiB / 32480MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   53C    P0   210W / 300W |  12012MiB / 32480MiB |     99%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    57W / 300W |   9210MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    58W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    57W / 300W |   9364MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    56W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    56W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2deb0d4d2e0c40bb8f37c27c95b3a0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()lve/main/config.json:   0%|          | 0.00/942 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d049c975714458b6a63a7670300332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()\"pytorch_model.bin\";:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fda2a7669743998f3433b2045d281f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()okenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4d3d6afb9e4b0594d31c8493f6a4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()solve/main/vocab.txt:   0%|          | 0.00/234k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9189ba918a54e039c3f05c009d808cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()/main/tokenizer.json:   0%|          | 0.00/737k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e346ee01bd4c4d92ef3d9f883a71dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pretrained_name_sc = \"afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased\"\n",
    "pretrained_name_qa = \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\"\n",
    "tokenizer_kwargs = {'padding': True, 'truncation': True, 'max_length': MAX_LENGTH}\n",
    "\n",
    "nlp_sc = pipeline(task=\"text-classification\", model=pretrained_name_sc, tokenizer=pretrained_name_sc, \n",
    "                   **tokenizer_kwargs)\n",
    "nlp_qa = pipeline(task=\"question-answering\", model=pretrained_name_qa, tokenizer=pretrained_name_qa, \n",
    "                   **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7775b66c8d56416b814ad060dca1e463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()lve/main/config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"Wikidepia/IndoT5-base-paraphrase\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"Wikidepia/IndoT5-base-paraphrase\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac45e2f84b2c4c30ad95c363ea8077d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()\"pytorch_model.bin\";:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Wikidepia/IndoT5-base-paraphrase.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b3515e718d4d978709f6290efbea01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()okenizer_config.json:   0%|          | 0.00/1.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f96ca3682674e3e84ec2f1c888b363b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()\"spiece.model\";:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5dccba7eca4259ac54a9005bcc679e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()/main/tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c8c9cea66441c6b7466ed6a8b50740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/spiece.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "pretrained_name_tg = \"Wikidepia/IndoT5-base-paraphrase\"\n",
    "\n",
    "tokenizer_kwargs = {'truncation': True, 'max_length': MAX_LENGTH}\n",
    "\n",
    "nlp_tg = pipeline(task=\"text2text-generation\", model=pretrained_name_tg, tokenizer=pretrained_name_tg, \n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coba buat method evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(question, pred_answer, gold_answer, type, question_mark=question_mark):\n",
    "    \n",
    "    if type == 'replace first':\n",
    "        pred_hypothesis = question.replace('?', '')\n",
    "        pred_hypothesis = pred_hypothesis.replace(question.split()[0], pred_answer)\n",
    "\n",
    "        gold_hypothesis = question.replace('?', '')\n",
    "        gold_hypothesis = gold_hypothesis.replace(question.split()[0], gold_answer)\n",
    "    \n",
    "    elif type == 'replace question mark':\n",
    "        for i in question.split():\n",
    "            if i in question_mark:\n",
    "                pred_hypothesis = question.replace('?', '')\n",
    "                pred_hypothesis = pred_hypothesis.replace(i, pred_answer)\n",
    "\n",
    "                gold_hypothesis = question.replace('?', '')\n",
    "                gold_hypothesis = gold_hypothesis.replace(i, gold_answer)\n",
    "    \n",
    "    elif type == 'add adalah':\n",
    "        pred_hypothesis = question.replace('?', '')\n",
    "        pred_hypothesis = pred_hypothesis.replace(question.split()[0], '')\n",
    "        pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "        gold_hypothesis = question.replace('?', '')\n",
    "        gold_hypothesis = gold_hypothesis.replace(question.split()[0], '')\n",
    "        gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "    \n",
    "    elif type == 'just concat answer and question':\n",
    "        pred_hypothesis = f\"{question} {pred_answer}\"         \n",
    "        gold_hypothesis = f\"{question} {gold_answer}\"\n",
    "        \n",
    "    elif type == 'rule based':\n",
    "        question = question.replace('kah', '')\n",
    "        for j in question.split():\n",
    "            if j in question_mark:\n",
    "                if j == 'siapa' or j == 'siapakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "\n",
    "                elif j == 'apa' or j == 'apakah' or j == 'adakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "\n",
    "                elif j == 'dimana' or j == 'dimanakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} di {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} di {gold_answer}\"\n",
    "\n",
    "                elif j == 'darimanakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} dari {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} dari {gold_answer}\"\n",
    "\n",
    "                elif j == 'kapan' or j == 'kapankah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} pada {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} pada {gold_answer}\"\n",
    "\n",
    "                elif j == 'bagaimana' or j == 'bagaimanakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '')\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "\n",
    "                elif j == 'kenapa' or j == 'mengapa':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, 'alasan').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah karena {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, 'alasan').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah karena {gold_answer}\"\n",
    "\n",
    "                elif j == 'berapa' or j == 'berapakah' or j == 'seberapa': \n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "                    \n",
    "    elif type == 'machine generation': \n",
    "        pred_hypothesis, gold_hypothesis = smoothing(question, pred_answer, gold_answer, type=\"rule based\")\n",
    "        pred_hypothesis = nlp_tg(pred_hypothesis)[0]['generated_text']\n",
    "        gold_hypothesis = nlp_tg(gold_hypothesis)[0]['generated_text']\n",
    "        \n",
    "    return pred_hypothesis, gold_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alasan mengapa Messi menang adalah karena dia jago kali\n",
      "alasan messi menang adalah karena tim yang unggul .\n"
     ]
    }
   ],
   "source": [
    "pred_hypothesis, gold_hypothesis = smoothing(\"kenapa messi menang?\", \"emang jago kali\", \"tim yang unggul\",\n",
    "                                            type=\"machine generation\")\n",
    "print(pred_hypothesis)\n",
    "print(gold_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_for_evaluation(predict_result, type_smoothing, type_qas, MAXIMUM_SEARCH_ITER=MAXIMUM_SEARCH_ITER):\n",
    "    \n",
    "    # Ekstrak dari PredictionOutput QAS\n",
    "    predictions_idx = np.argsort(predict_result.predictions, axis=2)[:, :, 1 * -1]\n",
    "    label_array = np.asarray(predict_result.label_ids)\n",
    "    \n",
    "    question_array = []\n",
    "    context_array = []\n",
    "    \n",
    "    pred_answer_before_filtering_array = []\n",
    "    pred_answer_after_filtering_array = []\n",
    "    \n",
    "    label_before_filtering_array = []\n",
    "    label_after_filtering_array = []\n",
    "    \n",
    "    pred_hypothesis_before_filtering_array = []\n",
    "    pred_hypothesis_after_filtering_array = []\n",
    "    \n",
    "    gold_answer_array = []\n",
    "    gold_hypothesis_array = []\n",
    "    \n",
    "    pred_answer_after_filtering_array_msi_recorded = []\n",
    "    pred_hypothesis_after_filtering_array_msi_recorded = []\n",
    "    label_after_filtering_array_msi_recorded = []\n",
    "    \n",
    "    # Iterasi ini ditujukan untuk retrieve answer\n",
    "    for i in tqdm(range(len(predict_result.predictions[0]))):\n",
    "        \n",
    "        isFoundBiggest = False\n",
    "        \n",
    "        start_pred_idx = predictions_idx[0][i]\n",
    "        end_pred_idx = predictions_idx[1][i] + 1\n",
    "        \n",
    "        start_gold_idx = label_array[0][i]\n",
    "        end_gold_idx = label_array[1][i] + 1\n",
    "        \n",
    "        # Retrieve answer prediksi\n",
    "        pred_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_pred_idx: end_pred_idx], skip_special_tokens=True)\n",
    "        \n",
    "        # Retrieve answer gold\n",
    "        gold_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_gold_idx: end_gold_idx], skip_special_tokens=True)\n",
    "        \n",
    "        question = []\n",
    "        context = []\n",
    "        \n",
    "        # Iterasi ini untuk retrieve question dan context index yang bersangkutan\n",
    "        for j in range(len(tokenized_data_qas_id_validation[i]['token_type_ids'])):\n",
    "            \n",
    "            # Bila token_type_ids-nya 0, maka itu question (sesuai dengan urutan tokenisasi)\n",
    "            if tokenized_data_qas_id_validation[i]['token_type_ids'][j] == 0:\n",
    "                question.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "            \n",
    "            # Bila token_type_ids-nya 1, maka itu context (sesuai dengan urutan tokenisasi)\n",
    "            else:\n",
    "                context.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "        \n",
    "        question_decoded = tokenizer.decode(question, skip_special_tokens=True)\n",
    "        context_decoded = tokenizer.decode(context, skip_special_tokens=True)\n",
    "        pred_hypothesis, gold_hypothesis = smoothing(question_decoded, pred_answer, gold_answer, type_smoothing)\n",
    "\n",
    "        # Cek label dari answer prediksi dan context\n",
    "        predicted_label = nlp_sc({'text': context_decoded, \n",
    "                                  'text_pair': pred_hypothesis}, \n",
    "                                 **tokenizer_kwargs)\n",
    "        \n",
    "        pred_answer_before_filtering_array.append([pred_answer])\n",
    "        pred_hypothesis_before_filtering_array.append([pred_hypothesis])\n",
    "        label_before_filtering_array.append([predicted_label])\n",
    "        \n",
    "        # Cek label dari answer prediksi dan context, bila labelnya entailment (atau neutral), maka answernya jadi hasil akhir\n",
    "        if predicted_label['label'] == 'neutral':\n",
    "            if type_qas == 'entailment or neutral':\n",
    "                question_array.append(question_decoded)\n",
    "                context_array.append(context_decoded)\n",
    "                pred_answer_after_filtering_array.append([pred_answer])\n",
    "                gold_answer_array.append(gold_answer)\n",
    "                pred_hypothesis_after_filtering_array.append([pred_hypothesis])\n",
    "                gold_hypothesis_array.append(gold_hypothesis)\n",
    "                label_after_filtering_array.append([predicted_label])\n",
    "\n",
    "        if predicted_label['label'] == 'entailment':\n",
    "            if type_qas == 'entailment only' or type_qas == 'entailment or neutral':\n",
    "                question_array.append(question_decoded)\n",
    "                context_array.append(context_decoded)\n",
    "                pred_answer_after_filtering_array.append([pred_answer])\n",
    "                gold_answer_array.append(gold_answer)\n",
    "                pred_hypothesis_after_filtering_array.append([pred_hypothesis])\n",
    "                gold_hypothesis_array.append(gold_hypothesis)\n",
    "                label_after_filtering_array.append([predicted_label])\n",
    "            \n",
    "        # Cek label dari answer prediksi dan context, bila labelnya bukan entailment (atau neutral), \n",
    "        # -- maka masuk ke for-loop untuk iterasi ke argmax selanjutnya, dengan menggunakan argsort\n",
    "        else:\n",
    "            \n",
    "            if predicted_label == 'neutral' and type_qas == 'entailment or neutral': continue\n",
    "            \n",
    "            # Bila MAXIMUM_SEARCH_ITER dibawah 2, maka continue langsung\n",
    "            if MAXIMUM_SEARCH_ITER < 2: continue\n",
    "\n",
    "            # Bila MAXIMUM_SEARCH_ITER diatas 2, maka continue langsung\n",
    "            else:\n",
    "\n",
    "                # Bila bukan entailment, loop sebanyak MAXIMUM_SEARCH_ITER kali.\n",
    "                for index_largest in range(MAXIMUM_SEARCH_ITER - 1):\n",
    "\n",
    "                    # Cari di index kedua, ketiga, keempat, dan seterusnya\n",
    "                    predictions_idx_inside_loop = np.argsort(predict_result.predictions, \n",
    "                                                             axis=2)[:, :, (index_largest + 2) * -1]\n",
    "\n",
    "                    start_pred_idx = predictions_idx_inside_loop[0][i]\n",
    "                    end_pred_idx = predictions_idx_inside_loop[1][i] + 1\n",
    "\n",
    "                    # Retrieve answer prediksi\n",
    "                    pred_answer_inside_loop = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                                   [start_pred_idx: end_pred_idx], skip_special_tokens=True)\n",
    "                    \n",
    "                    pred_hypothesis_inside_loop, gold_hypothesis = smoothing(\n",
    "                        question_decoded, pred_answer_inside_loop, gold_answer, type_smoothing)\n",
    "                    \n",
    "                    # Cek label dari answer prediksi dan context\n",
    "                    predicted_label_inside_loop = nlp_sc({'text': context_decoded, \n",
    "                                                          'text_pair': pred_hypothesis_inside_loop}\n",
    "                                                           , **tokenizer_kwargs)\n",
    "                            \n",
    "                    pred_answer_after_filtering_array_msi_recorded.append(pred_answer_inside_loop)\n",
    "                    pred_hypothesis_after_filtering_array_msi_recorded.append(pred_hypothesis_inside_loop)\n",
    "                    label_after_filtering_array_msi_recorded.append(predicted_label_inside_loop)\n",
    "                    \n",
    "                    # Bila label-nya sudah entailment (atau neutral), maka answernya jadi hasil akhir, dan break\n",
    "                    if type_qas == 'entailment only':\n",
    "                        if predicted_label_inside_loop['label'] == 'entailment':\n",
    "                            isFoundBiggest = True\n",
    "                            question_array.append(question_decoded)\n",
    "                            context_array.append(context_decoded)\n",
    "                            gold_answer_array.append(gold_answer)   \n",
    "                            gold_hypothesis_array.append(gold_hypothesis)\n",
    "                            \n",
    "                            pred_answer_after_filtering_array.append(pred_answer_after_filtering_array_msi_recorded)\n",
    "                            pred_hypothesis_after_filtering_array.append(pred_hypothesis_after_filtering_array_msi_recorded)\n",
    "                            label_after_filtering_array.append(label_after_filtering_array_msi_recorded)\n",
    "                            break\n",
    "                            \n",
    "                    elif type_qas == 'entailment or neutral':\n",
    "                        if predicted_label_inside_loop['label'] == 'entailment' or predicted_label_inside_loop['label'] == 'neutral':\n",
    "                            isFoundBiggest = True\n",
    "                            question_array.append(question_decoded)\n",
    "                            context_array.append(context_decoded)\n",
    "                            gold_answer_array.append(gold_answer)   \n",
    "                            gold_hypothesis_array.append(gold_hypothesis)\n",
    "                            \n",
    "                            pred_answer_after_filtering_array.append(pred_answer_after_filtering_array_msi_recorded)\n",
    "                            pred_hypothesis_after_filtering_array.append(pred_hypothesis_after_filtering_array_msi_recorded)\n",
    "                            label_after_filtering_array.append(label_after_filtering_array_msi_recorded)\n",
    "                            break\n",
    "\n",
    "                if isFoundBiggest == False:\n",
    "                    # Bila sampai iterasi terakhir, belum entailment (atau neutral) juga, maka append saja jawaban kosong\n",
    "                    \n",
    "                    pred_answer_not_found_biggest = \"\" # Disini, jawaban kosong\n",
    "                    \n",
    "                    question_array.append(question_decoded)\n",
    "                    context_array.append(context_decoded)\n",
    "                    \n",
    "                    pred_hypothesis_not_found_biggest, gold_hypothesis = smoothing(\n",
    "                        question_decoded, pred_answer_not_found_biggest, gold_answer, type_smoothing)\n",
    "                    \n",
    "                    pred_answer_after_filtering_array_msi_recorded.append(pred_answer_not_found_biggest)\n",
    "                    pred_hypothesis_after_filtering_array_msi_recorded.append(pred_hypothesis_not_found_biggest)\n",
    "                    label_after_filtering_array_msi_recorded.append(predicted_label_inside_loop)\n",
    "                    \n",
    "                    gold_answer_array.append(gold_answer)\n",
    "                    gold_hypothesis_array.append(gold_hypothesis)\n",
    "                    \n",
    "                    pred_answer_after_filtering_array.append(pred_answer_after_filtering_array_msi_recorded)\n",
    "                    pred_hypothesis_after_filtering_array.append(pred_hypothesis_after_filtering_array_msi_recorded)\n",
    "                    label_after_filtering_array.append(label_after_filtering_array_msi_recorded)\n",
    "    \n",
    "    # Buat DataFrame QAS\n",
    "    qas_df = pd.DataFrame({'Context': context_array, \n",
    "                           'Question': question_array, \n",
    "                           \n",
    "                           'Prediction Answer Before Filtering': pred_answer_before_filtering_array,\n",
    "                           'Prediction Hypothesis Before Filtering': pred_hypothesis_before_filtering_array,\n",
    "                           'Label Before Filtering': label_before_filtering_array,\n",
    "                                 \n",
    "                           'Prediction Answer After Filtering': pred_answer_after_filtering_array,\n",
    "                           'Prediction Hypothesis After Filtering': pred_hypothesis_after_filtering_array,\n",
    "                           'Label After Filtering': label_after_filtering_array,\n",
    "                          \n",
    "                           'Gold Answer': gold_answer_array,\n",
    "                          'Gold Hypothesis': gold_hypothesis_array})\n",
    "                          \n",
    "    assert len(predict_result.predictions[0]) == len(qas_df), \"Jumlah prediksi berbeda dengan jumlah evaluasi\"\n",
    "    \n",
    "    # Return DataFrame QAS\n",
    "    return qas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 573/573 [04:31<00:00,  2.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Question</th>\n",
       "      <th>Prediction Answer Before Filtering</th>\n",
       "      <th>Prediction Hypothesis Before Filtering</th>\n",
       "      <th>Label Before Filtering</th>\n",
       "      <th>Prediction Answer After Filtering</th>\n",
       "      <th>Prediction Hypothesis After Filtering</th>\n",
       "      <th>Label After Filtering</th>\n",
       "      <th>Gold Answer</th>\n",
       "      <th>Gold Hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kolumbus bukanlah orang pertama yang tiba di a...</td>\n",
       "      <td>siapakah yang menemuka benua amerika?</td>\n",
       "      <td>[kolumbus]</td>\n",
       "      <td>[kolumbus yang menemuka benua amerika]</td>\n",
       "      <td>[{'label': 'neutral', 'score': 0.4978318512439...</td>\n",
       "      <td>[, , periode 2000 - an bca memperkuat dan meng...</td>\n",
       "      <td>[ yang menemuka benua amerika,  yang menemuka ...</td>\n",
       "      <td>[{'label': 'contradiction', 'score': 0.4999576...</td>\n",
       "      <td>orang - orang viking dari eropa utara</td>\n",
       "      <td>orang - orang viking dari eropa utara yang men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kabupaten donggala ( english : donggala regenc...</td>\n",
       "      <td>dimanakah letak donggala?</td>\n",
       "      <td>[provinsi sulawesi tengah, indonesia]</td>\n",
       "      <td>[provinsi sulawesi tengah, indonesia letak don...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9416803121...</td>\n",
       "      <td>[provinsi sulawesi tengah, indonesia]</td>\n",
       "      <td>[provinsi sulawesi tengah, indonesia letak don...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9416803121...</td>\n",
       "      <td>provinsi sulawesi tengah, indonesia</td>\n",
       "      <td>provinsi sulawesi tengah, indonesia letak dong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awal mula teknik industri dapat ditelusuri dar...</td>\n",
       "      <td>siapa bapak teknik industri?</td>\n",
       "      <td>[frederick winslow taylor]</td>\n",
       "      <td>[frederick winslow taylor bapak teknik industri]</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9322418570...</td>\n",
       "      <td>[frederick winslow taylor]</td>\n",
       "      <td>[frederick winslow taylor bapak teknik industri]</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9322418570...</td>\n",
       "      <td>frederick winslow taylor</td>\n",
       "      <td>frederick winslow taylor bapak teknik industri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>penghulu rasyid ( lahir di desa telaga itar ta...</td>\n",
       "      <td>kapan penghulu rasyid meninggal?</td>\n",
       "      <td>[15 desember 1861]</td>\n",
       "      <td>[15 desember 1861 penghulu rasyid meninggal]</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9960013031...</td>\n",
       "      <td>[15 desember 1861]</td>\n",
       "      <td>[15 desember 1861 penghulu rasyid meninggal]</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9960013031...</td>\n",
       "      <td>15 desember 1861</td>\n",
       "      <td>15 desember 1861 penghulu rasyid meninggal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>samudra pasifik atau lautan teduh ( dari bahas...</td>\n",
       "      <td>seberapa luas kah samudera pasifik?</td>\n",
       "      <td>[179, 7 juta km]</td>\n",
       "      <td>[179, 7 juta km luas kah samudera pasifik]</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9817168116...</td>\n",
       "      <td>[179, 7 juta km]</td>\n",
       "      <td>[179, 7 juta km luas kah samudera pasifik]</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9817168116...</td>\n",
       "      <td>179, 7 juta km</td>\n",
       "      <td>179, 7 juta km luas kah samudera pasifik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>pt scooter victory inter part atau yang lebih ...</td>\n",
       "      <td>dimana kantor pusat pt scooter victory inter p...</td>\n",
       "      <td>[bekasi, indonesia]</td>\n",
       "      <td>[bekasi, indonesia kantor pusat pt scooter vic...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9985334873...</td>\n",
       "      <td>[bekasi, indonesia]</td>\n",
       "      <td>[bekasi, indonesia kantor pusat pt scooter vic...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9985334873...</td>\n",
       "      <td>bekasi, indonesia</td>\n",
       "      <td>bekasi, indonesia kantor pusat pt scooter vict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>studi hubungan internasional sebagai teori sud...</td>\n",
       "      <td>kapan teori hubungan internasional diciptakan?</td>\n",
       "      <td>[1939]</td>\n",
       "      <td>[1939 teori hubungan internasional diciptakan]</td>\n",
       "      <td>[{'label': 'neutral', 'score': 0.9211949110031...</td>\n",
       "      <td>[, , periode 2000 - an bca memperkuat dan meng...</td>\n",
       "      <td>[ yang menemuka benua amerika,  yang menemuka ...</td>\n",
       "      <td>[{'label': 'contradiction', 'score': 0.4999576...</td>\n",
       "      <td>1939</td>\n",
       "      <td>1939 teori hubungan internasional diciptakan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>musik hip hop atau hip - hop [ 1 ] [ 2 ] atau ...</td>\n",
       "      <td>kapan musik hip hop pertama kali muncul?</td>\n",
       "      <td>[1970an]</td>\n",
       "      <td>[1970an musik hip hop pertama kali muncul]</td>\n",
       "      <td>[{'label': 'neutral', 'score': 0.6594114303588...</td>\n",
       "      <td>[, , periode 2000 - an bca memperkuat dan meng...</td>\n",
       "      <td>[ yang menemuka benua amerika,  yang menemuka ...</td>\n",
       "      <td>[{'label': 'contradiction', 'score': 0.4999576...</td>\n",
       "      <td>1970an</td>\n",
       "      <td>1970an musik hip hop pertama kali muncul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>dalam melaksanakan kegiatan belajar - mengajar...</td>\n",
       "      <td>berapa luas smk negeri 1 cikampek?</td>\n",
       "      <td>[28997m, dan jika di tambah dengan luas kampu...</td>\n",
       "      <td>[28997m, dan jika di tambah dengan luas kampu...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9920784831...</td>\n",
       "      <td>[28997m, dan jika di tambah dengan luas kampu...</td>\n",
       "      <td>[28997m, dan jika di tambah dengan luas kampu...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9920784831...</td>\n",
       "      <td>29095m</td>\n",
       "      <td>29095m luas smk negeri 1 cikampek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>ikan pari manta ( manta birostris ) adalah sal...</td>\n",
       "      <td>berapakah berat ikan pari manta yag terbesar?</td>\n",
       "      <td>[3 ton]</td>\n",
       "      <td>[3 ton berat ikan pari manta yag terbesar]</td>\n",
       "      <td>[{'label': 'neutral', 'score': 0.9588347077369...</td>\n",
       "      <td>[, , periode 2000 - an bca memperkuat dan meng...</td>\n",
       "      <td>[ yang menemuka benua amerika,  yang menemuka ...</td>\n",
       "      <td>[{'label': 'contradiction', 'score': 0.4999576...</td>\n",
       "      <td>3 ton</td>\n",
       "      <td>3 ton berat ikan pari manta yag terbesar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Context  \\\n",
       "0    kolumbus bukanlah orang pertama yang tiba di a...   \n",
       "1    kabupaten donggala ( english : donggala regenc...   \n",
       "2    awal mula teknik industri dapat ditelusuri dar...   \n",
       "3    penghulu rasyid ( lahir di desa telaga itar ta...   \n",
       "4    samudra pasifik atau lautan teduh ( dari bahas...   \n",
       "..                                                 ...   \n",
       "568  pt scooter victory inter part atau yang lebih ...   \n",
       "569  studi hubungan internasional sebagai teori sud...   \n",
       "570  musik hip hop atau hip - hop [ 1 ] [ 2 ] atau ...   \n",
       "571  dalam melaksanakan kegiatan belajar - mengajar...   \n",
       "572  ikan pari manta ( manta birostris ) adalah sal...   \n",
       "\n",
       "                                              Question  \\\n",
       "0                siapakah yang menemuka benua amerika?   \n",
       "1                            dimanakah letak donggala?   \n",
       "2                         siapa bapak teknik industri?   \n",
       "3                     kapan penghulu rasyid meninggal?   \n",
       "4                  seberapa luas kah samudera pasifik?   \n",
       "..                                                 ...   \n",
       "568  dimana kantor pusat pt scooter victory inter p...   \n",
       "569     kapan teori hubungan internasional diciptakan?   \n",
       "570           kapan musik hip hop pertama kali muncul?   \n",
       "571                 berapa luas smk negeri 1 cikampek?   \n",
       "572      berapakah berat ikan pari manta yag terbesar?   \n",
       "\n",
       "                    Prediction Answer Before Filtering  \\\n",
       "0                                           [kolumbus]   \n",
       "1                [provinsi sulawesi tengah, indonesia]   \n",
       "2                           [frederick winslow taylor]   \n",
       "3                                   [15 desember 1861]   \n",
       "4                                    [179, 7 juta km]   \n",
       "..                                                 ...   \n",
       "568                                [bekasi, indonesia]   \n",
       "569                                             [1939]   \n",
       "570                                           [1970an]   \n",
       "571  [28997m, dan jika di tambah dengan luas kampu...   \n",
       "572                                            [3 ton]   \n",
       "\n",
       "                Prediction Hypothesis Before Filtering  \\\n",
       "0               [kolumbus yang menemuka benua amerika]   \n",
       "1    [provinsi sulawesi tengah, indonesia letak don...   \n",
       "2     [frederick winslow taylor bapak teknik industri]   \n",
       "3         [15 desember 1861 penghulu rasyid meninggal]   \n",
       "4          [179, 7 juta km luas kah samudera pasifik]   \n",
       "..                                                 ...   \n",
       "568  [bekasi, indonesia kantor pusat pt scooter vic...   \n",
       "569     [1939 teori hubungan internasional diciptakan]   \n",
       "570         [1970an musik hip hop pertama kali muncul]   \n",
       "571  [28997m, dan jika di tambah dengan luas kampu...   \n",
       "572         [3 ton berat ikan pari manta yag terbesar]   \n",
       "\n",
       "                                Label Before Filtering  \\\n",
       "0    [{'label': 'neutral', 'score': 0.4978318512439...   \n",
       "1    [{'label': 'entailment', 'score': 0.9416803121...   \n",
       "2    [{'label': 'entailment', 'score': 0.9322418570...   \n",
       "3    [{'label': 'entailment', 'score': 0.9960013031...   \n",
       "4    [{'label': 'entailment', 'score': 0.9817168116...   \n",
       "..                                                 ...   \n",
       "568  [{'label': 'entailment', 'score': 0.9985334873...   \n",
       "569  [{'label': 'neutral', 'score': 0.9211949110031...   \n",
       "570  [{'label': 'neutral', 'score': 0.6594114303588...   \n",
       "571  [{'label': 'entailment', 'score': 0.9920784831...   \n",
       "572  [{'label': 'neutral', 'score': 0.9588347077369...   \n",
       "\n",
       "                     Prediction Answer After Filtering  \\\n",
       "0    [, , periode 2000 - an bca memperkuat dan meng...   \n",
       "1                [provinsi sulawesi tengah, indonesia]   \n",
       "2                           [frederick winslow taylor]   \n",
       "3                                   [15 desember 1861]   \n",
       "4                                    [179, 7 juta km]   \n",
       "..                                                 ...   \n",
       "568                                [bekasi, indonesia]   \n",
       "569  [, , periode 2000 - an bca memperkuat dan meng...   \n",
       "570  [, , periode 2000 - an bca memperkuat dan meng...   \n",
       "571  [28997m, dan jika di tambah dengan luas kampu...   \n",
       "572  [, , periode 2000 - an bca memperkuat dan meng...   \n",
       "\n",
       "                 Prediction Hypothesis After Filtering  \\\n",
       "0    [ yang menemuka benua amerika,  yang menemuka ...   \n",
       "1    [provinsi sulawesi tengah, indonesia letak don...   \n",
       "2     [frederick winslow taylor bapak teknik industri]   \n",
       "3         [15 desember 1861 penghulu rasyid meninggal]   \n",
       "4          [179, 7 juta km luas kah samudera pasifik]   \n",
       "..                                                 ...   \n",
       "568  [bekasi, indonesia kantor pusat pt scooter vic...   \n",
       "569  [ yang menemuka benua amerika,  yang menemuka ...   \n",
       "570  [ yang menemuka benua amerika,  yang menemuka ...   \n",
       "571  [28997m, dan jika di tambah dengan luas kampu...   \n",
       "572  [ yang menemuka benua amerika,  yang menemuka ...   \n",
       "\n",
       "                                 Label After Filtering  \\\n",
       "0    [{'label': 'contradiction', 'score': 0.4999576...   \n",
       "1    [{'label': 'entailment', 'score': 0.9416803121...   \n",
       "2    [{'label': 'entailment', 'score': 0.9322418570...   \n",
       "3    [{'label': 'entailment', 'score': 0.9960013031...   \n",
       "4    [{'label': 'entailment', 'score': 0.9817168116...   \n",
       "..                                                 ...   \n",
       "568  [{'label': 'entailment', 'score': 0.9985334873...   \n",
       "569  [{'label': 'contradiction', 'score': 0.4999576...   \n",
       "570  [{'label': 'contradiction', 'score': 0.4999576...   \n",
       "571  [{'label': 'entailment', 'score': 0.9920784831...   \n",
       "572  [{'label': 'contradiction', 'score': 0.4999576...   \n",
       "\n",
       "                               Gold Answer  \\\n",
       "0    orang - orang viking dari eropa utara   \n",
       "1      provinsi sulawesi tengah, indonesia   \n",
       "2                 frederick winslow taylor   \n",
       "3                         15 desember 1861   \n",
       "4                          179, 7 juta km   \n",
       "..                                     ...   \n",
       "568                      bekasi, indonesia   \n",
       "569                                   1939   \n",
       "570                                 1970an   \n",
       "571                                29095m   \n",
       "572                                  3 ton   \n",
       "\n",
       "                                       Gold Hypothesis  \n",
       "0    orang - orang viking dari eropa utara yang men...  \n",
       "1    provinsi sulawesi tengah, indonesia letak dong...  \n",
       "2       frederick winslow taylor bapak teknik industri  \n",
       "3           15 desember 1861 penghulu rasyid meninggal  \n",
       "4            179, 7 juta km luas kah samudera pasifik  \n",
       "..                                                 ...  \n",
       "568  bekasi, indonesia kantor pusat pt scooter vict...  \n",
       "569       1939 teori hubungan internasional diciptakan  \n",
       "570           1970an musik hip hop pertama kali muncul  \n",
       "571                 29095m luas smk negeri 1 cikampek  \n",
       "572           3 ton berat ikan pari manta yag terbesar  \n",
       "\n",
       "[573 rows x 10 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = create_df_for_evaluation(predict_result, type_smoothing='replace first', type_qas='entailment only')\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kolumbus'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[\"Prediction Answer Before Filtering\"][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_from_df(df, type_qas):\n",
    "    \n",
    "    denominator = len(df)\n",
    "    total_correct = 0\n",
    "    f1_array = []\n",
    "    \n",
    "    true_positive_before_filtering = 0\n",
    "    false_positive_before_filtering = 0\n",
    "    false_negative_before_filtering = 0\n",
    "    true_negative_before_filtering = 0\n",
    "    \n",
    "    true_positive_after_filtering = 0\n",
    "    false_positive_after_filtering = 0\n",
    "    false_negative_after_filtering = 0\n",
    "    true_negative_after_filtering = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        pred_answer_before_filtering = df[\"Prediction Answer Before Filtering\"][i][-1]\n",
    "        pred_answer_after_filtering = df[\"Prediction Answer After Filtering\"][i][-1]\n",
    "        \n",
    "        pred_label_before_filtering = df[\"Label Before Filtering\"][i][-1]['label']\n",
    "        pred_label_after_filtering = df[\"Label After Filtering\"][i][-1]['label']\n",
    "        \n",
    "        gold_text = df[\"Gold Answer\"][i]\n",
    "\n",
    "        if pred_answer_after_filtering == gold_text:\n",
    "            total_correct += 1\n",
    "\n",
    "        f1 = compute_f1_prec_rec(pred=pred_answer_after_filtering, gold=gold_text)\n",
    "\n",
    "        f1_array.append(f1)\n",
    "        \n",
    "        # Terprediksi dengan label yang benar, dan hasil answernya benar -> True positive\n",
    "        # Terprediksi dengan label yang benar, padahal hasil answernya salah -> False positive\n",
    "        # Terprediksi dengan label yang salah, padahal hasil answernya benar -> False negative\n",
    "        # Terprediksi dengan label yang salah, dan hasil answernya salah -> True negative\n",
    "        \n",
    "        if type_qas == 'entailment only':\n",
    "        \n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering == 'entailment'):\n",
    "                true_positive_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering == 'entailment'):\n",
    "                false_positive_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering != 'entailment'):\n",
    "                false_negative_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering != 'entailment'):\n",
    "                true_negative_after_filtering += 1\n",
    "\n",
    "            if (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'entailment'):\n",
    "                true_positive_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'entailment'):\n",
    "                false_positive_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering != 'entailment'):\n",
    "                false_negative_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering != 'entailment'):\n",
    "                true_negative_before_filtering += 1\n",
    "        \n",
    "        elif type_qas == 'entailment or neutral':\n",
    "        \n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering == 'entailment' \n",
    "                                                               or pred_label_after_filtering == 'neutral'):\n",
    "                true_positive_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering == 'entailment' \n",
    "                                                                 or pred_label_after_filtering == 'neutral'):\n",
    "                false_positive_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering != 'entailment' \n",
    "                                                                 and pred_label_after_filtering != 'neutral'):\n",
    "                false_negative_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering != 'entailment' \n",
    "                                                                 and pred_label_after_filtering != 'neutral'):\n",
    "                true_negative_after_filtering += 1\n",
    "\n",
    "            if (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'entailment' \n",
    "                                                                or pred_label_after_filtering == 'neutral'):\n",
    "                true_positive_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'entailment' \n",
    "                                                                  or pred_label_after_filtering == 'neutral'):\n",
    "                false_positive_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering != 'entailment' \n",
    "                                                                  and pred_label_after_filtering != 'neutral'):\n",
    "                false_negative_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering != 'entailment' \n",
    "                                                                  and pred_label_after_filtering != 'neutral'):\n",
    "                true_negative_before_filtering += 1\n",
    "\n",
    "    exact_match = ((total_correct / denominator) * 100.0)\n",
    "    final_f1 = np.mean(f1_array) * 100.0\n",
    "    after_filtering_metric_array = [true_positive_after_filtering, false_positive_after_filtering, \n",
    "                          false_negative_after_filtering, true_negative_after_filtering]\n",
    "    before_filtering_metric_array = [true_positive_before_filtering, false_positive_before_filtering, \n",
    "                          false_negative_before_filtering, true_negative_before_filtering]\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': final_f1}, after_filtering_metric_array, before_filtering_metric_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 47.12041884816754, 'f1': 58.22636147737556}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_result_after_filtering, after_filtering_metric_array, before_filtering_metric_array = compute_metrics_from_df(\n",
    "    eval_df, \"entailment only\")\n",
    "metric_result_after_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_non_zero(number):\n",
    "    if number == 0:\n",
    "        number += sys.float_info.min\n",
    "    return number\n",
    "\n",
    "def compute_f1_prec_rec_whole(metric_array):\n",
    "    accuracy = (metric_array[0] + metric_array[3]) / \\\n",
    "        (metric_array[0] + metric_array[1] + \n",
    "         metric_array[2] + metric_array[3])\n",
    "    \n",
    "    precision = (metric_array[0]) / (metric_array[0] + metric_array[1])\n",
    "    \n",
    "    recall = (metric_array[0]) / (metric_array[0] + metric_array[2])\n",
    "    \n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def diff_verbose_metric(metric_result_before, metric_result_after, metric):\n",
    "    \n",
    "    percentage = round(((metric_result_after - metric_result_before) / metric_result_before) * 100, 2)\n",
    "    \n",
    "    if '&' in metric: vocab = \"nilai\"\n",
    "    else: vocab = \"metrik\"\n",
    "    \n",
    "    if metric_result_before ==  metric_result_after:\n",
    "        print(f\"Hasil {vocab} {metric} sebelum filtering NLI SAMA DENGAN metrik setelah filtering NLI\")\n",
    "    elif metric_result_before <  metric_result_after:\n",
    "        print(f\"Hasil {vocab} {metric} setelah filtering NLI mengalami KENAIKAN sebesar: {percentage} %\")\n",
    "    elif metric_result_before >  metric_result_after:\n",
    "        print(f\"Hasil {vocab} {metric} setelah filtering NLI mengalami PENURUNAN sebesar: {-1 * percentage} %\")\n",
    "    \n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_metrics(metrics_before, metrics_after, \n",
    "                    after_filtering_metric_array=after_filtering_metric_array, \n",
    "                    before_filtering_metric_array=before_filtering_metric_array):\n",
    "    \n",
    "    em_before = metrics_before['exact_match']\n",
    "    f1_before = metrics_before['f1']\n",
    "\n",
    "    print(\"~ METRIK PER TOKEN ~\")\n",
    "    print(f\"Skor Exact Match sebelum filtering NLI: {em_before}\")\n",
    "    print(f\"Skor F1 sebelum filtering NLI: {f1_before}\")\n",
    "    print()\n",
    "\n",
    "    em_after = metrics_after['exact_match']\n",
    "    f1_after = metrics_after['f1']\n",
    "\n",
    "    print(f\"Skor Exact Match setelah filtering NLI: {em_after}\")\n",
    "    print(f\"Skor F1 setelah filtering NLI: {f1_after}\")\n",
    "    print()\n",
    "\n",
    "    em_before = convert_to_non_zero(em_before)\n",
    "    f1_before = convert_to_non_zero(f1_before)\n",
    "\n",
    "    em_after = convert_to_non_zero(em_after)\n",
    "    f1_after = convert_to_non_zero(f1_after)\n",
    "\n",
    "    print(\"~ METRIK DENGAN PARAMETER NLI ~\")\n",
    "    print(f\"[BEFORE FILTERING] Jawaban benar & label NLI yang sesuai: {before_filtering_metric_array[0]}\")\n",
    "    print(f\"[BEFORE FILTERING] Jawaban TIDAK benar & label NLI yang sesuai: {before_filtering_metric_array[1]}\")\n",
    "    print(f\"[BEFORE FILTERING] Jawaban benar & label NLI yang TIDAK sesuai: {before_filtering_metric_array[2]}\")\n",
    "    print(f\"[BEFORE FILTERING] Jawaban TIDAK benar & label NLI yang TIDAK sesuai: {before_filtering_metric_array[3]}\")\n",
    "    print()\n",
    "\n",
    "    print(f\"[AFTER FILTERING] Jawaban benar & label NLI yang sesuai: {after_filtering_metric_array[0]}\")\n",
    "    print(f\"[AFTER FILTERING] Jawaban TIDAK benar & label NLI yang sesuai: {after_filtering_metric_array[1]}\")\n",
    "    print(f\"[AFTER FILTERING] Jawaban benar & label NLI yang TIDAK sesuai: {after_filtering_metric_array[2]}\")\n",
    "    print(f\"[AFTER FILTERING] Jawaban TIDAK benar & label NLI yang TIDAK sesuai: {after_filtering_metric_array[3]}\")\n",
    "    print()\n",
    "\n",
    "    print(\"Metrik di atas, bisa direpresentasikan menjadi:\")\n",
    "\n",
    "    acc_before_whole, prec_before_whole, rec_before_whole, f1_before_whole = compute_f1_prec_rec_whole(\n",
    "        before_filtering_metric_array)\n",
    "    acc_after_whole, prec_after_whole, rec_after_whole, f1_after_whole = compute_f1_prec_rec_whole(\n",
    "        after_filtering_metric_array)\n",
    "\n",
    "    print(f\"[BEFORE FILTERING] Akurasi: {acc_before_whole}\")\n",
    "    print(f\"[BEFORE FILTERING] Precision: {prec_before_whole}\")\n",
    "    print(f\"[BEFORE FILTERING] Recall: {rec_before_whole}\")\n",
    "    print(f\"[BEFORE FILTERING] F1: {f1_before_whole}\")\n",
    "    print()\n",
    "\n",
    "    print(f\"[AFTER FILTERING] Akurasi: {acc_after_whole}\")\n",
    "    print(f\"[AFTER FILTERING] Precision: {prec_after_whole}\")\n",
    "    print(f\"[AFTER FILTERING] Recall: {rec_after_whole}\")\n",
    "    print(f\"[AFTER FILTERING] F1: {f1_after_whole}\")\n",
    "    print()\n",
    "\n",
    "    print(\"--- Persentase perubahan hasil metrik ---\")\n",
    "    print(\"~ METRIK PER TOKEN ~\")\n",
    "    diff_verbose_metric(em_before, em_after, \"Exact Match\")\n",
    "    diff_verbose_metric(f1_before, f1_after, \"F1\")\n",
    "    print()\n",
    "\n",
    "    print(\"~ METRIK DENGAN PARAMETER NLI ~\")\n",
    "    diff_verbose_metric(before_filtering_metric_array[0], after_filtering_metric_array[0], \n",
    "                        \"Jawaban benar & label NLI yang sesuai\")\n",
    "    diff_verbose_metric(before_filtering_metric_array[1], after_filtering_metric_array[1], \n",
    "                        \"Jawaban TIDAK benar & label NLI yang sesuai\")\n",
    "    diff_verbose_metric(before_filtering_metric_array[2], after_filtering_metric_array[2], \n",
    "                        \"Jawaban benar & label NLI yang TIDAK sesuai\")\n",
    "    diff_verbose_metric(before_filtering_metric_array[3], after_filtering_metric_array[3], \n",
    "                        \"Jawaban TIDAK benar & label NLI yang TIDAK sesuai\")\n",
    "    print()\n",
    "\n",
    "    print(\"Metrik di atas, bisa direpresentasikan menjadi:\")\n",
    "    diff_verbose_metric(acc_before_whole, acc_after_whole, \"Akurasi\")\n",
    "    diff_verbose_metric(prec_before_whole, prec_after_whole, \"Precision\")\n",
    "    diff_verbose_metric(rec_before_whole, rec_after_whole, \"Recall\")\n",
    "    diff_verbose_metric(f1_before_whole, f1_after_whole, \"F1\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~ METRIK PER TOKEN ~\n",
      "Skor Exact Match sebelum filtering NLI: 62.129144851657934\n",
      "Skor F1 sebelum filtering NLI: 76.50970983610755\n",
      "\n",
      "Skor Exact Match setelah filtering NLI: 47.12041884816754\n",
      "Skor F1 setelah filtering NLI: 58.22636147737556\n",
      "\n",
      "~ METRIK DENGAN PARAMETER NLI ~\n",
      "[BEFORE FILTERING] Jawaban benar & label NLI yang sesuai: 263\n",
      "[BEFORE FILTERING] Jawaban TIDAK benar & label NLI yang sesuai: 142\n",
      "[BEFORE FILTERING] Jawaban benar & label NLI yang TIDAK sesuai: 94\n",
      "[BEFORE FILTERING] Jawaban TIDAK benar & label NLI yang TIDAK sesuai: 74\n",
      "\n",
      "[AFTER FILTERING] Jawaban benar & label NLI yang sesuai: 263\n",
      "[AFTER FILTERING] Jawaban TIDAK benar & label NLI yang sesuai: 142\n",
      "[AFTER FILTERING] Jawaban benar & label NLI yang TIDAK sesuai: 7\n",
      "[AFTER FILTERING] Jawaban TIDAK benar & label NLI yang TIDAK sesuai: 161\n",
      "\n",
      "Metrik di atas, bisa direpresentasikan menjadi:\n",
      "[BEFORE FILTERING] Akurasi: 0.5881326352530541\n",
      "[BEFORE FILTERING] Precision: 0.6493827160493827\n",
      "[BEFORE FILTERING] Recall: 0.7366946778711485\n",
      "[BEFORE FILTERING] F1: 0.6902887139107612\n",
      "\n",
      "[AFTER FILTERING] Akurasi: 0.7399650959860384\n",
      "[AFTER FILTERING] Precision: 0.6493827160493827\n",
      "[AFTER FILTERING] Recall: 0.9740740740740741\n",
      "[AFTER FILTERING] F1: 0.7792592592592592\n",
      "\n",
      "--- Persentase perubahan hasil metrik ---\n",
      "~ METRIK PER TOKEN ~\n",
      "Hasil metrik Exact Match setelah filtering NLI mengalami PENURUNAN sebesar: 24.16 %\n",
      "Hasil metrik F1 setelah filtering NLI mengalami PENURUNAN sebesar: 23.9 %\n",
      "\n",
      "~ METRIK DENGAN PARAMETER NLI ~\n",
      "Hasil nilai Jawaban benar & label NLI yang sesuai sebelum filtering NLI SAMA DENGAN metrik setelah filtering NLI\n",
      "Hasil nilai Jawaban TIDAK benar & label NLI yang sesuai sebelum filtering NLI SAMA DENGAN metrik setelah filtering NLI\n",
      "Hasil nilai Jawaban benar & label NLI yang TIDAK sesuai setelah filtering NLI mengalami PENURUNAN sebesar: 92.55 %\n",
      "Hasil nilai Jawaban TIDAK benar & label NLI yang TIDAK sesuai setelah filtering NLI mengalami KENAIKAN sebesar: 117.57 %\n",
      "\n",
      "Metrik di atas, bisa direpresentasikan menjadi:\n",
      "Hasil metrik Akurasi setelah filtering NLI mengalami KENAIKAN sebesar: 25.82 %\n",
      "Hasil metrik Precision sebelum filtering NLI SAMA DENGAN metrik setelah filtering NLI\n",
      "Hasil metrik Recall setelah filtering NLI mengalami KENAIKAN sebesar: 32.22 %\n",
      "Hasil metrik F1 setelah filtering NLI mengalami KENAIKAN sebesar: 12.89 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_metrics(metric_result_before_filtering, metric_result_after_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(ACCURACY_DIR), exist_ok=True)\n",
    "with open(f'{ACCURACY_DIR}/metric_comparison_results.txt', \"w\") as f, contextlib.redirect_stdout(f):\n",
    "    compare_metrics(metric_result_before_filtering, metric_result_after_filtering)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_evaluation(df):\n",
    "    \n",
    "    exist_true_answer_label_entailment = 0\n",
    "    exist_true_answer_label_neutral = 0\n",
    "    exist_true_answer_label_contradiction = 0\n",
    "    \n",
    "    exist_false_answer_label_entailment = 0\n",
    "    exist_false_answer_label_neutral = 0\n",
    "    exist_false_answer_label_contradiction = 0\n",
    "    \n",
    "    no_exist_true_answer = 0\n",
    "    no_exist_false_answer = 0\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        pred_answer_before_filtering = df[\"Prediction Answer Before Filtering\"][i][-1]\n",
    "        pred_answer_after_filtering = df[\"Prediction Answer After Filtering\"][i][-1]\n",
    "        \n",
    "        pred_label_before_filtering = df[\"Label Before Filtering\"][i][-1]['label']\n",
    "        pred_label_after_filtering = df[\"Label After Filtering\"][i][-1]['label']\n",
    "        \n",
    "        gold_text = df[\"Gold Answer\"][i]\n",
    "        \n",
    "        if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering == 'entailment') and (pred_answer_after_filtering != \"\"):\n",
    "            exist_true_answer_label_entailment += 1\n",
    "        elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering == 'neutral') and (pred_answer_after_filtering != \"\"):\n",
    "            exist_true_answer_label_neutral += 1\n",
    "        elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering == 'contradiction') and (pred_answer_after_filtering != \"\"):\n",
    "            exist_true_answer_label_contradiction += 1\n",
    "        \n",
    "        elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering == 'entailment') and (pred_answer_after_filtering != \"\"):\n",
    "            exist_false_answer_label_entailment += 1\n",
    "        elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering == 'neutral') and (pred_answer_after_filtering != \"\"):\n",
    "            exist_false_answer_label_neutral += 1\n",
    "        elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering == 'contradiction') and (pred_answer_after_filtering != \"\"):\n",
    "            exist_false_answer_label_contradiction += 1\n",
    "        \n",
    "        elif (pred_answer_after_filtering == gold_text) and (pred_answer_after_filtering == \"\"):\n",
    "            no_exist_true_answer += 1\n",
    "        elif (pred_answer_after_filtering != gold_text) and (pred_answer_after_filtering == \"\"):\n",
    "            no_exist_false_answer += 1\n",
    "    \n",
    "    print(f\"Jawaban benar (answer exist) entailment: {exist_true_answer_label_entailment}\")\n",
    "    print(f\"Jawaban benar (answer exist) neutral: {exist_true_answer_label_neutral}\")\n",
    "    print(f\"Jawaban benar (answer exist) contradiction: {exist_true_answer_label_contradiction}\")\n",
    "    \n",
    "    print(f\"Jawaban salah (answer exist) entailment: {exist_false_answer_label_entailment}\")\n",
    "    print(f\"Jawaban salah (answer exist) neutral: {exist_false_answer_label_neutral}\")\n",
    "    print(f\"Jawaban salah (answer exist) contradiction: {exist_false_answer_label_contradiction}\")\n",
    "    \n",
    "    print(f\"Jawaban prediksi no-answer benar: {no_exist_true_answer}\")\n",
    "    print(f\"Jawaban prediksi no-answer salah: {no_exist_false_answer}\")\n",
    "    \n",
    "    return exist_true_answer_label_entailment, \\\n",
    "        exist_true_answer_label_neutral, \\\n",
    "        exist_true_answer_label_contradiction, \\\n",
    "        exist_false_answer_label_entailment, \\\n",
    "        exist_false_answer_label_neutral, \\\n",
    "        exist_false_answer_label_contradiction, \\\n",
    "        no_exist_true_answer, \\\n",
    "        no_exist_false_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jawaban benar (answer exist) entailment: 263\n",
      "Jawaban benar (answer exist) neutral: 0\n",
      "Jawaban benar (answer exist) contradiction: 0\n",
      "Jawaban salah (answer exist) entailment: 132\n",
      "Jawaban salah (answer exist) neutral: 0\n",
      "Jawaban salah (answer exist) contradiction: 0\n",
      "Jawaban prediksi no-answer benar: 7\n",
      "Jawaban prediksi no-answer salah: 171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(263, 0, 0, 132, 0, 0, 7, 171)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_evaluation(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05c4878c74ad419f81e6abc2b9f9fae1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0633918b582f45c38356130338ccfb58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8aa4db7376e3445089af950c446aab70",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d7c6178f42ed4b9f8c5169c21783ce2d",
      "value": 2
     }
    },
    "065d96b02c7840019af97e3c135693a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ce874e529474f478a7acd45b4ffd649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_902bd6c12ea54039b8c913a7c1782ff4",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d86de7f1fbf452892fe6d4b177558f9",
      "value": 6
     }
    },
    "10fb50db270449b48382c815904245a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc7073fb093543fc990abc620348c1ca",
      "placeholder": "",
      "style": "IPY_MODEL_acac4d071be4488fbb272df63b625213",
      "value": " 66/66 [23:11&lt;00:00, 14.47s/ba]"
     }
    },
    "113ae282ca5a499a8a2ad36a796e763f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f10632530a1b499ba3d20f0286446bab",
      "placeholder": "",
      "style": "IPY_MODEL_94da3d6dfccc4a289c659b94bcc90658",
      "value": "#0: 100%"
     }
    },
    "12c3fd408f914038acb511c4ef57f965": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13ea541bb13b42738e74885a2c1db8df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14a450c5594148c3a4ca340521d167ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13ea541bb13b42738e74885a2c1db8df",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_68818d7eea5849afa82c3003a57a5b8d",
      "value": 6
     }
    },
    "1d86de7f1fbf452892fe6d4b177558f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23ce34e28a524ad9ab44a3715c8be175": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05c4878c74ad419f81e6abc2b9f9fae1",
      "placeholder": "",
      "style": "IPY_MODEL_7fc9c87c6cc44cdf82ea4f74c0bfe7b4",
      "value": "100%"
     }
    },
    "2721b15477d741519a3041a8f256575b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a373f9cb48a481d85803f0792ff27b3",
       "IPY_MODEL_54a2637e8c8e4ca0917e35f4b58b48f3",
       "IPY_MODEL_7dc3dc97044c48afb06a14575fc8e91b"
      ],
      "layout": "IPY_MODEL_6c3c7e4da5af40b088f3046d7698edff"
     }
    },
    "29b13cbb638947d5a17071d299e2420f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_113ae282ca5a499a8a2ad36a796e763f",
       "IPY_MODEL_0ce874e529474f478a7acd45b4ffd649",
       "IPY_MODEL_935752d8733c41fbb25ed436606352ca"
      ],
      "layout": "IPY_MODEL_cf2b519cc5ff446dae45afa2439c840e"
     }
    },
    "2e0e9b7ee63b4c748d6294d63cc7b088": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd4d3c6663db46c9934c0d1f7a128d4c",
      "placeholder": "",
      "style": "IPY_MODEL_3db65eb148504de6a4207708554874f0",
      "value": " 2/2 [00:00&lt;00:00,  8.75it/s]"
     }
    },
    "3db65eb148504de6a4207708554874f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f3c8a647bb24840ae71ce6e3219f4dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23ce34e28a524ad9ab44a3715c8be175",
       "IPY_MODEL_87c26a64111847f3a6cdb2f0797ae1f5",
       "IPY_MODEL_2e0e9b7ee63b4c748d6294d63cc7b088"
      ],
      "layout": "IPY_MODEL_750e7a8018e6415f8cfc15288d80f13f"
     }
    },
    "3f6183fb8ca348e0ac35b495681c705a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45f8bfb31705441c80a22dbb955dd933": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4836cff910304df894759890642ac333": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f2a3658a0f14f66a59c14d36558c7e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86b094a2e5564e4d8af8693dab674961",
      "placeholder": "",
      "style": "IPY_MODEL_7fc619affab644758cfea5857adb7c93",
      "value": " 2/2 [00:00&lt;00:00,  4.72it/s]"
     }
    },
    "4f80ae88d884407ab8f9e08aa58632a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "540c7fcbf3b14e538cba18329bc1fc20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54a2637e8c8e4ca0917e35f4b58b48f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dce089e985634a67a381bc85cbaca017",
      "max": 66,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd9cb96d5a3c4400bb7d145590540a54",
      "value": 66
     }
    },
    "6516c5b02aa24db2bc42cdd4d1f2c260": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cbed771e77314789bf8817404e5d6f67",
       "IPY_MODEL_b0be384a887847e981868df7c551d3de",
       "IPY_MODEL_10fb50db270449b48382c815904245a8"
      ],
      "layout": "IPY_MODEL_b23174d38e3940718252770bd39177f8"
     }
    },
    "68818d7eea5849afa82c3003a57a5b8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a373f9cb48a481d85803f0792ff27b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a040ab99cb964c32a0a968d97dd55a16",
      "placeholder": "",
      "style": "IPY_MODEL_9ef8c0b24cb5490b8fa7e8ea8ad619fe",
      "value": "#0: 100%"
     }
    },
    "6c3c7e4da5af40b088f3046d7698edff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "750e7a8018e6415f8cfc15288d80f13f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b80b681b46b4671a14a7cf82fd50991": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dc3dc97044c48afb06a14575fc8e91b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f940438113734ccdb198efd12b8a3936",
      "placeholder": "",
      "style": "IPY_MODEL_c5b567cd34b340dea77cefd40e9aaec9",
      "value": " 66/66 [23:07&lt;00:00, 15.32s/ba]"
     }
    },
    "7fc619affab644758cfea5857adb7c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fc9c87c6cc44cdf82ea4f74c0bfe7b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8104b020ba8b498591e14a51af34306d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83abc2e663e447e7af2f826eacd155c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86b094a2e5564e4d8af8693dab674961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87c26a64111847f3a6cdb2f0797ae1f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb1fb197045a44ab8c204a8bdc3c4584",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4836cff910304df894759890642ac333",
      "value": 2
     }
    },
    "8aa4db7376e3445089af950c446aab70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ce95fb7cd2f4cf49b54dfa319f1e835": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83abc2e663e447e7af2f826eacd155c7",
      "placeholder": "",
      "style": "IPY_MODEL_8104b020ba8b498591e14a51af34306d",
      "value": "100%"
     }
    },
    "902bd6c12ea54039b8c913a7c1782ff4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "935752d8733c41fbb25ed436606352ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f6183fb8ca348e0ac35b495681c705a",
      "placeholder": "",
      "style": "IPY_MODEL_bdf2cb9d10cc4b2aab466a9dfe7be5bc",
      "value": " 6/6 [02:04&lt;00:00, 20.17s/ba]"
     }
    },
    "94da3d6dfccc4a289c659b94bcc90658": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c9b27a0b2ac4be3b8dcbb65e23b84a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_065d96b02c7840019af97e3c135693a8",
      "placeholder": "",
      "style": "IPY_MODEL_45f8bfb31705441c80a22dbb955dd933",
      "value": " 6/6 [02:03&lt;00:00, 20.04s/ba]"
     }
    },
    "9ef8c0b24cb5490b8fa7e8ea8ad619fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a040ab99cb964c32a0a968d97dd55a16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acac4d071be4488fbb272df63b625213": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0be384a887847e981868df7c551d3de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12c3fd408f914038acb511c4ef57f965",
      "max": 66,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_540c7fcbf3b14e538cba18329bc1fc20",
      "value": 66
     }
    },
    "b23174d38e3940718252770bd39177f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bce02c83fbc142c5bb2d8c1f905a478d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd9cb96d5a3c4400bb7d145590540a54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bdf2cb9d10cc4b2aab466a9dfe7be5bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c069967001a542f5b0c3b88ff1841eba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f46e4c24b9294ffdb25e1e64bc594eee",
       "IPY_MODEL_14a450c5594148c3a4ca340521d167ba",
       "IPY_MODEL_9c9b27a0b2ac4be3b8dcbb65e23b84a2"
      ],
      "layout": "IPY_MODEL_c99472b37d644d63a471b65ecbe3bfbd"
     }
    },
    "c5b567cd34b340dea77cefd40e9aaec9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c99472b37d644d63a471b65ecbe3bfbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb1fb197045a44ab8c204a8bdc3c4584": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbed771e77314789bf8817404e5d6f67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b80b681b46b4671a14a7cf82fd50991",
      "placeholder": "",
      "style": "IPY_MODEL_bce02c83fbc142c5bb2d8c1f905a478d",
      "value": "#1: 100%"
     }
    },
    "cf2b519cc5ff446dae45afa2439c840e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7c6178f42ed4b9f8c5169c21783ce2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dce089e985634a67a381bc85cbaca017": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd4d3c6663db46c9934c0d1f7a128d4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8eebd19f57348669da4353e690a64eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ce95fb7cd2f4cf49b54dfa319f1e835",
       "IPY_MODEL_0633918b582f45c38356130338ccfb58",
       "IPY_MODEL_4f2a3658a0f14f66a59c14d36558c7e6"
      ],
      "layout": "IPY_MODEL_f2758827907544acaa282673eebff9d0"
     }
    },
    "e9ca9f63982744aba53e883882373160": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f10632530a1b499ba3d20f0286446bab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2758827907544acaa282673eebff9d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f46e4c24b9294ffdb25e1e64bc594eee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9ca9f63982744aba53e883882373160",
      "placeholder": "",
      "style": "IPY_MODEL_4f80ae88d884407ab8f9e08aa58632a4",
      "value": "#1: 100%"
     }
    },
    "f940438113734ccdb198efd12b8a3936": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc7073fb093543fc990abc620348c1ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
