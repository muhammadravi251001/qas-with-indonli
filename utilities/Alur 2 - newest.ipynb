{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhsDF_5UdqLI"
   },
   "source": [
    "# Menjalankan QA Tanpa Intermediate Task - Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKm8qASDXqR-"
   },
   "source": [
    "# Import semua module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DH9DifnXp5_",
    "outputId": "94161dcf-e635-486d-c6d8-5747b267b64f"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install transformers\n",
    "#!pip install tensorboard\n",
    "#!pip install evaluate\n",
    "#!pip install git+https://github.com/IndoNLP/nusa-crowd.git@release_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (23.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting nusacrowd@ git+https://github.com/IndoNLP/nusa-crowd.git@7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Cloning https://github.com/IndoNLP/nusa-crowd.git (to revision 7748513d20331e72f9969f94f5d43c7f2d4a59a5) to /tmp/pip-install-_3umgsaz/nusacrowd_5508d0472177444fbb9066491fe46d18\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/IndoNLP/nusa-crowd.git /tmp/pip-install-_3umgsaz/nusacrowd_5508d0472177444fbb9066491fe46d18\n",
      "  Running command git rev-parse -q --verify 'sha^7748513d20331e72f9969f94f5d43c7f2d4a59a5'\n",
      "  Running command git fetch -q https://github.com/IndoNLP/nusa-crowd.git 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Running command git checkout -q 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Resolved https://github.com/IndoNLP/nusa-crowd.git to commit 7748513d20331e72f9969f94f5d43c7f2d4a59a5\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: aiohttp==3.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (3.8.1)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: appdirs==1.4.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: async-timeout==4.0.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (4.0.2)\n",
      "Requirement already satisfied: attrs==22.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (22.2.0)\n",
      "Requirement already satisfied: audioread==3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: bioc==1.3.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (1.3.7)\n",
      "Requirement already satisfied: black==22.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (22.12.0)\n",
      "Requirement already satisfied: cachetools==5.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (5.3.0)\n",
      "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (2022.12.7)\n",
      "Requirement already satisfied: cffi==1.15.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (1.15.1)\n",
      "Requirement already satisfied: cfgv==3.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (3.3.1)\n",
      "Requirement already satisfied: charset-normalizer==2.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 14)) (2.1.1)\n",
      "Requirement already satisfied: click==8.1.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (8.1.3)\n",
      "Requirement already satisfied: colorama==0.4.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: conllu==4.5.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 17)) (4.5.2)\n",
      "Requirement already satisfied: datasets==2.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 18)) (2.2.0)\n",
      "Requirement already satisfied: decorator==5.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 19)) (5.1.1)\n",
      "Requirement already satisfied: dill==0.3.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 20)) (0.3.6)\n",
      "Requirement already satisfied: distlib==0.3.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 21)) (0.3.6)\n",
      "Requirement already satisfied: docutils==0.19 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 22)) (0.19)\n",
      "Requirement already satisfied: et-xmlfile==1.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 23)) (1.1.0)\n",
      "Requirement already satisfied: evaluate==0.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 24)) (0.4.0)\n",
      "Requirement already satisfied: ffmpeg==1.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 25)) (1.4)\n",
      "Requirement already satisfied: filelock==3.9.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 26)) (3.9.0)\n",
      "Requirement already satisfied: flake8==6.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 27)) (6.0.0)\n",
      "Requirement already satisfied: frozenlist==1.3.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 28)) (1.3.3)\n",
      "Requirement already satisfied: fsspec==2023.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 29)) (2023.1.0)\n",
      "Requirement already satisfied: git-lfs==1.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 30)) (1.6)\n",
      "Requirement already satisfied: google-auth==2.16.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 31)) (2.16.1)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 32)) (0.4.6)\n",
      "Requirement already satisfied: grpcio==1.51.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 33)) (1.51.1)\n",
      "Requirement already satisfied: huggingface-hub==0.12.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 34)) (0.12.1)\n",
      "Requirement already satisfied: identify==2.5.18 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 35)) (2.5.18)\n",
      "Requirement already satisfied: idna==3.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 36)) (3.4)\n",
      "Requirement already satisfied: importlib-metadata==6.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 37)) (6.0.0)\n",
      "Requirement already satisfied: isort==5.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 38)) (5.12.0)\n",
      "Requirement already satisfied: joblib==1.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 39)) (1.2.0)\n",
      "Requirement already satisfied: jsonlines==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 40)) (3.1.0)\n",
      "Requirement already satisfied: librosa==0.9.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 41)) (0.9.2)\n",
      "Requirement already satisfied: llvmlite==0.39.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 42)) (0.39.1)\n",
      "Requirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 43)) (0.5.3)\n",
      "Requirement already satisfied: lxml==4.9.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 44)) (4.9.2)\n",
      "Requirement already satisfied: Markdown==3.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 45)) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe==2.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 46)) (2.1.2)\n",
      "Requirement already satisfied: mccabe==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 47)) (0.7.0)\n",
      "Requirement already satisfied: multidict==6.0.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 48)) (6.0.4)\n",
      "Requirement already satisfied: multiprocess==0.70.14 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 49)) (0.70.14)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 50)) (1.0.0)\n",
      "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 51)) (3.8.1)\n",
      "Requirement already satisfied: nodeenv==1.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 52)) (1.7.0)\n",
      "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 53)) (0.56.4)\n",
      "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 54)) (1.23.5)\n",
      "Requirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 56)) (3.2.2)\n",
      "Requirement already satisfied: openpyxl==3.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 57)) (3.1.1)\n",
      "Requirement already satisfied: packaging==23.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 58)) (23.0)\n",
      "Requirement already satisfied: pandas==1.3.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 59)) (1.3.3)\n",
      "Requirement already satisfied: pathspec==0.11.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 60)) (0.11.0)\n",
      "Requirement already satisfied: platformdirs==3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 61)) (3.0.0)\n",
      "Requirement already satisfied: pooch==1.6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 62)) (1.6.0)\n",
      "Requirement already satisfied: pre-commit==2.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 63)) (2.19.0)\n",
      "Requirement already satisfied: protobuf==4.22.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 64)) (4.22.0)\n",
      "Requirement already satisfied: pyarrow==11.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 65)) (11.0.0)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 66)) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 67)) (0.2.8)\n",
      "Requirement already satisfied: pycodestyle==2.10.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 68)) (2.10.0)\n",
      "Requirement already satisfied: pycparser==2.21 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 69)) (2.21)\n",
      "Requirement already satisfied: pyflakes==3.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 70)) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 71)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2022.7.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 72)) (2022.7.1)\n",
      "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 73)) (6.0)\n",
      "Requirement already satisfied: regex==2022.10.31 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 74)) (2022.10.31)\n",
      "Requirement already satisfied: requests==2.28.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 75)) (2.28.2)\n",
      "Requirement already satisfied: requests-oauthlib==1.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 76)) (1.3.1)\n",
      "Requirement already satisfied: resampy==0.4.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 77)) (0.4.2)\n",
      "Requirement already satisfied: responses==0.18.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 78)) (0.18.0)\n",
      "Requirement already satisfied: rsa==4.9 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 79)) (4.9)\n",
      "Requirement already satisfied: scikit-learn==1.2.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 80)) (1.2.1)\n",
      "Requirement already satisfied: scipy==1.10.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 81)) (1.10.0)\n",
      "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 82)) (1.16.0)\n",
      "Requirement already satisfied: soundfile==0.12.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 83)) (0.12.1)\n",
      "Requirement already satisfied: tensorboard==2.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 84)) (2.12.0)\n",
      "Requirement already satisfied: tensorboard-data-server==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 85)) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 86)) (1.8.1)\n",
      "Requirement already satisfied: termcolor==2.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 87)) (2.2.0)\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 88)) (3.1.0)\n",
      "Requirement already satisfied: tokenizers==0.13.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 89)) (0.13.2)\n",
      "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 90)) (0.10.2)\n",
      "Requirement already satisfied: tomli==2.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 91)) (2.0.1)\n",
      "Collecting torch==1.13.1\n",
      "  Using cached torch-1.13.1-cp38-cp38-manylinux1_x86_64.whl (887.4 MB)\n",
      "Collecting torchaudio==0.13.1\n",
      "  Using cached torchaudio-0.13.1-cp38-cp38-manylinux1_x86_64.whl (4.2 MB)\n",
      "Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 94)) (4.64.1)\n",
      "Requirement already satisfied: transformers==4.26.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 95)) (4.26.1)\n",
      "Requirement already satisfied: translate-toolkit==3.8.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 96)) (3.8.4)\n",
      "Requirement already satisfied: typing_extensions==4.5.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 97)) (4.5.0)\n",
      "Requirement already satisfied: urllib3==1.26.14 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 98)) (1.26.14)\n",
      "Requirement already satisfied: virtualenv==20.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 99)) (20.19.0)\n",
      "Requirement already satisfied: Werkzeug==2.2.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 100)) (2.2.3)\n",
      "Requirement already satisfied: wget==3.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 101)) (3.2)\n",
      "Requirement already satisfied: win32-setctime==1.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 102)) (1.1.0)\n",
      "Requirement already satisfied: xxhash==3.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 103)) (3.2.0)\n",
      "Requirement already satisfied: yarl==1.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 104)) (1.8.2)\n",
      "Requirement already satisfied: zipp==3.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 105)) (3.14.0)\n",
      "Requirement already satisfied: zstandard==0.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 106)) (0.19.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nodeenv==1.7.0->-r requirements.txt (line 52)) (45.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard==2.12.0->-r requirements.txt (line 84)) (0.34.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->-r requirements.txt (line 92)) (11.7.99)\n",
      "Installing collected packages: torch, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0\n",
      "    Uninstalling torch-2.0.0:\n",
      "      Successfully uninstalled torch-2.0.0\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.0.1\n",
      "    Uninstalling torchaudio-2.0.1:\n",
      "      Successfully uninstalled torchaudio-2.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.13.1 torchaudio-0.13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 26 10:06:48 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    57W / 300W |  32476MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   60C    P0   295W / 300W |  32292MiB / 32480MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   53C    P0   155W / 300W |  12012MiB / 32480MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    57W / 300W |   9210MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    58W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    57W / 300W |   9364MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    57W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    56W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Melihat GPU yang tersedia dan penggunaannya.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memilih GPU yang akan digunakan (contohnya: GPU #7)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pgY_FL4lXuEu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "import operator\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from evaluate import load\n",
    "from nusacrowd import NusantaraConfigHelper\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from huggingface_hub import notebook_login\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    load_from_disk,\n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "from transformers import (\n",
    "    BigBirdTokenizerFast,\n",
    "    BigBirdForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BertForSequenceClassification,\n",
    "    BertForQuestionAnswering,\n",
    "    AutoModel, \n",
    "    BertTokenizerFast,\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    BertTokenizer, \n",
    "    BertForPreTraining,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    EvalPrediction,\n",
    "    AutoModel,\n",
    "    BertModel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtu0yFqMYsX9"
   },
   "source": [
    "# Definisikan hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gjHJwpxeYugs"
   },
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"indolem/indobert-base-uncased\"\n",
    "#MODEL_NAME = \"afaji/fine-tuned-IndoNLI-Translated-with-indobert-base-uncased\"\n",
    "MODEL_NAME = \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\"\n",
    "#MODEL_NAME = \"indobenchmark/indobert-large-p2\"\n",
    "SEED = 42\n",
    "EPOCH = 1\n",
    "BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_LENGTH = 400\n",
    "STRIDE = 100\n",
    "LOGGING_STEPS = 50\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAXIMUM_SEARCH_ITER =  2\n",
    "# Untuk mempercepat training, saya ubah SAMPLE menjadi 100.\n",
    "# Bila mau menggunakan keseluruhan data, gunakan: \n",
    "SAMPLE = sys.maxsize\n",
    "# SAMPLE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyH7dlPGYOSk"
   },
   "source": [
    "# Import dataset QAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/IndoNLP/nusa-crowd.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==1.7.0 torchvision==0.8.1 -f https://download.pytorch.org/whl/cu101/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#from tensorflow.python.platform import build_info as build\n",
    "#print(f\"tensorflow version: {tf.__version__}\")\n",
    "#print(f\"Cuda Version: {build.build_info['cuda_version']}\")\n",
    "#print(f\"Cudnn version: {build.build_info['cudnn_version']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\r\n",
      "Built on Mon_Sep_13_19:13:29_PDT_2021\r\n",
      "Cuda compilation tools, release 11.5, V11.5.50\r\n",
      "Build cuda_11.5.r11.5/compiler.30411180_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall torch\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 26 10:06:53 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    57W / 300W |  32476MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   59C    P0   242W / 300W |  32248MiB / 32480MiB |     99%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   52C    P0    86W / 300W |  12012MiB / 32480MiB |     96%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    57W / 300W |   9210MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    58W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    57W / 300W |   9364MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    56W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    56W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cudatoolkit==11.5 (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cudatoolkit==11.5\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install cudatoolkit==11.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conda in /usr/local/lib/python3.8/dist-packages (4.3.16)\n",
      "Requirement already satisfied: ruamel.yaml>=0.11.14 in /usr/local/lib/python3.8/dist-packages (from conda) (0.17.21)\n",
      "Requirement already satisfied: pycosat>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from conda) (0.6.3)\n",
      "Requirement already satisfied: requests>=2.12.4 in /usr/local/lib/python3.8/dist-packages (from conda) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.12.4->conda) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.12.4->conda) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.12.4->conda) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.12.4->conda) (2022.12.7)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/local/lib/python3.8/dist-packages (from ruamel.yaml>=0.11.14->conda) (0.2.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: The install method you used for conda--probably either `pip install conda`\r\n",
      "or `easy_install conda`--is not compatible with using conda as an application.\r\n",
      "If your intention is to install conda as a standalone application, currently\r\n",
      "supported install methods include the Anaconda installer and the miniconda\r\n",
      "installer.  You can download the miniconda installer from\r\n",
      "https://conda.io/miniconda.html.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda install cudatoolkit=11.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch._C._cuda_getDeviceCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlNhPlmDY2tk"
   },
   "source": [
    "# Definisikan tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "z3uXWOkUY4GD"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYYbFXzWYTr3"
   },
   "source": [
    "# Definisikan fungsi pre-processnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset idk_mrc/idk_mrc_source to /root/.cache/huggingface/datasets/idk_mrc/idk_mrc_source/1.0.0/cf468d86fa7341e69998db1449851672ebfb4fa46036929d66b9de15c421334f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd38e66bef4487b9682369975a2964a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.17M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f13db1e5034c2f824d100ee5397f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/108k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adc1be8c6734943a41da58dc09dac05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/119k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset idk_mrc downloaded and prepared to /root/.cache/huggingface/datasets/idk_mrc/idk_mrc_source/1.0.0/cf468d86fa7341e69998db1449851672ebfb4fa46036929d66b9de15c421334f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba31e9993144166a768b924e9827609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 358/358 [00:01<00:00, 258.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3659/3659 [00:16<00:00, 225.65it/s]\n"
     ]
    }
   ],
   "source": [
    "conhelps = NusantaraConfigHelper()\n",
    "data_qas_id = conhelps.filtered(lambda x: 'idk_mrc' in x.dataset_name)[0].load_dataset()\n",
    "\n",
    "df_train = pd.DataFrame(data_qas_id['train'])\n",
    "df_validation = pd.DataFrame(data_qas_id['validation'])\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_val = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_validation['context']))):\n",
    "    for j in df_validation[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                       \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                       \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                       ignore_index=True)\n",
    "        else:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": str(), \n",
    "                                                       \"answer_start\": 0, \n",
    "                                                       \"answer_end\": 0}}, \n",
    "                                                       ignore_index=True)\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_train = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_train['context']))):\n",
    "    for j in df_train[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                           \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                           \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                           ignore_index=True)\n",
    "        else:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": str(), \n",
    "                                                           \"answer_start\": 0, \n",
    "                                                           \"answer_end\": 0}}, \n",
    "                                                           ignore_index=True)\n",
    "\n",
    "train_dataset = Dataset.from_dict(new_df_train)\n",
    "validation_dataset = Dataset.from_dict(new_df_val)\n",
    "\n",
    "data_qas_id = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "EuSjYelvXJ9x"
   },
   "outputs": [],
   "source": [
    "def rindex(lst, value, operator=operator):\n",
    "      return len(lst) - operator.indexOf(reversed(lst), value) - 1\n",
    "\n",
    "def preprocess_function_qa(examples, tokenizer, MAX_LENGTH=MAX_LENGTH, STRIDE=STRIDE, rindex=rindex, operator=operator):\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    examples[\"context\"] = [c.lstrip() for c in examples[\"context\"]]\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "      examples['question'],\n",
    "      examples['context'],\n",
    "      truncation=True,\n",
    "      max_length = MAX_LENGTH,\n",
    "      stride=STRIDE,\n",
    "      return_overflowing_tokens=True,\n",
    "      return_offsets_mapping=True,\n",
    "      padding=\"max_length\",\n",
    "      return_tensors='np'\n",
    "    )\n",
    "\n",
    "    tokenized_examples['start_positions'] = []\n",
    "    tokenized_examples['end_positions'] = []\n",
    "\n",
    "    for seq_idx in range(len(tokenized_examples['input_ids'])):\n",
    "        seq_ids = tokenized_examples.sequence_ids(seq_idx)\n",
    "        offset_mappings = tokenized_examples['offset_mapping'][seq_idx]\n",
    "\n",
    "        cur_example_idx = tokenized_examples['overflow_to_sample_mapping'][seq_idx]\n",
    "\n",
    "        #answer = examples['answer'][seq_idx][0]\n",
    "        answer = examples['answer'][cur_example_idx]\n",
    "        answer = eval(str(answer))\n",
    "        #answer_text = answer['text'][0]\n",
    "        answer_start = answer['answer_start']\n",
    "        #answer_end = answer_start + len(answer_text)\n",
    "        answer_end = answer['answer_end']\n",
    "\n",
    "        context_pos_start = seq_ids.index(1)\n",
    "        context_pos_end = rindex(seq_ids, 1, operator)\n",
    "\n",
    "        s = e = 0\n",
    "        if (offset_mappings[context_pos_start][0] <= answer_start and\n",
    "            offset_mappings[context_pos_end][1] >= answer_end):\n",
    "          i = context_pos_start\n",
    "          while offset_mappings[i][0] < answer_start:\n",
    "            i += 1\n",
    "          if offset_mappings[i][0] == answer_start:\n",
    "            s = i\n",
    "          else:\n",
    "            s = i - 1\n",
    "\n",
    "          j = context_pos_end\n",
    "          while offset_mappings[j][1] > answer_end:\n",
    "            j -= 1      \n",
    "          if offset_mappings[j][1] == answer_end:\n",
    "            e = j\n",
    "          else:\n",
    "            e = j + 1\n",
    "\n",
    "        tokenized_examples['start_positions'].append(s)\n",
    "        tokenized_examples['end_positions'].append(e)\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0o9vkQ1YaO2"
   },
   "source": [
    "# Mulai tokenisasi dan pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "2721b15477d741519a3041a8f256575b",
      "6a373f9cb48a481d85803f0792ff27b3",
      "54a2637e8c8e4ca0917e35f4b58b48f3",
      "7dc3dc97044c48afb06a14575fc8e91b",
      "6c3c7e4da5af40b088f3046d7698edff",
      "a040ab99cb964c32a0a968d97dd55a16",
      "9ef8c0b24cb5490b8fa7e8ea8ad619fe",
      "dce089e985634a67a381bc85cbaca017",
      "bd9cb96d5a3c4400bb7d145590540a54",
      "f940438113734ccdb198efd12b8a3936",
      "c5b567cd34b340dea77cefd40e9aaec9",
      "6516c5b02aa24db2bc42cdd4d1f2c260",
      "cbed771e77314789bf8817404e5d6f67",
      "b0be384a887847e981868df7c551d3de",
      "10fb50db270449b48382c815904245a8",
      "b23174d38e3940718252770bd39177f8",
      "7b80b681b46b4671a14a7cf82fd50991",
      "bce02c83fbc142c5bb2d8c1f905a478d",
      "12c3fd408f914038acb511c4ef57f965",
      "540c7fcbf3b14e538cba18329bc1fc20",
      "fc7073fb093543fc990abc620348c1ca",
      "acac4d071be4488fbb272df63b625213",
      "29b13cbb638947d5a17071d299e2420f",
      "113ae282ca5a499a8a2ad36a796e763f",
      "0ce874e529474f478a7acd45b4ffd649",
      "935752d8733c41fbb25ed436606352ca",
      "cf2b519cc5ff446dae45afa2439c840e",
      "f10632530a1b499ba3d20f0286446bab",
      "94da3d6dfccc4a289c659b94bcc90658",
      "902bd6c12ea54039b8c913a7c1782ff4",
      "1d86de7f1fbf452892fe6d4b177558f9",
      "3f6183fb8ca348e0ac35b495681c705a",
      "bdf2cb9d10cc4b2aab466a9dfe7be5bc",
      "c069967001a542f5b0c3b88ff1841eba",
      "f46e4c24b9294ffdb25e1e64bc594eee",
      "14a450c5594148c3a4ca340521d167ba",
      "9c9b27a0b2ac4be3b8dcbb65e23b84a2",
      "c99472b37d644d63a471b65ecbe3bfbd",
      "e9ca9f63982744aba53e883882373160",
      "4f80ae88d884407ab8f9e08aa58632a4",
      "13ea541bb13b42738e74885a2c1db8df",
      "68818d7eea5849afa82c3003a57a5b8d",
      "065d96b02c7840019af97e3c135693a8",
      "45f8bfb31705441c80a22dbb955dd933"
     ]
    },
    "id": "uVGfobd8XNIF",
    "outputId": "9b911f15-dce5-456f-a0fd-446f9615be1c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'fn_kwargs'={'tokenizer': BertTokenizerFast(name_or_path='afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05', vocab_size=31923, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), 'MAX_LENGTH': 400, 'STRIDE': 100, 'rindex': <function rindex at 0x7f2cd87e38b0>, 'operator': <module 'operator' from '/usr/lib/python3.8/operator.py'>} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351f584958f04f70b95adf8e1e3c381a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f941a1e18f348aa864a3803f3db256e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data_qas_id = data_qas_id.map(\n",
    "    preprocess_function_qa,\n",
    "    batched=True,\n",
    "    remove_columns=data_qas_id['train'].column_names,\n",
    "    num_proc=1,\n",
    "    fn_kwargs={'tokenizer': tokenizer, 'MAX_LENGTH': MAX_LENGTH, 'STRIDE': STRIDE, 'rindex': rindex, 'operator': operator}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_qas_id = tokenized_data_qas_id.remove_columns([\"offset_mapping\", \n",
    "                                            \"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Fk1RbGEeldvi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_data_qas_id.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SLfcGWQEjd36"
   },
   "outputs": [],
   "source": [
    "tokenized_data_qas_id_train = Dataset.from_dict(tokenized_data_qas_id[\"train\"][:SAMPLE])\n",
    "tokenized_data_qas_id_validation = Dataset.from_dict(tokenized_data_qas_id[\"validation\"][:SAMPLE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06_Y5Xw9bLj3"
   },
   "source": [
    "# Mendefinisikan argumen (dataops) untuk training nanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "foTQgHuujf46"
   },
   "outputs": [],
   "source": [
    "TIME_NOW = str(datetime.now()).replace(\":\", \"-\").replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "QA = './results/alur2-idk-mrc'\n",
    "CHECKPOINT_DIR = f'{QA}-{TIME_NOW}/checkpoint/'\n",
    "MODEL_DIR = f'{QA}-{TIME_NOW}/model/'\n",
    "OUTPUT_DIR = f'{QA}-{TIME_NOW}/output/'\n",
    "ACCURACY_DIR = f'{QA}-{TIME_NOW}/accuracy/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9YdVKNRbPHk"
   },
   "source": [
    "# Mendefinisikan Training Arguments untuk train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3VMs4pBjgXE",
    "outputId": "577270d3-fe73-44be-f1c2-e710bb5e1d1b"
   },
   "outputs": [],
   "source": [
    "training_args_qa = TrainingArguments(\n",
    "    \n",
    "    # Checkpoint\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=EPOCH,\n",
    "    \n",
    "    # Log\n",
    "    report_to='tensorboard',\n",
    "    logging_strategy='steps',\n",
    "    logging_first_step=True,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    \n",
    "    # Train\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    bf16=False,\n",
    "    dataloader_num_workers=cpu_count(),\n",
    "    \n",
    "    # Miscellaneous\n",
    "    evaluation_strategy='epoch',\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INibCqT5bR3d"
   },
   "source": [
    "# Pendefinisian model Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqTEhdqqtL4z",
    "outputId": "5b4394f3-63f5-4610-e72d-d383d9b8940a"
   },
   "outputs": [],
   "source": [
    "model_qa = BertForQuestionAnswering.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qa = model_qa.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyJvhxZ6bbbF"
   },
   "source": [
    "# Melakukan pengumpulan data dengan padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "rGeJNonStSXT"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nF_E_CHbb8m"
   },
   "source": [
    "# Mulai training untuk fine-tune SQUAD diatas IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import collections\n",
    "\n",
    "# # Melakukan evaluasi dari prediksi\n",
    "def normalize_text(s):\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_f1_prec_rec(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split() # True positive + False positive = Untuk precision\n",
    "    gold_tokens = normalize_text(gold).split() # True positive + False negatives = Untuk recall\n",
    "    common = collections.Counter(pred_tokens) & collections.Counter(gold_tokens)\n",
    "    num_same = sum(common.values()) # True positive\n",
    "\n",
    "    if len(gold_tokens) == 0 or len(pred_tokens) == 0: \n",
    "        return int(gold_tokens == pred_tokens)\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(pred_tokens)\n",
    "    recall = 1.0 * num_same / len(gold_tokens)\n",
    "    f1 = (2.0 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1\n",
    "\n",
    "def compute_metrics(predict_result):\n",
    "    predictions_idx = np.argmax(predict_result.predictions, axis=2)\n",
    "    denominator = len(predictions_idx[0])\n",
    "    label_array = np.asarray(predict_result.label_ids)\n",
    "    total_correct = 0\n",
    "    f1_array = []\n",
    "    precision_array = []\n",
    "    recall_array = []\n",
    "\n",
    "    for i in range(len(predict_result.predictions[0])):\n",
    "        start_pred_idx = predictions_idx[0][i]\n",
    "        end_pred_idx = predictions_idx[1][i] + 1\n",
    "        start_gold_idx = label_array[0][i]\n",
    "        end_gold_idx = label_array[1][i] + 1\n",
    "\n",
    "        pred_text = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                    [start_pred_idx: end_pred_idx])\n",
    "        gold_text = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                    [start_gold_idx: end_gold_idx])\n",
    "\n",
    "        if pred_text == gold_text:\n",
    "            total_correct += 1\n",
    "\n",
    "        f1 = compute_f1_prec_rec(pred=pred_text, gold=gold_text)\n",
    "\n",
    "        f1_array.append(f1)\n",
    "\n",
    "    exact_match = ((total_correct / denominator) * 100.0)\n",
    "    final_f1 = np.mean(f1_array) * 100.0\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': final_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "G6pLmNzCjhra"
   },
   "outputs": [],
   "source": [
    "trainer_qa = Trainer(\n",
    "    model=model_qa,\n",
    "    args=training_args_qa,\n",
    "    #train_dataset=tokenized_data_qas_id_train,\n",
    "    #eval_dataset=tokenized_data_qas_id_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    #data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qp96lB8vjtXG",
    "outputId": "b118d7ac-a90d-43c2-bd8b-ee0a0c4ab8de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainer_qa.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DreIeglDbrHY"
   },
   "source": [
    "# Menyimpan model Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "KJick7YDjuoq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/alur2-idk-mrc-2023-03-26_10-07-40_084469/model/\n",
      "Configuration saved in ./results/alur2-idk-mrc-2023-03-26_10-07-40_084469/model/config.json\n",
      "Model weights saved in ./results/alur2-idk-mrc-2023-03-26_10-07-40_084469/model/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/alur2-idk-mrc-2023-03-26_10-07-40_084469/model/tokenizer_config.json\n",
      "Special tokens file saved in ./results/alur2-idk-mrc-2023-03-26_10-07-40_084469/model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer_qa.save_model(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX-y8EbobugF"
   },
   "source": [
    "# Melakukan prediksi dari model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "6aE7w2yMkWxj",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 764\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=(array([[-8.514204 , -8.426989 , -8.106972 , ..., -8.405422 , -8.40814  ,\n",
       "        -9.136951 ],\n",
       "       [-7.3476777, -7.306528 , -4.8059587, ..., -9.1575775, -9.139073 ,\n",
       "        -9.153362 ],\n",
       "       [-7.382496 , -7.642652 , -4.0328264, ..., -8.926677 , -8.9915085,\n",
       "        -8.875996 ],\n",
       "       ...,\n",
       "       [-7.005841 , -7.613452 , -8.198171 , ..., -7.45728  , -7.7524405,\n",
       "        -6.498203 ],\n",
       "       [-7.6022134, -7.4058123, -8.244312 , ..., -8.298668 , -8.298182 ,\n",
       "        -7.5270724],\n",
       "       [-7.6271653, -7.4068747, -8.218372 , ..., -8.294968 , -8.294479 ,\n",
       "        -7.605162 ]], dtype=float32), array([[-8.534399 , -8.604107 , -8.806009 , ..., -7.257086 , -7.261894 ,\n",
       "        -8.723284 ],\n",
       "       [-6.931199 , -7.8831453, -6.8691025, ..., -8.48402  , -8.358842 ,\n",
       "        -8.258781 ],\n",
       "       [-7.578419 , -8.0381155, -6.6029034, ..., -8.459461 , -8.628995 ,\n",
       "        -8.426313 ],\n",
       "       ...,\n",
       "       [-6.358011 , -7.654443 , -7.941966 , ..., -8.342517 , -7.7406406,\n",
       "        -8.05034  ],\n",
       "       [-7.4747896, -7.679789 , -8.047646 , ..., -7.1712346, -7.169904 ,\n",
       "        -7.711631 ],\n",
       "       [-7.535035 , -7.7101483, -8.100753 , ..., -7.1901426, -7.1887636,\n",
       "        -7.8053102]], dtype=float32)), label_ids=(array([ 19,  71,  13,  25,  10,  15,  14,  48,   9,  29,   6,  12,   8,\n",
       "        13,   9,  10,  12,  11,  31,  13,  14,  10,  15,  12,  13,  12,\n",
       "        26,   9,  26,  21,  10,  27,  10,  37,  10,  20,   8,  27,  15,\n",
       "        24,   7,  51,   8,  54,   8, 106,  15,  15,  30,   8,  24,  29,\n",
       "        11,  14,   8,  77,  47,  15,  11,  16,  11,  16,  51,  12,  20,\n",
       "        28,  11,  26,  11,  37,   8,  80,   9,   9,   8,  31,  10,  48,\n",
       "        12,  40,  58,   7,  76,   9,   8,   8,   8,  82,  10,  72,   9,\n",
       "        33,  10,  10,  10,  17,  21,  11,  10,  35,   8,  12,   7,  47,\n",
       "        10,  12,   8,  12,  40,  13,  31,  13, 136,  13,   9,  16,  43,\n",
       "         9,  25,  10,  12,  10,  14,  40,  10,  47,  20,  45,   9,  29,\n",
       "        14,  10,  12,  38,   9,  38,   9, 133,  13, 134,  13,  57,   7,\n",
       "        18,  19,  12,  11, 183,  15,  12,  13,  11,   9,  55,  13,  14,\n",
       "        86,   8,   9,   9,  17,  16,  14,   9,  24,  21,   9,  61,  12,\n",
       "        20,   8,   7,   7,   8,  17,   8,   8,   9, 364,   9,  60,  13,\n",
       "        11,  11,  13,  22,  11,  20,  15,  92,  12,  79,  12,  81,   9,\n",
       "         9,   8,   8,   7,  12,  14,  16,  10,  31,  10,   8,  11,  16,\n",
       "        11,  65,  10,  11,  11, 155,  11,  13,  16,  27,  11,  11,  23,\n",
       "        10,  15,  12,   8,  12,  10,   7,  31,   9,  15,   7,  13,   9,\n",
       "       142,   9, 136,  10,   9,  12,   9,  29,   6,   7,   6,   7,  37,\n",
       "        10,  20,   9, 127,   8,   9,   9,   8,  10,   9,   9,  11,  12,\n",
       "         9,  13,  38,   9,  11,  12,  11,  23, 186,  15,   9,  36,   7,\n",
       "        31,  25,  15,  35,  10,  24,  14,  35,   7,  57,   8,  96,   8,\n",
       "        95,   7,  10,  11,  10,  15,   9,   7,  10,   8,  24,  11,  24,\n",
       "         8,  20,  10,   9,  12,  10,  13,   8,  31,   9,  51,  24,   8,\n",
       "         9,  13,   9,  22,  10,  41,  10, 106,  21,   7,   7,  17,   9,\n",
       "         7,   7,  68,  10,  28,  11,  76,  12,  96,  10,  11,  14, 150,\n",
       "        13,  35,  10,  74,  15,  20,  10, 110,  14,  92,  11,  30,  10,\n",
       "       108, 146,  10,  14,  25,  10,  31,  15,  14,  11,  14,  12,  11,\n",
       "        23,  57,   8,  50,  13,  46,  10,  12,   7,  13,  10,  10,   9,\n",
       "         9,   9,   8,   8,  26,   9,  91,  11, 112,  14,  13,  10,   9,\n",
       "         5,   9,  12,   8,  41,  38,   7,  21,   6,  44,   7, 125,  11,\n",
       "        37,  12,   9,  16,  10,  21,  10,  10,  38,   9,  13,   8,  25,\n",
       "        10,  30,  10,  45,  10,  36,   8,  11,  16,  18,  11,   9,  12,\n",
       "        71,  13,  47,  10,   9,   9,  12,  10,  53,   8,  18,   7,  26,\n",
       "        10,  41,  10,  12, 106,  12,  34,  13,  13,  10,  46,  15,  44,\n",
       "         8,   9,  12,  76,   8, 123,   6,  19,  10,  10,  60,   9,  42,\n",
       "         7,   8,   9,  31,   9,  11, 220,  11,  10,  13, 164,  10,  26,\n",
       "        11,  48,  11,  11,   7,  10,  85,   9,  18,  11,  82,   8,   8,\n",
       "         8,  32,  15,  17,  10,   8,   8,  51,   9,  52,  10,   9,   9,\n",
       "        44,  12,  10,  39,  13,  52,  11,  13,  21,  20,   9,  10,   6,\n",
       "       107,  13,  37,   7,  18,   7,  13,  13,  48,   7,  39,  11,  13,\n",
       "         7,   8,  27,  17,  39,  11,  13,   9,  12,   7,  18,   9,   7,\n",
       "        10,  10,  15,  12,  22,  13,  12,  43,   8,  11,  21,  43,  44,\n",
       "        18,  14,  26,   8,  22,  12,  11,   9,  11,  30,   9,  73,  14,\n",
       "        14,  12,  15,  17,  18,   9,  39,  12,  13,  11,   9,  12,   8,\n",
       "        10,  39,  10,  48, 178,  11,  47,   9,  14,   8,  38,  25,   9,\n",
       "         8,  42,  56,   9,  20,   9,  57,  12,  21,  22,   8,   9,  21,\n",
       "        11,  46,  10,  12, 101,  12,  14,  21,  11,   9,  78,  11,  13,\n",
       "        35,  15,  29,  11,  13,  15,  15,  11,   8,   8,  36,   8,  19,\n",
       "        12,  14,  21,  12,   9,  14,  78,  19,  13,   9,  37,  11,   9,\n",
       "        43,  21,  12,  17,  10,  49,  38,   9,  11,  11,   8,  53,  24,\n",
       "        12,  35,   7,  15,  15,   8, 222,  14,  20,  24,  10,  10, 125,\n",
       "        52,  14,  25,  13,  10,  29,  24,  28,   7, 110,  10,  89,  10,\n",
       "        14,  16,  23,   6,  38,  17,  71,  12,  10,  11,  14,  19,  11,\n",
       "       223,  26,  65,   9,  72,  44,  17,  10,  31,   9,  33,  10,  33,\n",
       "        19,  11,  42,  91,  11,  26,   9,  30,   7,  13, 151, 174,  11,\n",
       "        18,  15,  12,   7,  50,   8,  27,  12,  56,  11,  58,  18,  59,\n",
       "       222, 223,  11,  42,  77,   9,  30,   8,  76,  83,  47,  88,  11,\n",
       "        35,  34,  10,  45,   7,  18,   9,  11,  16,  15]), array([ 22,  73,  12,  32,   9,  15,  13,  78,   8,  29,   5,  52,   7,\n",
       "        14,   8,   9,  23,  10,  33,  12,  16,   9,  28,  11,  21,  11,\n",
       "        36,   8,  28,  22,   9,  29,   9,  38,   9,  20,   7,  54,  14,\n",
       "        26,   6,  76,   7,  61,   7, 106,  14,  14,  31,   7,  48,  29,\n",
       "        10,  17,   7,  78,  48,  14,  10,  21,  10,  33,  53,  11,  24,\n",
       "        34,  10,  27,  10,  40,   7,  81,   8,   9,   7,  34,   9,  59,\n",
       "        11,  53,  60,   6,  79,   8,   7,   7,   7,  88,   9, 103,   8,\n",
       "        35,   9,   9,   9,  34,  32,  10,   9,  41,   7,  12,   6,  60,\n",
       "         9,  15,   7,  14,  41,  12,  70,  12, 137,  12,   8,  15,  56,\n",
       "         8,  32,   9,  12,   9,  13,  50,   9,  47,  19,  46,   8,  29,\n",
       "        13,   9,  11,  39,   8,  39,   8, 133,  12, 138,  12,  81,   6,\n",
       "        20,  18,  19,  10, 187,  14,  11,  12,  11,   8,  55,  12,  13,\n",
       "        86,   7,  11,   8,  19,  15,  28,   8,  34,  21,   8,  61,  11,\n",
       "        20,   7,  16,   6,   8,  18,   7,   7,   8, 366,   8,  63,  12,\n",
       "        19,  10,  12,  56,  10,  20,  14, 111,  11,  83,  11,  83,   8,\n",
       "        12,   7,   9,   6,  13,  13,  18,   9,  31,   9,   9,  10,  18,\n",
       "        10,  65,   9,  10,  10, 156,  10,  12,  15,  99,  10,  10,  30,\n",
       "         9,  51,  11,   7,  11,  19,   6,  32,   8,  20,   6,  33,   8,\n",
       "       144,   8, 136,   9,   8,  11,   8,  31,   5,   6,   5,   6,  37,\n",
       "         9,  24,   8, 132,   7,  20,   8,  19,  11,   8,   8,  10,  21,\n",
       "         8,  12,  39,   8,  14,  11,  10,  23, 196,  34,   8,  41,   6,\n",
       "        32,  26,  14,  42,   9,  24,  13,  59,   6,  65,   7,  97,   7,\n",
       "        96,   6,  16,  10,  40,  14,   8,   6,  55,   7,  26,  10,  25,\n",
       "         7,  21,   9,   8,  14,   9,  30,   7,  32,   8,  51,  24,   7,\n",
       "         8,  15,   8,  22,   9,  59,   9, 122,  20,   8,   6,  19,   8,\n",
       "        10,   6,  70,   9,  30,  10,  90,  11,  99,   9,  10,  13, 152,\n",
       "        12,  45,   9,  84,  14,  20,   9, 112,  13,  92,  10,  36,   9,\n",
       "       110, 161,   9,  13,  38,   9,  49,  16,  13,  10,  31,  11,  10,\n",
       "        23,  59,   7,  50,  12,  50,   9,  14,   6,  18,   9,  37,   8,\n",
       "        11,   8,   9,   7,  28,   8,  91,  10, 114,  13,  12,   9,  22,\n",
       "         4,  18,  11,   7,  44,  43,   6,  23,   5,  48,   6, 129,  10,\n",
       "        41,  12,   8,  27,   9,  25,   9,   9,  38,   8,  27,   7,  27,\n",
       "         9,  38,   9,  47,   9,  36,   7,  15,  15,  29,  32,   8,  13,\n",
       "        74,  12,  47,   9,  15,   8,  41,   9,  58,   7,  32,   6,  29,\n",
       "         9,  61,   9,  22, 109,  11,  46,  12,  55,   9,  47,  14,  64,\n",
       "         7,   8,  11,  84,   7, 128,   5,  21,   9,  15,  60,   8,  44,\n",
       "         6,  10,   8,  31,   8,  10, 242,  10,   9,  12, 169,   9,  27,\n",
       "        10,  72,  10,  76,   6,   9,  86,   8,  18,  10,  82,   7,  17,\n",
       "         7,  33,  14,  17,   9,   7,   7,  52,   8,  53,   9,  10,   8,\n",
       "        51,  11,   9,  70,  12,  54,  10,  38,  24,  19,   8,  16,   5,\n",
       "       108,  12,  53,   6,  22,   6,  14,  12,  51,   6,  41,  10,  14,\n",
       "         6,   7,  39,  16,  46,  10,  17,   8,  36,   6,  40,   8,   6,\n",
       "        12,   9,  17,  11,  23,  14,  11,  49,   7,  10,  25,  48,  49,\n",
       "        22,  13,  28,   7,  30,  11,  13,   8,  10,  38,   8,  76,  15,\n",
       "        15,  11,  14,  47,  48,   8,  46,  32,  33,  31,   8,  11,   7,\n",
       "         9,  41,   9,  58, 202,  10,  48,   8,  14,   7,  42,  31,   8,\n",
       "        10,  46,  59,   8,  21,   8,  62,  11,  24,  25,   7,   8,  27,\n",
       "        10,  49,   9,  11, 105,  11,  13,  24,  10,   8,  86,  10,  12,\n",
       "        38,  14,  38,  10,  12,  21,  20,  19,  16,   7,  37,   7,  19,\n",
       "        11,  13,  22,  28,   8,  13,  83,  18,  16,   8,  38,  12,   8,\n",
       "        65,  22,  11,  16,   9,  58,  39,  12,  11,  10,   9,  55,  25,\n",
       "        11,  39,   6,  27,  37,  11, 223,  13,  20,  31,   9,   9, 125,\n",
       "        59,  13,  25,  28,   9,  30,  32,  56,   6, 113,   9,  93,   9,\n",
       "        13,  18,  25,   5,  43,  24,  83,  11,  11,  10,  13,  36,  10,\n",
       "       232,  39,  77,   8,  74,  53,  24,   9,  34,   8,  36,   9,  56,\n",
       "        19,  10,  42,  92,  10,  36,   8,  39,   6,  15, 155, 178,  10,\n",
       "        25,  14,  18,   6,  52,   7,  43,  11,  60,  10,  58,  20,  61,\n",
       "       225, 224,  10,  44,  77,   8,  35,   7,  89,  85,  49,  91,  10,\n",
       "        37,  36,   9,  50,   6,  20,  13,  10,  23,  14])), metrics={'test_loss': 6.006591796875, 'test_runtime': 92.1594, 'test_samples_per_second': 8.29, 'test_steps_per_second': 1.042})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result = trainer_qa.predict(tokenized_data_qas_id_validation)\n",
    "predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "with open(f'{OUTPUT_DIR}/output.txt', \"w\") as f:\n",
    "  f.write(str(predict_result))\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2BS4XC5byxE"
   },
   "source": [
    "# Melakukan evaluasi dari prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "9J-zKl_zkUze"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 36.518324607329845, 'f1': 42.63542386279981}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_result_before_filtering = compute_metrics(predict_result)\n",
    "metric_result_before_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(ACCURACY_DIR), exist_ok=True)\n",
    "with open(f'{ACCURACY_DIR}/accuracy.txt', \"w\") as f:\n",
    "  f.write(str(metric_result_before_filtering))\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coba Alur 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_qas_dataframe(predict_result=predict_result, index_largest=1):\n",
    "    predictions_idx = np.argsort(predict_result.predictions, axis=2)[:, :, index_largest * -1]\n",
    "    #predictions_idx = np.argmax(predict_result.predictions, axis=2)\n",
    "    label_array = np.asarray(predict_result.label_ids)\n",
    "    question_decoded = []\n",
    "    context_decoded = []\n",
    "    pred_answer_decoded = []\n",
    "    gold_answer_decoded = []\n",
    "    \n",
    "    for i in tqdm(range(len(predict_result.predictions[0]))):\n",
    "        start_pred_idx = predictions_idx[0][i]\n",
    "        end_pred_idx = predictions_idx[1][i] + 1\n",
    "        pred_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_pred_idx: end_pred_idx], skip_special_tokens=True)\n",
    "        pred_answer_decoded.append(pred_answer)\n",
    "        \n",
    "        start_gold_idx = label_array[0][i]\n",
    "        end_gold_idx = label_array[1][i] + 1\n",
    "        gold_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_gold_idx: end_gold_idx], skip_special_tokens=True)\n",
    "        gold_answer_decoded.append(gold_answer)\n",
    "        \n",
    "        question = []\n",
    "        context = []\n",
    "         \n",
    "        for j in range(len(tokenized_data_qas_id_validation[i]['token_type_ids'])):\n",
    "            if tokenized_data_qas_id_validation[i]['token_type_ids'][j] == 0:\n",
    "                question.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "            else:\n",
    "                context.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "\n",
    "        question_decoded.append(tokenizer.decode(question, skip_special_tokens=True))\n",
    "        context_decoded.append(tokenizer.decode(context, skip_special_tokens=True))\n",
    "    \n",
    "    qas_df = pd.DataFrame({'Context': context_decoded, \n",
    "                           'Question': question_decoded, \n",
    "                           'Prediction Answer': pred_answer_decoded,\n",
    "                          'Gold Answer': gold_answer_decoded})\n",
    "                      \n",
    "    return qas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 764/764 [05:00<00:00,  2.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Question</th>\n",
       "      <th>Prediction Answer</th>\n",
       "      <th>Gold Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sistem pemosisi global [ 1 ] ( bahasa inggris ...</td>\n",
       "      <td>apa kepanjangan dari gps?</td>\n",
       "      <td>global positioning system</td>\n",
       "      <td>global positioning system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ukuran reptil bervariasi, dari yang berukuran ...</td>\n",
       "      <td>apakah cabang ilmu pengetahuan alam yang mempe...</td>\n",
       "      <td>herpetologi</td>\n",
       "      <td>herpetologi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ukuran reptil bervariasi, dari yang berukuran ...</td>\n",
       "      <td>apa cabang ilmu pengetahuan alam yang tidak me...</td>\n",
       "      <td>herpetologi</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reptil ( binatang melata, atau dalam bahasa la...</td>\n",
       "      <td>apakah maksud reptil dalam bahasa latin?</td>\n",
       "      <td>kelompok hewan vertebrata berdarah dingin dan ...</td>\n",
       "      <td>' melata'atau'merayap '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reptil ( binatang melata, atau dalam bahasa la...</td>\n",
       "      <td>apakah maksud reptil ganas dalam bahasa latin?</td>\n",
       "      <td>kelompok hewan vertebrata berdarah dingin dan ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>realme meluncurkan smartphone pertamanya \" rea...</td>\n",
       "      <td>apakah smartphone pertama yang diproduksi realme?</td>\n",
       "      <td></td>\n",
       "      <td>realme 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>thariq bin ziyad lebih banyak dikenal sebagai ...</td>\n",
       "      <td>siapa yang dikenal sebagai penakluk spanyol?</td>\n",
       "      <td>thariq bin ziyad</td>\n",
       "      <td>thariq bin ziyad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>thariq bin ziyad lebih banyak dikenal sebagai ...</td>\n",
       "      <td>siapa yang dikenal sebagai penakluk spanyol pa...</td>\n",
       "      <td>thariq bin ziyad</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>adalah angkatan laut kekaisaran jepang ( kaigu...</td>\n",
       "      <td>siapa yang menggunakan kapal induk secara efek...</td>\n",
       "      <td>angkatan laut kekaisaran jepang</td>\n",
       "      <td>angkatan laut kekaisaran jepang ( kaigun )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>adalah angkatan laut kekaisaran jepang ( kaigu...</td>\n",
       "      <td>siapa yang menggunakan kapal induk secara efek...</td>\n",
       "      <td>angkatan laut kekaisaran jepang</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>764 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Context  \\\n",
       "0    sistem pemosisi global [ 1 ] ( bahasa inggris ...   \n",
       "1    ukuran reptil bervariasi, dari yang berukuran ...   \n",
       "2    ukuran reptil bervariasi, dari yang berukuran ...   \n",
       "3    reptil ( binatang melata, atau dalam bahasa la...   \n",
       "4    reptil ( binatang melata, atau dalam bahasa la...   \n",
       "..                                                 ...   \n",
       "759  realme meluncurkan smartphone pertamanya \" rea...   \n",
       "760  thariq bin ziyad lebih banyak dikenal sebagai ...   \n",
       "761  thariq bin ziyad lebih banyak dikenal sebagai ...   \n",
       "762  adalah angkatan laut kekaisaran jepang ( kaigu...   \n",
       "763  adalah angkatan laut kekaisaran jepang ( kaigu...   \n",
       "\n",
       "                                              Question  \\\n",
       "0                            apa kepanjangan dari gps?   \n",
       "1    apakah cabang ilmu pengetahuan alam yang mempe...   \n",
       "2    apa cabang ilmu pengetahuan alam yang tidak me...   \n",
       "3             apakah maksud reptil dalam bahasa latin?   \n",
       "4       apakah maksud reptil ganas dalam bahasa latin?   \n",
       "..                                                 ...   \n",
       "759  apakah smartphone pertama yang diproduksi realme?   \n",
       "760       siapa yang dikenal sebagai penakluk spanyol?   \n",
       "761  siapa yang dikenal sebagai penakluk spanyol pa...   \n",
       "762  siapa yang menggunakan kapal induk secara efek...   \n",
       "763  siapa yang menggunakan kapal induk secara efek...   \n",
       "\n",
       "                                     Prediction Answer  \\\n",
       "0                            global positioning system   \n",
       "1                                          herpetologi   \n",
       "2                                          herpetologi   \n",
       "3    kelompok hewan vertebrata berdarah dingin dan ...   \n",
       "4    kelompok hewan vertebrata berdarah dingin dan ...   \n",
       "..                                                 ...   \n",
       "759                                                      \n",
       "760                                   thariq bin ziyad   \n",
       "761                                   thariq bin ziyad   \n",
       "762                    angkatan laut kekaisaran jepang   \n",
       "763                    angkatan laut kekaisaran jepang   \n",
       "\n",
       "                                    Gold Answer  \n",
       "0                     global positioning system  \n",
       "1                                   herpetologi  \n",
       "2                                                \n",
       "3                       ' melata'atau'merayap '  \n",
       "4                                                \n",
       "..                                          ...  \n",
       "759                                    realme 1  \n",
       "760                            thariq bin ziyad  \n",
       "761                                              \n",
       "762  angkatan laut kekaisaran jepang ( kaigun )  \n",
       "763                                              \n",
       "\n",
       "[764 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qas_df = create_qas_dataframe(predict_result)\n",
    "qas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_mark = ['siapa', 'siapakah',\n",
    "                    'apa', 'apakah', 'adakah',\n",
    "                    'dimana', 'dimanakah', 'darimanakah',\n",
    "                    'kapan', 'kapankah',\n",
    "                    'bagaimana', 'bagaimanakah',\n",
    "                    'kenapa', 'mengapa',\n",
    "                    'berapa', 'berapakah', 'seberapa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mencoba cara retrieve model dari HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.15.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (45.2.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.34.2)\n",
      "Collecting torch\n",
      "  Using cached torch-2.0.0-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/dist-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/dist-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/dist-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch) (3.26.0)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.0.1-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torch, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.13.1\n",
      "    Uninstalling torchaudio-0.13.1:\n",
      "      Successfully uninstalled torchaudio-0.13.1\n",
      "Successfully installed torch-2.0.0 torchaudio-2.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 26 10:14:58 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    57W / 300W |  32476MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   60C    P0   289W / 300W |  11709MiB / 32480MiB |     98%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   63C    P0   200W / 300W |  12012MiB / 32480MiB |     98%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    57W / 300W |   9210MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    58W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    57W / 300W |   9364MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    56W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    56W / 300W |    747MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased/snapshots/baf8065c541ffd323cf43d1e93868ccbb20febbd/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--afaji--fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05/snapshots/d3be4101548f3c43d990e50450aad62c3193064e/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pretrained_name_sc = \"afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased\"\n",
    "pretrained_name_qa = \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\"\n",
    "tokenizer_kwargs = {'padding': True, 'truncation': True, 'max_length': MAX_LENGTH}\n",
    "\n",
    "nlp_sc = pipeline(task=\"text-classification\", model=pretrained_name_sc, tokenizer=pretrained_name_sc, \n",
    "                   **tokenizer_kwargs)\n",
    "nlp_qa = pipeline(task=\"question-answering\", model=pretrained_name_qa, tokenizer=pretrained_name_qa, \n",
    "                   **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"Wikidepia/IndoT5-base-paraphrase\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"Wikidepia/IndoT5-base-paraphrase\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Wikidepia/IndoT5-base-paraphrase.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/spiece.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Wikidepia--IndoT5-base-paraphrase/snapshots/5d591dc3aeae0aade0f327a5ebdd0f071c83f567/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "pretrained_name_tg = \"Wikidepia/IndoT5-base-paraphrase\"\n",
    "\n",
    "tokenizer_kwargs = {'truncation': True, 'max_length': MAX_LENGTH}\n",
    "\n",
    "nlp_tg = pipeline(task=\"text2text-generation\", model=pretrained_name_tg, tokenizer=pretrained_name_tg, \n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coba buat method evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(question, pred_answer, gold_answer, type, question_mark=question_mark):\n",
    "    \n",
    "    if type == 'replace first':\n",
    "        pred_hypothesis = question.replace('?', '')\n",
    "        pred_hypothesis = pred_hypothesis.replace(question.split()[0], pred_answer)\n",
    "\n",
    "        gold_hypothesis = question.replace('?', '')\n",
    "        gold_hypothesis = gold_hypothesis.replace(question.split()[0], gold_answer)\n",
    "    \n",
    "    elif type == 'replace question mark':\n",
    "        for i in question.split():\n",
    "            if i in question_mark:\n",
    "                pred_hypothesis = question.replace('?', '')\n",
    "                pred_hypothesis = pred_hypothesis.replace(i, pred_answer)\n",
    "\n",
    "                gold_hypothesis = question.replace('?', '')\n",
    "                gold_hypothesis = gold_hypothesis.replace(i, gold_answer)\n",
    "    \n",
    "    elif type == 'add adalah':\n",
    "        pred_hypothesis = question.replace('?', '')\n",
    "        pred_hypothesis = pred_hypothesis.replace(question.split()[0], '')\n",
    "        pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "        gold_hypothesis = question.replace('?', '')\n",
    "        gold_hypothesis = gold_hypothesis.replace(question.split()[0], '')\n",
    "        gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "    \n",
    "    elif type == 'just concat answer and question':\n",
    "        pred_hypothesis = f\"{question} {pred_answer}\"         \n",
    "        gold_hypothesis = f\"{question} {gold_answer}\"\n",
    "        \n",
    "    elif type == 'rule based':\n",
    "        question = question.replace('kah', '')\n",
    "        for j in question.split():\n",
    "            if j in question_mark:\n",
    "                if j == 'siapa' or j == 'siapakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "\n",
    "                elif j == 'apa' or j == 'apakah' or j == 'adakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "\n",
    "                elif j == 'dimana' or j == 'dimanakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} di {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} di {gold_answer}\"\n",
    "\n",
    "                elif j == 'darimanakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} dari {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} dari {gold_answer}\"\n",
    "\n",
    "                elif j == 'kapan' or j == 'kapankah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} pada {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} pada {gold_answer}\"\n",
    "\n",
    "                elif j == 'bagaimana' or j == 'bagaimanakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '')\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "\n",
    "                elif j == 'kenapa' or j == 'mengapa':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, 'alasan').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah karena {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, 'alasan').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah karena {gold_answer}\"\n",
    "\n",
    "                elif j == 'berapa' or j == 'berapakah' or j == 'seberapa': \n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "                    \n",
    "    elif type == 'machine generation': \n",
    "        pred_hypothesis, gold_hypothesis = smoothing(question, pred_answer, gold_answer, type=\"rule based\")\n",
    "        pred_hypothesis = nlp_tg(pred_hypothesis)[0]['generated_text']\n",
    "        gold_hypothesis = nlp_tg(gold_hypothesis)[0]['generated_text']\n",
    "        \n",
    "    return pred_hypothesis, gold_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alasan mengapa Messi menang adalah karena dia jago kali\n",
      "alasan messi menang adalah karena tim yang unggul .\n"
     ]
    }
   ],
   "source": [
    "pred_hypothesis, gold_hypothesis = smoothing(\"kenapa messi menang?\", \"emang jago kali\", \"tim yang unggul\",\n",
    "                                            type=\"machine generation\")\n",
    "print(pred_hypothesis)\n",
    "print(gold_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_for_evaluation(predict_result, type_smoothing, type_qas, MAXIMUM_SEARCH_ITER=MAXIMUM_SEARCH_ITER):\n",
    "    \n",
    "    # Ekstrak dari PredictionOutput QAS\n",
    "    predictions_idx = np.argsort(predict_result.predictions, axis=2)[:, :, 1 * -1]\n",
    "    label_array = np.asarray(predict_result.label_ids)\n",
    "    \n",
    "    question_array = []\n",
    "    context_array = []\n",
    "    \n",
    "    pred_answer_before_filtering_array = []\n",
    "    pred_answer_after_filtering_array = []\n",
    "    \n",
    "    label_before_filtering_array = []\n",
    "    label_after_filtering_array = []\n",
    "    \n",
    "    pred_hypothesis_before_filtering_array = []\n",
    "    pred_hypothesis_after_filtering_array = []\n",
    "    \n",
    "    gold_answer_array = []\n",
    "    gold_hypothesis_array = []\n",
    "    \n",
    "    # Iterasi ini ditujukan untuk retrieve answer\n",
    "    for i in tqdm(range(len(predict_result.predictions[0]))):\n",
    "        \n",
    "        isFoundBiggest = False\n",
    "        \n",
    "        start_pred_idx = predictions_idx[0][i]\n",
    "        end_pred_idx = predictions_idx[1][i] + 1\n",
    "        \n",
    "        start_gold_idx = label_array[0][i]\n",
    "        end_gold_idx = label_array[1][i] + 1\n",
    "        \n",
    "        # Retrieve answer prediksi\n",
    "        pred_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_pred_idx: end_pred_idx], skip_special_tokens=True)\n",
    "        \n",
    "        # Retrieve answer gold\n",
    "        gold_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_gold_idx: end_gold_idx], skip_special_tokens=True)\n",
    "        \n",
    "        question = []\n",
    "        context = []\n",
    "        \n",
    "        # Iterasi ini untuk retrieve question dan context index yang bersangkutan\n",
    "        for j in range(len(tokenized_data_qas_id_validation[i]['token_type_ids'])):\n",
    "            \n",
    "            # Bila token_type_ids-nya 0, maka itu question (sesuai dengan urutan tokenisasi)\n",
    "            if tokenized_data_qas_id_validation[i]['token_type_ids'][j] == 0:\n",
    "                question.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "            \n",
    "            # Bila token_type_ids-nya 1, maka itu context (sesuai dengan urutan tokenisasi)\n",
    "            else:\n",
    "                context.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "        \n",
    "        question_decoded = tokenizer.decode(question, skip_special_tokens=True)\n",
    "        context_decoded = tokenizer.decode(context, skip_special_tokens=True)\n",
    "        pred_hypothesis, gold_hypothesis = smoothing(question_decoded, pred_answer, gold_answer, type_smoothing)\n",
    "\n",
    "        # Cek label dari answer prediksi dan context\n",
    "        predicted_label = nlp_sc({'text': context_decoded, \n",
    "                                  'text_pair': pred_hypothesis}, \n",
    "                                 **tokenizer_kwargs)\n",
    "        \n",
    "        pred_answer_before_filtering_array.append([pred_answer])\n",
    "        pred_hypothesis_before_filtering_array.append([pred_hypothesis])\n",
    "        label_before_filtering_array.append([predicted_label])\n",
    "        \n",
    "        # Cek label dari answer prediksi dan context, bila labelnya entailment (atau neutral), maka answernya jadi hasil akhir\n",
    "        if predicted_label['label'] == 'neutral':\n",
    "            if type_qas == 'entailment or neutral':\n",
    "                question_array.append(question_decoded)\n",
    "                context_array.append(context_decoded)\n",
    "                pred_answer_after_filtering_array.append([pred_answer])\n",
    "                gold_answer_array.append(gold_answer)\n",
    "                pred_hypothesis_after_filtering_array.append([pred_hypothesis])\n",
    "                gold_hypothesis_array.append(gold_hypothesis)\n",
    "                label_after_filtering_array.append([predicted_label])\n",
    "\n",
    "        if predicted_label['label'] == 'entailment':\n",
    "            if type_qas == 'entailment only' or type_qas == 'entailment or neutral':\n",
    "                question_array.append(question_decoded)\n",
    "                context_array.append(context_decoded)\n",
    "                pred_answer_after_filtering_array.append([pred_answer])\n",
    "                gold_answer_array.append(gold_answer)\n",
    "                pred_hypothesis_after_filtering_array.append([pred_hypothesis])\n",
    "                gold_hypothesis_array.append(gold_hypothesis)\n",
    "                label_after_filtering_array.append([predicted_label])\n",
    "            \n",
    "        # Cek label dari answer prediksi dan context, bila labelnya bukan entailment (atau neutral), \n",
    "        # -- maka masuk ke for-loop untuk iterasi ke argmax selanjutnya, dengan menggunakan argsort\n",
    "        else:\n",
    "            \n",
    "            if predicted_label == 'neutral' and type_qas == 'entailment or neutral': continue\n",
    "            \n",
    "            # Bila MAXIMUM_SEARCH_ITER dibawah 2, maka continue langsung\n",
    "            if MAXIMUM_SEARCH_ITER < 2: continue\n",
    "\n",
    "            # Bila MAXIMUM_SEARCH_ITER diatas 2, maka continue langsung\n",
    "            \n",
    "            else:\n",
    "                # Bila bukan entailment, loop sebanyak MAXIMUM_SEARCH_ITER kali.\n",
    "                pred_answer_after_filtering_array_msi_recorded = []\n",
    "                pred_hypothesis_after_filtering_array_msi_recorded = []\n",
    "                label_after_filtering_array_msi_recorded = []\n",
    "                for index_largest in range(MAXIMUM_SEARCH_ITER - 1):\n",
    "                    \n",
    "                    #pred_answer_after_filtering_array_msi_recorded = []\n",
    "                    #pred_hypothesis_after_filtering_array_msi_recorded = []\n",
    "                    #label_after_filtering_array_msi_recorded = []\n",
    "\n",
    "                    # Cari di index kedua, ketiga, keempat, dan seterusnya\n",
    "                    predictions_idx_inside_loop = np.argsort(predict_result.predictions, \n",
    "                                                             axis=2)[:, :, (index_largest + 2) * -1]\n",
    "\n",
    "                    start_pred_idx = predictions_idx_inside_loop[0][i]\n",
    "                    end_pred_idx = predictions_idx_inside_loop[1][i] + 1\n",
    "\n",
    "                    # Retrieve answer prediksi\n",
    "                    pred_answer_inside_loop = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                                   [start_pred_idx: end_pred_idx], skip_special_tokens=True)\n",
    "                    \n",
    "                    pred_hypothesis_inside_loop, gold_hypothesis = smoothing(\n",
    "                        question_decoded, pred_answer_inside_loop, gold_answer, type_smoothing)\n",
    "                    \n",
    "                    # Cek label dari answer prediksi dan context\n",
    "                    predicted_label_inside_loop = nlp_sc({'text': context_decoded, \n",
    "                                                          'text_pair': pred_hypothesis_inside_loop}\n",
    "                                                           , **tokenizer_kwargs)\n",
    "                    \n",
    "                    pred_answer_after_filtering_array_msi_recorded.append(pred_answer_inside_loop)\n",
    "                    pred_hypothesis_after_filtering_array_msi_recorded.append(pred_hypothesis_inside_loop)\n",
    "                    label_after_filtering_array_msi_recorded.append(predicted_label_inside_loop)\n",
    "                    \n",
    "                    # Bila label-nya sudah entailment (atau neutral), maka answernya jadi hasil akhir, dan break\n",
    "                    if type_qas == 'entailment only':\n",
    "                        if predicted_label_inside_loop['label'] == 'entailment':\n",
    "                            isFoundBiggest = True\n",
    "                            question_array.append(question_decoded)\n",
    "                            context_array.append(context_decoded)\n",
    "                            gold_answer_array.append(gold_answer)   \n",
    "                            gold_hypothesis_array.append(gold_hypothesis)\n",
    "                            \n",
    "                            pred_answer_after_filtering_array.append(pred_answer_after_filtering_array_msi_recorded)\n",
    "                            pred_hypothesis_after_filtering_array.append(pred_hypothesis_after_filtering_array_msi_recorded)\n",
    "                            label_after_filtering_array.append(label_after_filtering_array_msi_recorded)\n",
    "                            break\n",
    "                            \n",
    "                    elif type_qas == 'entailment or neutral':\n",
    "                        if predicted_label_inside_loop['label'] == 'entailment' or predicted_label_inside_loop['label'] == 'neutral':\n",
    "                            isFoundBiggest = True\n",
    "                            question_array.append(question_decoded)\n",
    "                            context_array.append(context_decoded)\n",
    "                            gold_answer_array.append(gold_answer)   \n",
    "                            gold_hypothesis_array.append(gold_hypothesis)\n",
    "                            \n",
    "                            pred_answer_after_filtering_array.append(pred_answer_after_filtering_array_msi_recorded)\n",
    "                            pred_hypothesis_after_filtering_array.append(pred_hypothesis_after_filtering_array_msi_recorded)\n",
    "                            label_after_filtering_array.append(label_after_filtering_array_msi_recorded)\n",
    "                            break\n",
    "\n",
    "                if isFoundBiggest == False:\n",
    "                    # Bila sampai iterasi terakhir, belum entailment (atau neutral) juga, maka append saja jawaban kosong\n",
    "                    \n",
    "                    pred_answer_not_found_biggest = \"\" # Disini, jawaban kosong\n",
    "                    \n",
    "                    question_array.append(question_decoded)\n",
    "                    context_array.append(context_decoded)\n",
    "                    \n",
    "                    pred_hypothesis_not_found_biggest, gold_hypothesis = smoothing(\n",
    "                        question_decoded, pred_answer_not_found_biggest, gold_answer, type_smoothing)\n",
    "                    \n",
    "                    pred_answer_after_filtering_array_msi_recorded.append(pred_answer_not_found_biggest)\n",
    "                    pred_hypothesis_after_filtering_array_msi_recorded.append(pred_hypothesis_not_found_biggest)\n",
    "                    label_after_filtering_array_msi_recorded.append(predicted_label_inside_loop)\n",
    "                    \n",
    "                    gold_answer_array.append(gold_answer)\n",
    "                    gold_hypothesis_array.append(gold_hypothesis)\n",
    "                    \n",
    "                    pred_answer_after_filtering_array.append(pred_answer_after_filtering_array_msi_recorded)\n",
    "                    pred_hypothesis_after_filtering_array.append(pred_hypothesis_after_filtering_array_msi_recorded)\n",
    "                    label_after_filtering_array.append(label_after_filtering_array_msi_recorded)\n",
    "    \n",
    "    # Buat DataFrame QAS\n",
    "    qas_df = pd.DataFrame({'Context': context_array, \n",
    "                           'Question': question_array, \n",
    "                           \n",
    "                           'Prediction Answer Before Filtering': pred_answer_before_filtering_array,\n",
    "                           'Prediction Hypothesis Before Filtering': pred_hypothesis_before_filtering_array,\n",
    "                           'Label Before Filtering': label_before_filtering_array,\n",
    "                                 \n",
    "                           'Prediction Answer After Filtering': pred_answer_after_filtering_array,\n",
    "                           'Prediction Hypothesis After Filtering': pred_hypothesis_after_filtering_array,\n",
    "                           'Label After Filtering': label_after_filtering_array,\n",
    "                          \n",
    "                           'Gold Answer': gold_answer_array,\n",
    "                          'Gold Hypothesis': gold_hypothesis_array})\n",
    "                          \n",
    "    assert len(predict_result.predictions[0]) == len(qas_df), \"Jumlah prediksi berbeda dengan jumlah evaluasi\"\n",
    "    \n",
    "    # Return DataFrame QAS\n",
    "    return qas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 764/764 [06:03<00:00,  2.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Question</th>\n",
       "      <th>Prediction Answer Before Filtering</th>\n",
       "      <th>Prediction Hypothesis Before Filtering</th>\n",
       "      <th>Label Before Filtering</th>\n",
       "      <th>Prediction Answer After Filtering</th>\n",
       "      <th>Prediction Hypothesis After Filtering</th>\n",
       "      <th>Label After Filtering</th>\n",
       "      <th>Gold Answer</th>\n",
       "      <th>Gold Hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sistem pemosisi global [ 1 ] ( bahasa inggris ...</td>\n",
       "      <td>apa kepanjangan dari gps?</td>\n",
       "      <td>[global positioning system]</td>\n",
       "      <td>[global positioning system kepanjangan dari gps]</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9869425296...</td>\n",
       "      <td>[global positioning system]</td>\n",
       "      <td>[global positioning system kepanjangan dari gps]</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9869425296...</td>\n",
       "      <td>global positioning system</td>\n",
       "      <td>global positioning system kepanjangan dari gps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ukuran reptil bervariasi, dari yang berukuran ...</td>\n",
       "      <td>apakah cabang ilmu pengetahuan alam yang mempe...</td>\n",
       "      <td>[herpetologi]</td>\n",
       "      <td>[herpetologi cabang ilmu pengetahuan alam yang...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9994400143...</td>\n",
       "      <td>[herpetologi]</td>\n",
       "      <td>[herpetologi cabang ilmu pengetahuan alam yang...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9994400143...</td>\n",
       "      <td>herpetologi</td>\n",
       "      <td>herpetologi cabang ilmu pengetahuan alam yang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ukuran reptil bervariasi, dari yang berukuran ...</td>\n",
       "      <td>apa cabang ilmu pengetahuan alam yang tidak me...</td>\n",
       "      <td>[herpetologi]</td>\n",
       "      <td>[herpetologi cabang ilmu pengetahuan alam yang...</td>\n",
       "      <td>[{'label': 'contradiction', 'score': 0.9999427...</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[ cabang ilmu pengetahuan alam yang tidak memp...</td>\n",
       "      <td>[{'label': 'contradiction', 'score': 0.9994680...</td>\n",
       "      <td></td>\n",
       "      <td>cabang ilmu pengetahuan alam yang tidak mempe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reptil ( binatang melata, atau dalam bahasa la...</td>\n",
       "      <td>apakah maksud reptil dalam bahasa latin?</td>\n",
       "      <td>[kelompok hewan vertebrata berdarah dingin dan...</td>\n",
       "      <td>[kelompok hewan vertebrata berdarah dingin dan...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9996963739...</td>\n",
       "      <td>[kelompok hewan vertebrata berdarah dingin dan...</td>\n",
       "      <td>[kelompok hewan vertebrata berdarah dingin dan...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9996963739...</td>\n",
       "      <td>' melata'atau'merayap '</td>\n",
       "      <td>' melata'atau'merayap ' maksud reptil dalam ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reptil ( binatang melata, atau dalam bahasa la...</td>\n",
       "      <td>apakah maksud reptil ganas dalam bahasa latin?</td>\n",
       "      <td>[kelompok hewan vertebrata berdarah dingin dan...</td>\n",
       "      <td>[kelompok hewan vertebrata berdarah dingin dan...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9815422892...</td>\n",
       "      <td>[kelompok hewan vertebrata berdarah dingin dan...</td>\n",
       "      <td>[kelompok hewan vertebrata berdarah dingin dan...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9815422892...</td>\n",
       "      <td></td>\n",
       "      <td>maksud reptil ganas dalam bahasa latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>realme meluncurkan smartphone pertamanya \" rea...</td>\n",
       "      <td>apakah smartphone pertama yang diproduksi realme?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ smartphone pertama yang diproduksi realme]</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9927445650...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ smartphone pertama yang diproduksi realme]</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9927445650...</td>\n",
       "      <td>realme 1</td>\n",
       "      <td>realme 1 smartphone pertama yang diproduksi re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>thariq bin ziyad lebih banyak dikenal sebagai ...</td>\n",
       "      <td>siapa yang dikenal sebagai penakluk spanyol?</td>\n",
       "      <td>[thariq bin ziyad]</td>\n",
       "      <td>[thariq bin ziyad yang dikenal sebagai penaklu...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9913252592...</td>\n",
       "      <td>[thariq bin ziyad]</td>\n",
       "      <td>[thariq bin ziyad yang dikenal sebagai penaklu...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9913252592...</td>\n",
       "      <td>thariq bin ziyad</td>\n",
       "      <td>thariq bin ziyad yang dikenal sebagai penakluk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>thariq bin ziyad lebih banyak dikenal sebagai ...</td>\n",
       "      <td>siapa yang dikenal sebagai penakluk spanyol pa...</td>\n",
       "      <td>[thariq bin ziyad]</td>\n",
       "      <td>[thariq bin ziyad yang dikenal sebagai penaklu...</td>\n",
       "      <td>[{'label': 'neutral', 'score': 0.9973908066749...</td>\n",
       "      <td>[##yad lebih banyak dikenal sebagai penakluk s...</td>\n",
       "      <td>[##yad lebih banyak dikenal sebagai penakluk s...</td>\n",
       "      <td>[{'label': 'contradiction', 'score': 0.8795180...</td>\n",
       "      <td></td>\n",
       "      <td>yang dikenal sebagai penakluk spanyol paling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>adalah angkatan laut kekaisaran jepang ( kaigu...</td>\n",
       "      <td>siapa yang menggunakan kapal induk secara efek...</td>\n",
       "      <td>[angkatan laut kekaisaran jepang]</td>\n",
       "      <td>[angkatan laut kekaisaran jepang yang mengguna...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9997662901...</td>\n",
       "      <td>[angkatan laut kekaisaran jepang]</td>\n",
       "      <td>[angkatan laut kekaisaran jepang yang mengguna...</td>\n",
       "      <td>[{'label': 'entailment', 'score': 0.9997662901...</td>\n",
       "      <td>angkatan laut kekaisaran jepang ( kaigun )</td>\n",
       "      <td>angkatan laut kekaisaran jepang ( kaigun ) yan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>adalah angkatan laut kekaisaran jepang ( kaigu...</td>\n",
       "      <td>siapa yang menggunakan kapal induk secara efek...</td>\n",
       "      <td>[angkatan laut kekaisaran jepang]</td>\n",
       "      <td>[angkatan laut kekaisaran jepang yang mengguna...</td>\n",
       "      <td>[{'label': 'contradiction', 'score': 0.9841200...</td>\n",
       "      <td>[jepang ( kaigun, ]</td>\n",
       "      <td>[jepang ( kaigun yang menggunakan kapal induk ...</td>\n",
       "      <td>[{'label': 'contradiction', 'score': 0.9756415...</td>\n",
       "      <td></td>\n",
       "      <td>yang menggunakan kapal induk secara efektif p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>764 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Context  \\\n",
       "0    sistem pemosisi global [ 1 ] ( bahasa inggris ...   \n",
       "1    ukuran reptil bervariasi, dari yang berukuran ...   \n",
       "2    ukuran reptil bervariasi, dari yang berukuran ...   \n",
       "3    reptil ( binatang melata, atau dalam bahasa la...   \n",
       "4    reptil ( binatang melata, atau dalam bahasa la...   \n",
       "..                                                 ...   \n",
       "759  realme meluncurkan smartphone pertamanya \" rea...   \n",
       "760  thariq bin ziyad lebih banyak dikenal sebagai ...   \n",
       "761  thariq bin ziyad lebih banyak dikenal sebagai ...   \n",
       "762  adalah angkatan laut kekaisaran jepang ( kaigu...   \n",
       "763  adalah angkatan laut kekaisaran jepang ( kaigu...   \n",
       "\n",
       "                                              Question  \\\n",
       "0                            apa kepanjangan dari gps?   \n",
       "1    apakah cabang ilmu pengetahuan alam yang mempe...   \n",
       "2    apa cabang ilmu pengetahuan alam yang tidak me...   \n",
       "3             apakah maksud reptil dalam bahasa latin?   \n",
       "4       apakah maksud reptil ganas dalam bahasa latin?   \n",
       "..                                                 ...   \n",
       "759  apakah smartphone pertama yang diproduksi realme?   \n",
       "760       siapa yang dikenal sebagai penakluk spanyol?   \n",
       "761  siapa yang dikenal sebagai penakluk spanyol pa...   \n",
       "762  siapa yang menggunakan kapal induk secara efek...   \n",
       "763  siapa yang menggunakan kapal induk secara efek...   \n",
       "\n",
       "                    Prediction Answer Before Filtering  \\\n",
       "0                          [global positioning system]   \n",
       "1                                        [herpetologi]   \n",
       "2                                        [herpetologi]   \n",
       "3    [kelompok hewan vertebrata berdarah dingin dan...   \n",
       "4    [kelompok hewan vertebrata berdarah dingin dan...   \n",
       "..                                                 ...   \n",
       "759                                                 []   \n",
       "760                                 [thariq bin ziyad]   \n",
       "761                                 [thariq bin ziyad]   \n",
       "762                  [angkatan laut kekaisaran jepang]   \n",
       "763                  [angkatan laut kekaisaran jepang]   \n",
       "\n",
       "                Prediction Hypothesis Before Filtering  \\\n",
       "0     [global positioning system kepanjangan dari gps]   \n",
       "1    [herpetologi cabang ilmu pengetahuan alam yang...   \n",
       "2    [herpetologi cabang ilmu pengetahuan alam yang...   \n",
       "3    [kelompok hewan vertebrata berdarah dingin dan...   \n",
       "4    [kelompok hewan vertebrata berdarah dingin dan...   \n",
       "..                                                 ...   \n",
       "759       [ smartphone pertama yang diproduksi realme]   \n",
       "760  [thariq bin ziyad yang dikenal sebagai penaklu...   \n",
       "761  [thariq bin ziyad yang dikenal sebagai penaklu...   \n",
       "762  [angkatan laut kekaisaran jepang yang mengguna...   \n",
       "763  [angkatan laut kekaisaran jepang yang mengguna...   \n",
       "\n",
       "                                Label Before Filtering  \\\n",
       "0    [{'label': 'entailment', 'score': 0.9869425296...   \n",
       "1    [{'label': 'entailment', 'score': 0.9994400143...   \n",
       "2    [{'label': 'contradiction', 'score': 0.9999427...   \n",
       "3    [{'label': 'entailment', 'score': 0.9996963739...   \n",
       "4    [{'label': 'entailment', 'score': 0.9815422892...   \n",
       "..                                                 ...   \n",
       "759  [{'label': 'entailment', 'score': 0.9927445650...   \n",
       "760  [{'label': 'entailment', 'score': 0.9913252592...   \n",
       "761  [{'label': 'neutral', 'score': 0.9973908066749...   \n",
       "762  [{'label': 'entailment', 'score': 0.9997662901...   \n",
       "763  [{'label': 'contradiction', 'score': 0.9841200...   \n",
       "\n",
       "                     Prediction Answer After Filtering  \\\n",
       "0                          [global positioning system]   \n",
       "1                                        [herpetologi]   \n",
       "2                                                 [, ]   \n",
       "3    [kelompok hewan vertebrata berdarah dingin dan...   \n",
       "4    [kelompok hewan vertebrata berdarah dingin dan...   \n",
       "..                                                 ...   \n",
       "759                                                 []   \n",
       "760                                 [thariq bin ziyad]   \n",
       "761  [##yad lebih banyak dikenal sebagai penakluk s...   \n",
       "762                  [angkatan laut kekaisaran jepang]   \n",
       "763                                [jepang ( kaigun, ]   \n",
       "\n",
       "                 Prediction Hypothesis After Filtering  \\\n",
       "0     [global positioning system kepanjangan dari gps]   \n",
       "1    [herpetologi cabang ilmu pengetahuan alam yang...   \n",
       "2    [ cabang ilmu pengetahuan alam yang tidak memp...   \n",
       "3    [kelompok hewan vertebrata berdarah dingin dan...   \n",
       "4    [kelompok hewan vertebrata berdarah dingin dan...   \n",
       "..                                                 ...   \n",
       "759       [ smartphone pertama yang diproduksi realme]   \n",
       "760  [thariq bin ziyad yang dikenal sebagai penaklu...   \n",
       "761  [##yad lebih banyak dikenal sebagai penakluk s...   \n",
       "762  [angkatan laut kekaisaran jepang yang mengguna...   \n",
       "763  [jepang ( kaigun yang menggunakan kapal induk ...   \n",
       "\n",
       "                                 Label After Filtering  \\\n",
       "0    [{'label': 'entailment', 'score': 0.9869425296...   \n",
       "1    [{'label': 'entailment', 'score': 0.9994400143...   \n",
       "2    [{'label': 'contradiction', 'score': 0.9994680...   \n",
       "3    [{'label': 'entailment', 'score': 0.9996963739...   \n",
       "4    [{'label': 'entailment', 'score': 0.9815422892...   \n",
       "..                                                 ...   \n",
       "759  [{'label': 'entailment', 'score': 0.9927445650...   \n",
       "760  [{'label': 'entailment', 'score': 0.9913252592...   \n",
       "761  [{'label': 'contradiction', 'score': 0.8795180...   \n",
       "762  [{'label': 'entailment', 'score': 0.9997662901...   \n",
       "763  [{'label': 'contradiction', 'score': 0.9756415...   \n",
       "\n",
       "                                    Gold Answer  \\\n",
       "0                     global positioning system   \n",
       "1                                   herpetologi   \n",
       "2                                                 \n",
       "3                       ' melata'atau'merayap '   \n",
       "4                                                 \n",
       "..                                          ...   \n",
       "759                                    realme 1   \n",
       "760                            thariq bin ziyad   \n",
       "761                                               \n",
       "762  angkatan laut kekaisaran jepang ( kaigun )   \n",
       "763                                               \n",
       "\n",
       "                                       Gold Hypothesis  \n",
       "0       global positioning system kepanjangan dari gps  \n",
       "1    herpetologi cabang ilmu pengetahuan alam yang ...  \n",
       "2     cabang ilmu pengetahuan alam yang tidak mempe...  \n",
       "3    ' melata'atau'merayap ' maksud reptil dalam ba...  \n",
       "4               maksud reptil ganas dalam bahasa latin  \n",
       "..                                                 ...  \n",
       "759  realme 1 smartphone pertama yang diproduksi re...  \n",
       "760  thariq bin ziyad yang dikenal sebagai penakluk...  \n",
       "761   yang dikenal sebagai penakluk spanyol paling ...  \n",
       "762  angkatan laut kekaisaran jepang ( kaigun ) yan...  \n",
       "763   yang menggunakan kapal induk secara efektif p...  \n",
       "\n",
       "[764 rows x 10 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = create_df_for_evaluation(predict_result, type_smoothing='replace first', type_qas='entailment only', MAXIMUM_SEARCH_ITER=2)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apa cabang ilmu pengetahuan alam yang tidak mempelajari tentang reptil?'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['Question'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ukuran reptil bervariasi, dari yang berukuran hingga 1, 6cm ( tokek kecil, sphaerodactylus ariasae ) hingga berukuran 6 m dan mencapai berat 1 ton ( buaya air asin, crocodylus porosus ). cabang ilmu pengetahuan alam yang mempelajari tentang reptil adalah herpetologi.'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['Context'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['Prediction Answer After Filtering'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_from_df(df, type_qas):\n",
    "    \n",
    "    denominator = len(df)\n",
    "    total_correct = 0\n",
    "    f1_array = []\n",
    "    \n",
    "    true_positive_before_filtering = 0\n",
    "    false_positive_before_filtering = 0\n",
    "    false_negative_before_filtering = 0\n",
    "    true_negative_before_filtering = 0\n",
    "    \n",
    "    true_positive_after_filtering = 0\n",
    "    false_positive_after_filtering = 0\n",
    "    false_negative_after_filtering = 0\n",
    "    true_negative_after_filtering = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        pred_answer_before_filtering = df[\"Prediction Answer Before Filtering\"][i][-1]\n",
    "        pred_answer_after_filtering = df[\"Prediction Answer After Filtering\"][i][-1]\n",
    "        \n",
    "        pred_label_before_filtering = df[\"Label Before Filtering\"][i][-1]['label']\n",
    "        pred_label_after_filtering = df[\"Label After Filtering\"][i][-1]['label']\n",
    "        \n",
    "        gold_text = df[\"Gold Answer\"][i]\n",
    "\n",
    "        if pred_answer_after_filtering == gold_text:\n",
    "            total_correct += 1\n",
    "\n",
    "        f1 = compute_f1_prec_rec(pred=pred_answer_after_filtering, gold=gold_text)\n",
    "\n",
    "        f1_array.append(f1)\n",
    "        \n",
    "        # Terprediksi dengan label yang benar, dan hasil answernya benar -> True positive\n",
    "        # Terprediksi dengan label yang benar, padahal hasil answernya salah -> False positive\n",
    "        # Terprediksi dengan label yang salah, padahal hasil answernya benar -> False negative\n",
    "        # Terprediksi dengan label yang salah, dan hasil answernya salah -> True negative\n",
    "        \n",
    "        if type_qas == 'entailment only':\n",
    "        \n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering == 'entailment'):\n",
    "                true_positive_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering == 'entailment'):\n",
    "                false_positive_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering != 'entailment'):\n",
    "                false_negative_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering != 'entailment'):\n",
    "                true_negative_after_filtering += 1\n",
    "\n",
    "            if (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'entailment'):\n",
    "                true_positive_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'entailment'):\n",
    "                false_positive_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering != 'entailment'):\n",
    "                false_negative_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering != 'entailment'):\n",
    "                true_negative_before_filtering += 1\n",
    "        \n",
    "        elif type_qas == 'entailment or neutral':\n",
    "        \n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering == 'entailment' \n",
    "                                                               or pred_label_after_filtering == 'neutral'):\n",
    "                true_positive_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering == 'entailment' \n",
    "                                                                 or pred_label_after_filtering == 'neutral'):\n",
    "                false_positive_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering != 'entailment' \n",
    "                                                                 and pred_label_after_filtering != 'neutral'):\n",
    "                false_negative_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering != 'entailment' \n",
    "                                                                 and pred_label_after_filtering != 'neutral'):\n",
    "                true_negative_after_filtering += 1\n",
    "\n",
    "            if (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'entailment' \n",
    "                                                                or pred_label_after_filtering == 'neutral'):\n",
    "                true_positive_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'entailment' \n",
    "                                                                  or pred_label_after_filtering == 'neutral'):\n",
    "                false_positive_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering != 'entailment' \n",
    "                                                                  and pred_label_after_filtering != 'neutral'):\n",
    "                false_negative_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering != 'entailment' \n",
    "                                                                  and pred_label_after_filtering != 'neutral'):\n",
    "                true_negative_before_filtering += 1\n",
    "\n",
    "    exact_match = ((total_correct / denominator) * 100.0)\n",
    "    final_f1 = np.mean(f1_array) * 100.0\n",
    "    after_filtering_metric_array = [true_positive_after_filtering, false_positive_after_filtering, \n",
    "                          false_negative_after_filtering, true_negative_after_filtering]\n",
    "    before_filtering_metric_array = [true_positive_before_filtering, false_positive_before_filtering, \n",
    "                          false_negative_before_filtering, true_negative_before_filtering]\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': final_f1}, after_filtering_metric_array, before_filtering_metric_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 62.17277486910995, 'f1': 66.467874209069}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_result_after_filtering, after_filtering_metric_array, before_filtering_metric_array = compute_metrics_from_df(\n",
    "    eval_df, \"entailment only\")\n",
    "metric_result_after_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_non_zero(number):\n",
    "    if number == 0:\n",
    "        number += sys.float_info.min\n",
    "    return number\n",
    "\n",
    "def compute_f1_prec_rec_whole(metric_array):\n",
    "    accuracy = (metric_array[0] + metric_array[3]) / \\\n",
    "        (metric_array[0] + metric_array[1] + \n",
    "         metric_array[2] + metric_array[3])\n",
    "    \n",
    "    precision = (metric_array[0]) / (metric_array[0] + metric_array[1])\n",
    "    \n",
    "    recall = (metric_array[0]) / (metric_array[0] + metric_array[2])\n",
    "    \n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def diff_verbose_metric(metric_result_before, metric_result_after, metric):\n",
    "    \n",
    "    percentage = round(((metric_result_after - metric_result_before) / metric_result_before) * 100, 2)\n",
    "    \n",
    "    if '&' in metric: vocab = \"nilai\"\n",
    "    else: vocab = \"metrik\"\n",
    "    \n",
    "    if metric_result_before ==  metric_result_after:\n",
    "        print(f\"Hasil {vocab} {metric} sebelum filtering NLI SAMA DENGAN metrik setelah filtering NLI\")\n",
    "    elif metric_result_before <  metric_result_after:\n",
    "        print(f\"Hasil {vocab} {metric} setelah filtering NLI mengalami KENAIKAN sebesar: {percentage} %\")\n",
    "    elif metric_result_before >  metric_result_after:\n",
    "        print(f\"Hasil {vocab} {metric} setelah filtering NLI mengalami PENURUNAN sebesar: {-1 * percentage} %\")\n",
    "    \n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_metrics(metrics_before, metrics_after, \n",
    "                    after_filtering_metric_array=after_filtering_metric_array, \n",
    "                    before_filtering_metric_array=before_filtering_metric_array):\n",
    "    \n",
    "    em_before = metrics_before['exact_match']\n",
    "    f1_before = metrics_before['f1']\n",
    "\n",
    "    print(\"~ METRIK PER TOKEN ~\")\n",
    "    print(f\"Skor Exact Match sebelum filtering NLI: {em_before}\")\n",
    "    print(f\"Skor F1 sebelum filtering NLI: {f1_before}\")\n",
    "    print()\n",
    "\n",
    "    em_after = metrics_after['exact_match']\n",
    "    f1_after = metrics_after['f1']\n",
    "\n",
    "    print(f\"Skor Exact Match setelah filtering NLI: {em_after}\")\n",
    "    print(f\"Skor F1 setelah filtering NLI: {f1_after}\")\n",
    "    print()\n",
    "\n",
    "    em_before = convert_to_non_zero(em_before)\n",
    "    f1_before = convert_to_non_zero(f1_before)\n",
    "\n",
    "    em_after = convert_to_non_zero(em_after)\n",
    "    f1_after = convert_to_non_zero(f1_after)\n",
    "\n",
    "    print(\"~ METRIK DENGAN PARAMETER NLI ~\")\n",
    "    print(f\"[BEFORE FILTERING] Jawaban benar & label NLI yang sesuai: {before_filtering_metric_array[0]}\")\n",
    "    print(f\"[BEFORE FILTERING] Jawaban TIDAK benar & label NLI yang sesuai: {before_filtering_metric_array[1]}\")\n",
    "    print(f\"[BEFORE FILTERING] Jawaban benar & label NLI yang TIDAK sesuai: {before_filtering_metric_array[2]}\")\n",
    "    print(f\"[BEFORE FILTERING] Jawaban TIDAK benar & label NLI yang TIDAK sesuai: {before_filtering_metric_array[3]}\")\n",
    "    print()\n",
    "\n",
    "    print(f\"[AFTER FILTERING] Jawaban benar & label NLI yang sesuai: {after_filtering_metric_array[0]}\")\n",
    "    print(f\"[AFTER FILTERING] Jawaban TIDAK benar & label NLI yang sesuai: {after_filtering_metric_array[1]}\")\n",
    "    print(f\"[AFTER FILTERING] Jawaban benar & label NLI yang TIDAK sesuai: {after_filtering_metric_array[2]}\")\n",
    "    print(f\"[AFTER FILTERING] Jawaban TIDAK benar & label NLI yang TIDAK sesuai: {after_filtering_metric_array[3]}\")\n",
    "    print()\n",
    "\n",
    "    print(\"Metrik di atas, bisa direpresentasikan menjadi:\")\n",
    "\n",
    "    acc_before_whole, prec_before_whole, rec_before_whole, f1_before_whole = compute_f1_prec_rec_whole(\n",
    "        before_filtering_metric_array)\n",
    "    acc_after_whole, prec_after_whole, rec_after_whole, f1_after_whole = compute_f1_prec_rec_whole(\n",
    "        after_filtering_metric_array)\n",
    "\n",
    "    print(f\"[BEFORE FILTERING] Akurasi: {acc_before_whole}\")\n",
    "    print(f\"[BEFORE FILTERING] Precision: {prec_before_whole}\")\n",
    "    print(f\"[BEFORE FILTERING] Recall: {rec_before_whole}\")\n",
    "    print(f\"[BEFORE FILTERING] F1: {f1_before_whole}\")\n",
    "    print()\n",
    "\n",
    "    print(f\"[AFTER FILTERING] Akurasi: {acc_after_whole}\")\n",
    "    print(f\"[AFTER FILTERING] Precision: {prec_after_whole}\")\n",
    "    print(f\"[AFTER FILTERING] Recall: {rec_after_whole}\")\n",
    "    print(f\"[AFTER FILTERING] F1: {f1_after_whole}\")\n",
    "    print()\n",
    "\n",
    "    print(\"--- Persentase perubahan hasil metrik ---\")\n",
    "    print(\"~ METRIK PER TOKEN ~\")\n",
    "    diff_verbose_metric(em_before, em_after, \"Exact Match\")\n",
    "    diff_verbose_metric(f1_before, f1_after, \"F1\")\n",
    "    print()\n",
    "\n",
    "    print(\"~ METRIK DENGAN PARAMETER NLI ~\")\n",
    "    diff_verbose_metric(before_filtering_metric_array[0], after_filtering_metric_array[0], \n",
    "                        \"Jawaban benar & label NLI yang sesuai\")\n",
    "    diff_verbose_metric(before_filtering_metric_array[1], after_filtering_metric_array[1], \n",
    "                        \"Jawaban TIDAK benar & label NLI yang sesuai\")\n",
    "    diff_verbose_metric(before_filtering_metric_array[2], after_filtering_metric_array[2], \n",
    "                        \"Jawaban benar & label NLI yang TIDAK sesuai\")\n",
    "    diff_verbose_metric(before_filtering_metric_array[3], after_filtering_metric_array[3], \n",
    "                        \"Jawaban TIDAK benar & label NLI yang TIDAK sesuai\")\n",
    "    print()\n",
    "\n",
    "    print(\"Metrik di atas, bisa direpresentasikan menjadi:\")\n",
    "    diff_verbose_metric(acc_before_whole, acc_after_whole, \"Akurasi\")\n",
    "    diff_verbose_metric(prec_before_whole, prec_after_whole, \"Precision\")\n",
    "    diff_verbose_metric(rec_before_whole, rec_after_whole, \"Recall\")\n",
    "    diff_verbose_metric(f1_before_whole, f1_after_whole, \"F1\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~ METRIK PER TOKEN ~\n",
      "Skor Exact Match sebelum filtering NLI: 36.518324607329845\n",
      "Skor F1 sebelum filtering NLI: 42.63542386279981\n",
      "\n",
      "Skor Exact Match setelah filtering NLI: 62.17277486910995\n",
      "Skor F1 setelah filtering NLI: 66.467874209069\n",
      "\n",
      "~ METRIK DENGAN PARAMETER NLI ~\n",
      "[BEFORE FILTERING] Jawaban benar & label NLI yang sesuai: 199\n",
      "[BEFORE FILTERING] Jawaban TIDAK benar & label NLI yang sesuai: 185\n",
      "[BEFORE FILTERING] Jawaban benar & label NLI yang TIDAK sesuai: 80\n",
      "[BEFORE FILTERING] Jawaban TIDAK benar & label NLI yang TIDAK sesuai: 300\n",
      "\n",
      "[AFTER FILTERING] Jawaban benar & label NLI yang sesuai: 199\n",
      "[AFTER FILTERING] Jawaban TIDAK benar & label NLI yang sesuai: 185\n",
      "[AFTER FILTERING] Jawaban benar & label NLI yang TIDAK sesuai: 276\n",
      "[AFTER FILTERING] Jawaban TIDAK benar & label NLI yang TIDAK sesuai: 104\n",
      "\n",
      "Metrik di atas, bisa direpresentasikan menjadi:\n",
      "[BEFORE FILTERING] Akurasi: 0.6531413612565445\n",
      "[BEFORE FILTERING] Precision: 0.5182291666666666\n",
      "[BEFORE FILTERING] Recall: 0.7132616487455197\n",
      "[BEFORE FILTERING] F1: 0.6003016591251885\n",
      "\n",
      "[AFTER FILTERING] Akurasi: 0.39659685863874344\n",
      "[AFTER FILTERING] Precision: 0.5182291666666666\n",
      "[AFTER FILTERING] Recall: 0.4189473684210526\n",
      "[AFTER FILTERING] F1: 0.46332945285215366\n",
      "\n",
      "--- Persentase perubahan hasil metrik ---\n",
      "~ METRIK PER TOKEN ~\n",
      "Hasil metrik Exact Match setelah filtering NLI mengalami KENAIKAN sebesar: 70.25 %\n",
      "Hasil metrik F1 setelah filtering NLI mengalami KENAIKAN sebesar: 55.9 %\n",
      "\n",
      "~ METRIK DENGAN PARAMETER NLI ~\n",
      "Hasil nilai Jawaban benar & label NLI yang sesuai sebelum filtering NLI SAMA DENGAN metrik setelah filtering NLI\n",
      "Hasil nilai Jawaban TIDAK benar & label NLI yang sesuai sebelum filtering NLI SAMA DENGAN metrik setelah filtering NLI\n",
      "Hasil nilai Jawaban benar & label NLI yang TIDAK sesuai setelah filtering NLI mengalami KENAIKAN sebesar: 245.0 %\n",
      "Hasil nilai Jawaban TIDAK benar & label NLI yang TIDAK sesuai setelah filtering NLI mengalami PENURUNAN sebesar: 65.33 %\n",
      "\n",
      "Metrik di atas, bisa direpresentasikan menjadi:\n",
      "Hasil metrik Akurasi setelah filtering NLI mengalami PENURUNAN sebesar: 39.28 %\n",
      "Hasil metrik Precision sebelum filtering NLI SAMA DENGAN metrik setelah filtering NLI\n",
      "Hasil metrik Recall setelah filtering NLI mengalami PENURUNAN sebesar: 41.26 %\n",
      "Hasil metrik F1 setelah filtering NLI mengalami PENURUNAN sebesar: 22.82 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_metrics(metric_result_before_filtering, metric_result_after_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(ACCURACY_DIR), exist_ok=True)\n",
    "with open(f'{ACCURACY_DIR}/metric_comparison_results.txt', \"w\") as f, contextlib.redirect_stdout(f):\n",
    "    compare_metrics(metric_result_before_filtering, metric_result_after_filtering)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakdown_evaluation(df, TYPE_QAS, MAXIMUM_SEARCH_ITER=MAXIMUM_SEARCH_ITER):\n",
    "    \n",
    "    exist_true_answer_label_entailment = 0\n",
    "    exist_true_answer_label_neutral = 0\n",
    "    exist_true_answer_label_contradiction = 0\n",
    "    \n",
    "    exist_false_answer_label_entailment = 0\n",
    "    exist_false_answer_label_neutral = 0\n",
    "    exist_false_answer_label_contradiction = 0\n",
    "    \n",
    "    no_exist_true_answer = 0\n",
    "    no_exist_false_answer = 0\n",
    "    \n",
    "    filtered_in_right_answer_to_filtered_out_wrong_answer = 0\n",
    "    filtered_out_wrong_answer_to_filtered_in_right_answer = 0\n",
    "    \n",
    "    filtered_in_right_answer_to_filtered_out_wrong_answer_unanswered = 0\n",
    "    filtered_out_wrong_answer_to_filtered_in_right_answer_unanswered = 0\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        pred_answer_before_filtering = df[\"Prediction Answer Before Filtering\"][i][-1]\n",
    "        pred_answer_after_filtering = df[\"Prediction Answer After Filtering\"][i][-1]\n",
    "        \n",
    "        pred_label_before_filtering = df[\"Label Before Filtering\"][i][-1]['label']\n",
    "        pred_label_after_filtering = df[\"Label After Filtering\"][i][-1]['label']\n",
    "        \n",
    "        pred_prob_dist_before_filtering = df[\"Label Before Filtering\"][i][-1]['score']\n",
    "        pred_prob_dist_after_filtering = df[\"Label After Filtering\"][i][-1]['score']\n",
    "        \n",
    "        gold_text = df[\"Gold Answer\"][i]\n",
    "        \n",
    "        if (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'entailment') and (pred_answer_before_filtering != \"\"):\n",
    "            exist_true_answer_label_entailment += 1\n",
    "        elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'neutral') and (pred_answer_before_filtering != \"\"):\n",
    "            exist_true_answer_label_neutral += 1\n",
    "            if (TYPE_QAS == 'entailment only'):\n",
    "                if (pred_answer_after_filtering != gold_text):\n",
    "                    filtered_in_right_answer_to_filtered_out_wrong_answer += 1\n",
    "                #print(f\"Prob Distribution right answer but wrong label NLI before filtering: {pred_prob_dist_before_filtering}\")\n",
    "                #print(f\"Prob Distribution right answer but wrong label NLI after filtering: {pred_prob_dist_after_filtering}\")\n",
    "                #print()\n",
    "                \n",
    "        elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'contradiction') and (pred_answer_before_filtering != \"\"):\n",
    "            exist_true_answer_label_contradiction += 1\n",
    "            if (pred_answer_after_filtering != gold_text):\n",
    "                filtered_in_right_answer_to_filtered_out_wrong_answer += 1\n",
    "            #print(f\"Prob Distribution right answer but wrong label NLI before filtering: {pred_prob_dist_before_filtering}\")\n",
    "            #print(f\"Prob Distribution right answer but wrong label NLI after filtering: {pred_prob_dist_after_filtering}\")\n",
    "            #print()\n",
    "        \n",
    "        elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'entailment') and (pred_answer_before_filtering != \"\"):\n",
    "            exist_false_answer_label_entailment += 1\n",
    "        elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'neutral') and (pred_answer_before_filtering != \"\"):\n",
    "            exist_false_answer_label_neutral += 1\n",
    "            if (TYPE_QAS == 'entailment only'):\n",
    "                if (pred_answer_after_filtering == gold_text):\n",
    "                    filtered_out_wrong_answer_to_filtered_in_right_answer += 1\n",
    "        elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'contradiction') and (pred_answer_before_filtering != \"\"):\n",
    "            exist_false_answer_label_contradiction += 1\n",
    "            if (pred_answer_after_filtering == gold_text):\n",
    "                filtered_out_wrong_answer_to_filtered_in_right_answer += 1\n",
    "        \n",
    "        elif (pred_answer_before_filtering == gold_text) and (pred_answer_before_filtering == \"\"):\n",
    "            no_exist_true_answer += 1\n",
    "            if (pred_answer_after_filtering != gold_text):\n",
    "                filtered_in_right_answer_to_filtered_out_wrong_answer_unanswered += 1\n",
    "        elif (pred_answer_before_filtering != gold_text) and (pred_answer_before_filtering == \"\"):\n",
    "            no_exist_false_answer += 1\n",
    "            if (pred_answer_after_filtering == gold_text):\n",
    "                filtered_out_wrong_answer_to_filtered_in_right_answer_unanswered += 1\n",
    "            \n",
    "        #print(f\"Pred answer before filtering: {pred_answer_before_filtering}\")\n",
    "        #print(f\"Pred answer after filtering: {pred_answer_after_filtering}\")\n",
    "        #print(f\"Gold answer: {gold_text}\")\n",
    "        #print()\n",
    "    \n",
    "    print(f\"Jawaban benar (answer exist) entailment: {exist_true_answer_label_entailment}\")\n",
    "    print(f\"Jawaban benar (answer exist) neutral: {exist_true_answer_label_neutral}\")\n",
    "    print(f\"Jawaban benar (answer exist) contradiction: {exist_true_answer_label_contradiction}\")\n",
    "    print()\n",
    "    print(f\"Jawaban salah (answer exist) entailment: {exist_false_answer_label_entailment}\")\n",
    "    print(f\"Jawaban salah (answer exist) neutral: {exist_false_answer_label_neutral}\")\n",
    "    print(f\"Jawaban salah (answer exist) contradiction: {exist_false_answer_label_contradiction}\")\n",
    "    print()\n",
    "    print(f\"Jawaban prediksi no-answer benar: {no_exist_true_answer}\")\n",
    "    print(f\"Jawaban prediksi no-answer salah: {no_exist_false_answer}\")\n",
    "    print()\n",
    "    print(f\"Jawaban prediksi benar, tapi tidak masuk prediksi final (menjadi salah) karena label NLI yang tidak sesuai: {filtered_in_right_answer_to_filtered_out_wrong_answer}\")\n",
    "    print(f\"Jawaban prediksi salah, tapi masuk prediksi final (menjadi benar) karena label NLI yang sesuai: {filtered_out_wrong_answer_to_filtered_in_right_answer}\")\n",
    "    print()\n",
    "    print(f\"Jawaban prediksi benar (unanswerable), tapi tidak masuk prediksi final (menjadi salah) karena label NLI yang tidak sesuai: {filtered_in_right_answer_to_filtered_out_wrong_answer_unanswered}\")\n",
    "    print(f\"Jawaban prediksi salah (unanswerable), tapi masuk prediksi final (menjadi benar) karena label NLI yang sesuai: {filtered_out_wrong_answer_to_filtered_in_right_answer_unanswered}\")\n",
    "    print()\n",
    "    print(f\"Total prediksi jawaban: {len(df)}\")\n",
    "    print()\n",
    "    \n",
    "    assert len(df) == exist_true_answer_label_entailment+exist_true_answer_label_neutral+exist_true_answer_label_contradiction+\\\n",
    "            exist_false_answer_label_entailment+exist_false_answer_label_neutral+exist_false_answer_label_contradiction+\\\n",
    "            no_exist_true_answer+no_exist_false_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jawaban benar (answer exist) entailment: 197\n",
      "Jawaban benar (answer exist) neutral: 53\n",
      "Jawaban benar (answer exist) contradiction: 17\n",
      "\n",
      "Jawaban salah (answer exist) entailment: 180\n",
      "Jawaban salah (answer exist) neutral: 192\n",
      "Jawaban salah (answer exist) contradiction: 106\n",
      "\n",
      "Jawaban prediksi no-answer benar: 12\n",
      "Jawaban prediksi no-answer salah: 7\n",
      "\n",
      "Jawaban prediksi benar, tapi tidak masuk prediksi final (menjadi salah) karena label NLI yang tidak sesuai: 70\n",
      "Jawaban prediksi salah, tapi masuk prediksi final (menjadi benar) karena label NLI yang sesuai: 239\n",
      "\n",
      "Jawaban prediksi benar (unanswerable), tapi tidak masuk prediksi final (menjadi salah) karena label NLI yang tidak sesuai: 5\n",
      "Jawaban prediksi salah (unanswerable), tapi masuk prediksi final (menjadi benar) karena label NLI yang sesuai: 0\n",
      "\n",
      "Total prediksi jawaban: 764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "breakdown_evaluation(eval_df, TYPE_QAS='entailment only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9994680285453796"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['Label After Filtering'][2][0]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05c4878c74ad419f81e6abc2b9f9fae1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0633918b582f45c38356130338ccfb58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8aa4db7376e3445089af950c446aab70",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d7c6178f42ed4b9f8c5169c21783ce2d",
      "value": 2
     }
    },
    "065d96b02c7840019af97e3c135693a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ce874e529474f478a7acd45b4ffd649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_902bd6c12ea54039b8c913a7c1782ff4",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d86de7f1fbf452892fe6d4b177558f9",
      "value": 6
     }
    },
    "10fb50db270449b48382c815904245a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc7073fb093543fc990abc620348c1ca",
      "placeholder": "​",
      "style": "IPY_MODEL_acac4d071be4488fbb272df63b625213",
      "value": " 66/66 [23:11&lt;00:00, 14.47s/ba]"
     }
    },
    "113ae282ca5a499a8a2ad36a796e763f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f10632530a1b499ba3d20f0286446bab",
      "placeholder": "​",
      "style": "IPY_MODEL_94da3d6dfccc4a289c659b94bcc90658",
      "value": "#0: 100%"
     }
    },
    "12c3fd408f914038acb511c4ef57f965": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13ea541bb13b42738e74885a2c1db8df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14a450c5594148c3a4ca340521d167ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13ea541bb13b42738e74885a2c1db8df",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_68818d7eea5849afa82c3003a57a5b8d",
      "value": 6
     }
    },
    "1d86de7f1fbf452892fe6d4b177558f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23ce34e28a524ad9ab44a3715c8be175": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05c4878c74ad419f81e6abc2b9f9fae1",
      "placeholder": "​",
      "style": "IPY_MODEL_7fc9c87c6cc44cdf82ea4f74c0bfe7b4",
      "value": "100%"
     }
    },
    "2721b15477d741519a3041a8f256575b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a373f9cb48a481d85803f0792ff27b3",
       "IPY_MODEL_54a2637e8c8e4ca0917e35f4b58b48f3",
       "IPY_MODEL_7dc3dc97044c48afb06a14575fc8e91b"
      ],
      "layout": "IPY_MODEL_6c3c7e4da5af40b088f3046d7698edff"
     }
    },
    "29b13cbb638947d5a17071d299e2420f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_113ae282ca5a499a8a2ad36a796e763f",
       "IPY_MODEL_0ce874e529474f478a7acd45b4ffd649",
       "IPY_MODEL_935752d8733c41fbb25ed436606352ca"
      ],
      "layout": "IPY_MODEL_cf2b519cc5ff446dae45afa2439c840e"
     }
    },
    "2e0e9b7ee63b4c748d6294d63cc7b088": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd4d3c6663db46c9934c0d1f7a128d4c",
      "placeholder": "​",
      "style": "IPY_MODEL_3db65eb148504de6a4207708554874f0",
      "value": " 2/2 [00:00&lt;00:00,  8.75it/s]"
     }
    },
    "3db65eb148504de6a4207708554874f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f3c8a647bb24840ae71ce6e3219f4dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23ce34e28a524ad9ab44a3715c8be175",
       "IPY_MODEL_87c26a64111847f3a6cdb2f0797ae1f5",
       "IPY_MODEL_2e0e9b7ee63b4c748d6294d63cc7b088"
      ],
      "layout": "IPY_MODEL_750e7a8018e6415f8cfc15288d80f13f"
     }
    },
    "3f6183fb8ca348e0ac35b495681c705a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45f8bfb31705441c80a22dbb955dd933": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4836cff910304df894759890642ac333": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f2a3658a0f14f66a59c14d36558c7e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86b094a2e5564e4d8af8693dab674961",
      "placeholder": "​",
      "style": "IPY_MODEL_7fc619affab644758cfea5857adb7c93",
      "value": " 2/2 [00:00&lt;00:00,  4.72it/s]"
     }
    },
    "4f80ae88d884407ab8f9e08aa58632a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "540c7fcbf3b14e538cba18329bc1fc20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54a2637e8c8e4ca0917e35f4b58b48f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dce089e985634a67a381bc85cbaca017",
      "max": 66,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd9cb96d5a3c4400bb7d145590540a54",
      "value": 66
     }
    },
    "6516c5b02aa24db2bc42cdd4d1f2c260": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cbed771e77314789bf8817404e5d6f67",
       "IPY_MODEL_b0be384a887847e981868df7c551d3de",
       "IPY_MODEL_10fb50db270449b48382c815904245a8"
      ],
      "layout": "IPY_MODEL_b23174d38e3940718252770bd39177f8"
     }
    },
    "68818d7eea5849afa82c3003a57a5b8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a373f9cb48a481d85803f0792ff27b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a040ab99cb964c32a0a968d97dd55a16",
      "placeholder": "​",
      "style": "IPY_MODEL_9ef8c0b24cb5490b8fa7e8ea8ad619fe",
      "value": "#0: 100%"
     }
    },
    "6c3c7e4da5af40b088f3046d7698edff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "750e7a8018e6415f8cfc15288d80f13f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b80b681b46b4671a14a7cf82fd50991": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dc3dc97044c48afb06a14575fc8e91b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f940438113734ccdb198efd12b8a3936",
      "placeholder": "​",
      "style": "IPY_MODEL_c5b567cd34b340dea77cefd40e9aaec9",
      "value": " 66/66 [23:07&lt;00:00, 15.32s/ba]"
     }
    },
    "7fc619affab644758cfea5857adb7c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fc9c87c6cc44cdf82ea4f74c0bfe7b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8104b020ba8b498591e14a51af34306d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83abc2e663e447e7af2f826eacd155c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86b094a2e5564e4d8af8693dab674961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87c26a64111847f3a6cdb2f0797ae1f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb1fb197045a44ab8c204a8bdc3c4584",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4836cff910304df894759890642ac333",
      "value": 2
     }
    },
    "8aa4db7376e3445089af950c446aab70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ce95fb7cd2f4cf49b54dfa319f1e835": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83abc2e663e447e7af2f826eacd155c7",
      "placeholder": "​",
      "style": "IPY_MODEL_8104b020ba8b498591e14a51af34306d",
      "value": "100%"
     }
    },
    "902bd6c12ea54039b8c913a7c1782ff4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "935752d8733c41fbb25ed436606352ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f6183fb8ca348e0ac35b495681c705a",
      "placeholder": "​",
      "style": "IPY_MODEL_bdf2cb9d10cc4b2aab466a9dfe7be5bc",
      "value": " 6/6 [02:04&lt;00:00, 20.17s/ba]"
     }
    },
    "94da3d6dfccc4a289c659b94bcc90658": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c9b27a0b2ac4be3b8dcbb65e23b84a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_065d96b02c7840019af97e3c135693a8",
      "placeholder": "​",
      "style": "IPY_MODEL_45f8bfb31705441c80a22dbb955dd933",
      "value": " 6/6 [02:03&lt;00:00, 20.04s/ba]"
     }
    },
    "9ef8c0b24cb5490b8fa7e8ea8ad619fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a040ab99cb964c32a0a968d97dd55a16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acac4d071be4488fbb272df63b625213": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0be384a887847e981868df7c551d3de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12c3fd408f914038acb511c4ef57f965",
      "max": 66,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_540c7fcbf3b14e538cba18329bc1fc20",
      "value": 66
     }
    },
    "b23174d38e3940718252770bd39177f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bce02c83fbc142c5bb2d8c1f905a478d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd9cb96d5a3c4400bb7d145590540a54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bdf2cb9d10cc4b2aab466a9dfe7be5bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c069967001a542f5b0c3b88ff1841eba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f46e4c24b9294ffdb25e1e64bc594eee",
       "IPY_MODEL_14a450c5594148c3a4ca340521d167ba",
       "IPY_MODEL_9c9b27a0b2ac4be3b8dcbb65e23b84a2"
      ],
      "layout": "IPY_MODEL_c99472b37d644d63a471b65ecbe3bfbd"
     }
    },
    "c5b567cd34b340dea77cefd40e9aaec9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c99472b37d644d63a471b65ecbe3bfbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb1fb197045a44ab8c204a8bdc3c4584": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbed771e77314789bf8817404e5d6f67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b80b681b46b4671a14a7cf82fd50991",
      "placeholder": "​",
      "style": "IPY_MODEL_bce02c83fbc142c5bb2d8c1f905a478d",
      "value": "#1: 100%"
     }
    },
    "cf2b519cc5ff446dae45afa2439c840e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7c6178f42ed4b9f8c5169c21783ce2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dce089e985634a67a381bc85cbaca017": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd4d3c6663db46c9934c0d1f7a128d4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8eebd19f57348669da4353e690a64eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ce95fb7cd2f4cf49b54dfa319f1e835",
       "IPY_MODEL_0633918b582f45c38356130338ccfb58",
       "IPY_MODEL_4f2a3658a0f14f66a59c14d36558c7e6"
      ],
      "layout": "IPY_MODEL_f2758827907544acaa282673eebff9d0"
     }
    },
    "e9ca9f63982744aba53e883882373160": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f10632530a1b499ba3d20f0286446bab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2758827907544acaa282673eebff9d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f46e4c24b9294ffdb25e1e64bc594eee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9ca9f63982744aba53e883882373160",
      "placeholder": "​",
      "style": "IPY_MODEL_4f80ae88d884407ab8f9e08aa58632a4",
      "value": "#1: 100%"
     }
    },
    "f940438113734ccdb198efd12b8a3936": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc7073fb093543fc990abc620348c1ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
