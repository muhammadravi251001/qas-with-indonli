{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhsDF_5UdqLI"
   },
   "source": [
    "# Menjalankan QA Tanpa Intermediate Task - Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKm8qASDXqR-"
   },
   "source": [
    "# Import semua module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DH9DifnXp5_",
    "outputId": "94161dcf-e635-486d-c6d8-5747b267b64f"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install transformers\n",
    "#!pip install tensorboard\n",
    "#!pip install evaluate\n",
    "#!pip install git+https://github.com/IndoNLP/nusa-crowd.git@release_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp==3.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (3.8.1)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: anyio==3.6.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (3.6.2)\n",
      "Requirement already satisfied: appdirs==1.4.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: argon2-cffi==21.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (21.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (21.2.0)\n",
      "Requirement already satisfied: arrow==1.2.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (1.2.3)\n",
      "Requirement already satisfied: asttokens==2.2.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (2.2.1)\n",
      "Requirement already satisfied: async-timeout==4.0.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (4.0.2)\n",
      "Requirement already satisfied: attrs==22.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (22.2.0)\n",
      "Requirement already satisfied: audioread==3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (0.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (4.12.2)\n",
      "Requirement already satisfied: bioc==1.3.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 14)) (1.3.7)\n",
      "Requirement already satisfied: black==22.12.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (22.12.0)\n",
      "Requirement already satisfied: bleach==6.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 16)) (6.0.0)\n",
      "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 17)) (2022.12.7)\n",
      "Requirement already satisfied: cffi==1.15.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 18)) (1.15.1)\n",
      "Requirement already satisfied: cfgv==3.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 19)) (3.3.1)\n",
      "Requirement already satisfied: charset-normalizer==2.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 20)) (2.1.1)\n",
      "Requirement already satisfied: click==8.1.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 21)) (8.1.3)\n",
      "Requirement already satisfied: cmake==3.26.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 22)) (3.26.1)\n",
      "Requirement already satisfied: comm==0.1.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 23)) (0.1.3)\n",
      "Requirement already satisfied: conllu==4.5.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 24)) (4.5.2)\n",
      "Requirement already satisfied: datasets==2.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 25)) (2.2.0)\n",
      "Requirement already satisfied: debugpy==1.6.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 26)) (1.6.7)\n",
      "Requirement already satisfied: decorator==5.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 27)) (5.1.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 28)) (0.7.1)\n",
      "Requirement already satisfied: dill==0.3.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 29)) (0.3.6)\n",
      "Requirement already satisfied: distlib==0.3.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 30)) (0.3.6)\n",
      "Requirement already satisfied: docutils==0.19 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 31)) (0.19)\n",
      "Requirement already satisfied: et-xmlfile==1.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 32)) (1.1.0)\n",
      "Requirement already satisfied: evaluate==0.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 33)) (0.4.0)\n",
      "Requirement already satisfied: executing==1.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 34)) (1.2.0)\n",
      "Requirement already satisfied: nbclient==0.7.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 86)) (0.7.3)\n",
      "Requirement already satisfied: nbconvert==7.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 87)) (7.3.0)\n",
      "Requirement already satisfied: nbformat==5.8.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 88)) (5.8.0)\n",
      "Requirement already satisfied: nest-asyncio==1.5.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 89)) (1.5.6)\n",
      "Requirement already satisfied: networkx==3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 90)) (3.1)\n",
      "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 91)) (3.8.1)\n",
      "Requirement already satisfied: nodeenv==1.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 92)) (1.7.0)\n",
      "Requirement already satisfied: notebook==6.5.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 93)) (6.5.4)\n",
      "Requirement already satisfied: notebook_shim==0.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 94)) (0.2.2)\n",
      "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 95)) (0.56.4)\n",
      "Requirement already satisfied: numpy==1.21.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 96)) (1.21.4)\n",
      "Requirement already satisfied: nusacrowd@ git+https://github.com/IndoNLP/nusa-crowd.git@7748513d20331e72f9969f94f5d43c7f2d4a59a5 from git+https://github.com/IndoNLP/nusa-crowd.git@7748513d20331e72f9969f94f5d43c7f2d4a59a5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 97)) (0.0.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 98)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 99)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 100)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 101)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 102)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 103)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 104)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 105)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 106)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 107)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 108)) (11.7.91)\n",
      "Requirement already satisfied: openpyxl==3.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 109)) (3.1.2)\n",
      "Requirement already satisfied: packaging==23.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 110)) (23.0)\n",
      "Requirement already satisfied: pandas==1.3.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 111)) (1.3.3)\n",
      "Requirement already satisfied: pandocfilters==1.5.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 112)) (1.5.0)\n",
      "Requirement already satisfied: parso==0.8.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 113)) (0.8.3)\n",
      "Requirement already satisfied: pathspec==0.11.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 114)) (0.11.1)\n",
      "Requirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 115)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 116)) (0.7.5)\n",
      "Requirement already satisfied: Pillow==8.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 117)) (8.4.0)\n",
      "Requirement already satisfied: pkgutil_resolve_name==1.3.10 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 118)) (1.3.10)\n",
      "Requirement already satisfied: platformdirs==3.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 119)) (3.2.0)\n",
      "Requirement already satisfied: polygraphy==0.33.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 120)) (0.33.2)\n",
      "Requirement already satisfied: pooch==1.6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 121)) (1.6.0)\n",
      "Requirement already satisfied: pre-commit==2.19.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 122)) (2.19.0)\n",
      "Requirement already satisfied: prometheus-client==0.16.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 123)) (0.16.0)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.38 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 124)) (3.0.38)\n",
      "Requirement already satisfied: protobuf==3.19.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 125)) (3.19.1)\n",
      "Requirement already satisfied: psutil==5.9.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 126)) (5.9.4)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 127)) (0.7.0)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 128)) (0.2.2)\n",
      "Requirement already satisfied: pyarrow==11.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 129)) (11.0.0)\n",
      "Requirement already satisfied: pycodestyle==2.10.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 130)) (2.10.0)\n",
      "Requirement already satisfied: pycparser==2.21 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 131)) (2.21)\n",
      "Requirement already satisfied: pycuda==2021.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 132)) (2021.1)\n",
      "Requirement already satisfied: pyflakes==3.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 133)) (3.0.1)\n",
      "Requirement already satisfied: Pygments==2.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 134)) (2.14.0)\n",
      "Requirement already satisfied: pyrsistent==0.19.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 135)) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 136)) (2.8.2)\n",
      "Requirement already satisfied: python-json-logger==2.0.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 137)) (2.0.7)\n",
      "Requirement already satisfied: pytools==2021.2.9 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 138)) (2021.2.9)\n",
      "Requirement already satisfied: pytz==2023.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 139)) (2023.3)\n",
      "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 140)) (6.0)\n",
      "Requirement already satisfied: pyzmq==25.0.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 141)) (25.0.2)\n",
      "Requirement already satisfied: qtconsole==5.4.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 142)) (5.4.2)\n",
      "Requirement already satisfied: QtPy==2.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 143)) (2.3.1)\n",
      "Requirement already satisfied: regex==2023.3.23 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 144)) (2023.3.23)\n",
      "Requirement already satisfied: requests==2.28.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 145)) (2.28.2)\n",
      "Requirement already satisfied: responses==0.18.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 146)) (0.18.0)\n",
      "Requirement already satisfied: rfc3339-validator==0.1.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 147)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator==0.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 148)) (0.1.1)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 149)) (1.2.2)\n",
      "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 150)) (1.10.1)\n",
      "Requirement already satisfied: Send2Trash==1.8.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 151)) (1.8.0)\n",
      "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 152)) (1.16.0)\n",
      "Requirement already satisfied: sniffio==1.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 153)) (1.3.0)\n",
      "Requirement already satisfied: soundfile==0.12.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 154)) (0.12.1)\n",
      "Requirement already satisfied: soupsieve==2.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 155)) (2.4)\n",
      "Requirement already satisfied: soxr==0.3.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 156)) (0.3.5)\n",
      "Requirement already satisfied: stack-data==0.6.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 157)) (0.6.2)\n",
      "Requirement already satisfied: sympy==1.11.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 158)) (1.11.1)\n",
      "Requirement already satisfied: tensorrt@ file:///TensorRT-8.0.3.4/python/tensorrt-8.0.3.4-cp38-none-linux_x86_64.whl from file:///TensorRT-8.0.3.4/python/tensorrt-8.0.3.4-cp38-none-linux_x86_64.whl in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 159)) (8.0.3.4)\n",
      "Requirement already satisfied: terminado==0.17.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 160)) (0.17.1)\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 161)) (3.1.0)\n",
      "Requirement already satisfied: tinycss2==1.2.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 162)) (1.2.1)\n",
      "Requirement already satisfied: tokenizers==0.13.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 163)) (0.13.3)\n",
      "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 164)) (0.10.2)\n",
      "Requirement already satisfied: tomli==2.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 165)) (2.0.1)\n",
      "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 166)) (2.0.0)\n",
      "Requirement already satisfied: torchaudio==2.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 167)) (2.0.1)\n",
      "Requirement already satisfied: tornado==6.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 168)) (6.2)\n",
      "Requirement already satisfied: tqdm==4.65.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 169)) (4.65.0)\n",
      "Requirement already satisfied: traitlets==5.9.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 170)) (5.9.0)\n",
      "Requirement already satisfied: transformers==4.27.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 171)) (4.27.4)\n",
      "Requirement already satisfied: translate-toolkit==3.8.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 172)) (3.8.6)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 173)) (2.0.0)\n",
      "Requirement already satisfied: typing_extensions==4.5.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 174)) (4.5.0)\n",
      "Requirement already satisfied: tzdata==2023.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 175)) (2023.3)\n",
      "Requirement already satisfied: uff@ file:///TensorRT-8.0.3.4/uff/uff-0.6.9-py2.py3-none-any.whl from file:///TensorRT-8.0.3.4/uff/uff-0.6.9-py2.py3-none-any.whl in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 176)) (0.6.9)\n",
      "Requirement already satisfied: uri-template==1.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 177)) (1.2.0)\n",
      "Requirement already satisfied: urllib3==1.26.15 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 178)) (1.26.15)\n",
      "Requirement already satisfied: virtualenv==20.21.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 179)) (20.21.0)\n",
      "Requirement already satisfied: wcwidth==0.2.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 180)) (0.2.6)\n",
      "Requirement already satisfied: webcolors==1.13 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 181)) (1.13)\n",
      "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 182)) (0.5.1)\n",
      "Requirement already satisfied: websocket-client==1.5.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 183)) (1.5.1)\n",
      "Requirement already satisfied: widgetsnbextension==4.0.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 184)) (4.0.7)\n",
      "Requirement already satisfied: xxhash==3.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 185)) (3.2.0)\n",
      "Requirement already satisfied: yarl==1.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 186)) (1.8.2)\n",
      "Requirement already satisfied: zipp==3.15.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 187)) (3.15.0)\n",
      "Requirement already satisfied: zstandard==0.20.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 188)) (0.20.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nodeenv==1.7.0->-r requirements.txt (line 92)) (45.2.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->-r requirements.txt (line 98)) (0.34.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade pip\n",
    "!pip install -r requirements.txt --use-deprecated=legacy-resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 14 10:05:44 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   59C    P0   257W / 300W |  26259MiB / 32510MiB |     40%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    44W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   60C    P0   265W / 300W |  23870MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    92W / 300W |  20212MiB / 32510MiB |     16%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   47C    P0   130W / 300W |  20212MiB / 32510MiB |     59%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   47C    P0    68W / 300W |  19940MiB / 32510MiB |     43%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   55C    P0   142W / 300W |  19547MiB / 32510MiB |     73%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   50C    P0   287W / 300W |  31560MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Melihat GPU yang tersedia dan penggunaannya.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memilih GPU yang akan digunakan (contohnya: GPU #7)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pgY_FL4lXuEu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "import operator\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from evaluate import load\n",
    "from nusacrowd import NusantaraConfigHelper\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from huggingface_hub import notebook_login\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    load_from_disk,\n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "from transformers import (\n",
    "    BigBirdTokenizerFast,\n",
    "    BigBirdForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BertForSequenceClassification,\n",
    "    BertForQuestionAnswering,\n",
    "    AutoModel, \n",
    "    BertTokenizerFast,\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    BertTokenizer, \n",
    "    BertForPreTraining,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    EvalPrediction,\n",
    "    AutoModel,\n",
    "    BertModel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtu0yFqMYsX9"
   },
   "source": [
    "# Definisikan hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gjHJwpxeYugs"
   },
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"indolem/indobert-base-uncased\"\n",
    "#MODEL_NAME = \"afaji/fine-tuned-IndoNLI-Translated-with-indobert-base-uncased\"\n",
    "MODEL_NAME = \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\"\n",
    "#MODEL_NAME = \"indobenchmark/indobert-large-p2\"\n",
    "SEED = 42\n",
    "EPOCH = 1\n",
    "BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_LENGTH = 400\n",
    "STRIDE = 100\n",
    "LOGGING_STEPS = 50\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAXIMUM_SEARCH_ITER =  2\n",
    "# Untuk mempercepat training, saya ubah SAMPLE menjadi 100.\n",
    "# Bila mau menggunakan keseluruhan data, gunakan: \n",
    "SAMPLE = sys.maxsize\n",
    "# SAMPLE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyH7dlPGYOSk"
   },
   "source": [
    "# Import dataset QAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/IndoNLP/nusa-crowd.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==1.7.0 torchvision==0.8.1 -f https://download.pytorch.org/whl/cu101/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#from tensorflow.python.platform import build_info as build\n",
    "#print(f\"tensorflow version: {tf.__version__}\")\n",
    "#print(f\"Cuda Version: {build.build_info['cuda_version']}\")\n",
    "#print(f\"Cudnn version: {build.build_info['cudnn_version']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall torch\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cudatoolkit==11.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install cudatoolkit=11.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch._C._cuda_getDeviceCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlNhPlmDY2tk"
   },
   "source": [
    "# Definisikan tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "z3uXWOkUY4GD"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYYbFXzWYTr3"
   },
   "source": [
    "# Definisikan fungsi pre-processnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset idk_mrc (/root/.cache/huggingface/datasets/idk_mrc/idk_mrc_source/1.0.0/cf468d86fa7341e69998db1449851672ebfb4fa46036929d66b9de15c421334f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea38f714ccd406da0d0b02e361bce3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 358/358 [00:01<00:00, 200.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3659/3659 [00:23<00:00, 154.30it/s]\n"
     ]
    }
   ],
   "source": [
    "conhelps = NusantaraConfigHelper()\n",
    "data_qas_id = conhelps.filtered(lambda x: 'idk_mrc' in x.dataset_name)[0].load_dataset()\n",
    "\n",
    "df_train = pd.DataFrame(data_qas_id['train'])\n",
    "df_validation = pd.DataFrame(data_qas_id['validation'])\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_val = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_validation['context']))):\n",
    "    for j in df_validation[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                       \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                       \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                       ignore_index=True)\n",
    "        else:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": str(), \n",
    "                                                       \"answer_start\": 0, \n",
    "                                                       \"answer_end\": 0}}, \n",
    "                                                       ignore_index=True)\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_train = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_train['context']))):\n",
    "    for j in df_train[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                           \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                           \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                           ignore_index=True)\n",
    "        else:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": str(), \n",
    "                                                           \"answer_start\": 0, \n",
    "                                                           \"answer_end\": 0}}, \n",
    "                                                           ignore_index=True)\n",
    "\n",
    "train_dataset = Dataset.from_dict(new_df_train)\n",
    "validation_dataset = Dataset.from_dict(new_df_val)\n",
    "\n",
    "data_qas_id = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "EuSjYelvXJ9x"
   },
   "outputs": [],
   "source": [
    "def rindex(lst, value, operator=operator):\n",
    "      return len(lst) - operator.indexOf(reversed(lst), value) - 1\n",
    "\n",
    "def preprocess_function_qa(examples, tokenizer, MAX_LENGTH=MAX_LENGTH, STRIDE=STRIDE, rindex=rindex, operator=operator):\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    examples[\"context\"] = [c.lstrip() for c in examples[\"context\"]]\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "      examples['question'],\n",
    "      examples['context'],\n",
    "      truncation=True,\n",
    "      max_length = MAX_LENGTH,\n",
    "      stride=STRIDE,\n",
    "      return_overflowing_tokens=True,\n",
    "      return_offsets_mapping=True,\n",
    "      padding=\"max_length\",\n",
    "      return_tensors='np'\n",
    "    )\n",
    "\n",
    "    tokenized_examples['start_positions'] = []\n",
    "    tokenized_examples['end_positions'] = []\n",
    "\n",
    "    for seq_idx in range(len(tokenized_examples['input_ids'])):\n",
    "        seq_ids = tokenized_examples.sequence_ids(seq_idx)\n",
    "        offset_mappings = tokenized_examples['offset_mapping'][seq_idx]\n",
    "\n",
    "        cur_example_idx = tokenized_examples['overflow_to_sample_mapping'][seq_idx]\n",
    "\n",
    "        #answer = examples['answer'][seq_idx][0]\n",
    "        answer = examples['answer'][cur_example_idx]\n",
    "        answer = eval(str(answer))\n",
    "        #answer_text = answer['text'][0]\n",
    "        answer_start = answer['answer_start']\n",
    "        #answer_end = answer_start + len(answer_text)\n",
    "        answer_end = answer['answer_end']\n",
    "\n",
    "        context_pos_start = seq_ids.index(1)\n",
    "        context_pos_end = rindex(seq_ids, 1, operator)\n",
    "\n",
    "        s = e = 0\n",
    "        if (offset_mappings[context_pos_start][0] <= answer_start and\n",
    "            offset_mappings[context_pos_end][1] >= answer_end):\n",
    "          i = context_pos_start\n",
    "          while offset_mappings[i][0] < answer_start:\n",
    "            i += 1\n",
    "          if offset_mappings[i][0] == answer_start:\n",
    "            s = i\n",
    "          else:\n",
    "            s = i - 1\n",
    "\n",
    "          j = context_pos_end\n",
    "          while offset_mappings[j][1] > answer_end:\n",
    "            j -= 1      \n",
    "          if offset_mappings[j][1] == answer_end:\n",
    "            e = j\n",
    "          else:\n",
    "            e = j + 1\n",
    "\n",
    "        tokenized_examples['start_positions'].append(s)\n",
    "        tokenized_examples['end_positions'].append(e)\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0o9vkQ1YaO2"
   },
   "source": [
    "# Mulai tokenisasi dan pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "2721b15477d741519a3041a8f256575b",
      "6a373f9cb48a481d85803f0792ff27b3",
      "54a2637e8c8e4ca0917e35f4b58b48f3",
      "7dc3dc97044c48afb06a14575fc8e91b",
      "6c3c7e4da5af40b088f3046d7698edff",
      "a040ab99cb964c32a0a968d97dd55a16",
      "9ef8c0b24cb5490b8fa7e8ea8ad619fe",
      "dce089e985634a67a381bc85cbaca017",
      "bd9cb96d5a3c4400bb7d145590540a54",
      "f940438113734ccdb198efd12b8a3936",
      "c5b567cd34b340dea77cefd40e9aaec9",
      "6516c5b02aa24db2bc42cdd4d1f2c260",
      "cbed771e77314789bf8817404e5d6f67",
      "b0be384a887847e981868df7c551d3de",
      "10fb50db270449b48382c815904245a8",
      "b23174d38e3940718252770bd39177f8",
      "7b80b681b46b4671a14a7cf82fd50991",
      "bce02c83fbc142c5bb2d8c1f905a478d",
      "12c3fd408f914038acb511c4ef57f965",
      "540c7fcbf3b14e538cba18329bc1fc20",
      "fc7073fb093543fc990abc620348c1ca",
      "acac4d071be4488fbb272df63b625213",
      "29b13cbb638947d5a17071d299e2420f",
      "113ae282ca5a499a8a2ad36a796e763f",
      "0ce874e529474f478a7acd45b4ffd649",
      "935752d8733c41fbb25ed436606352ca",
      "cf2b519cc5ff446dae45afa2439c840e",
      "f10632530a1b499ba3d20f0286446bab",
      "94da3d6dfccc4a289c659b94bcc90658",
      "902bd6c12ea54039b8c913a7c1782ff4",
      "1d86de7f1fbf452892fe6d4b177558f9",
      "3f6183fb8ca348e0ac35b495681c705a",
      "bdf2cb9d10cc4b2aab466a9dfe7be5bc",
      "c069967001a542f5b0c3b88ff1841eba",
      "f46e4c24b9294ffdb25e1e64bc594eee",
      "14a450c5594148c3a4ca340521d167ba",
      "9c9b27a0b2ac4be3b8dcbb65e23b84a2",
      "c99472b37d644d63a471b65ecbe3bfbd",
      "e9ca9f63982744aba53e883882373160",
      "4f80ae88d884407ab8f9e08aa58632a4",
      "13ea541bb13b42738e74885a2c1db8df",
      "68818d7eea5849afa82c3003a57a5b8d",
      "065d96b02c7840019af97e3c135693a8",
      "45f8bfb31705441c80a22dbb955dd933"
     ]
    },
    "id": "uVGfobd8XNIF",
    "outputId": "9b911f15-dce5-456f-a0fd-446f9615be1c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'fn_kwargs'={'tokenizer': BertTokenizerFast(name_or_path='afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05', vocab_size=31923, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), 'MAX_LENGTH': 400, 'STRIDE': 100, 'rindex': <function rindex at 0x7f11e685a700>, 'operator': <module 'operator' from '/usr/lib/python3.8/operator.py'>} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc05e521e4c4831bb9dbff022058033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3416b5cbd814fa2ad970edbf80f36c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data_qas_id = data_qas_id.map(\n",
    "    preprocess_function_qa,\n",
    "    batched=True,\n",
    "    remove_columns=data_qas_id['train'].column_names,\n",
    "    num_proc=1,\n",
    "    fn_kwargs={'tokenizer': tokenizer, 'MAX_LENGTH': MAX_LENGTH, 'STRIDE': STRIDE, 'rindex': rindex, 'operator': operator}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_qas_id = tokenized_data_qas_id.remove_columns([\"offset_mapping\", \n",
    "                                            \"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Fk1RbGEeldvi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_data_qas_id.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SLfcGWQEjd36"
   },
   "outputs": [],
   "source": [
    "tokenized_data_qas_id_train = Dataset.from_dict(tokenized_data_qas_id[\"train\"][:SAMPLE])\n",
    "tokenized_data_qas_id_validation = Dataset.from_dict(tokenized_data_qas_id[\"validation\"][:SAMPLE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06_Y5Xw9bLj3"
   },
   "source": [
    "# Mendefinisikan argumen (dataops) untuk training nanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "foTQgHuujf46"
   },
   "outputs": [],
   "source": [
    "TIME_NOW = str(datetime.now()).replace(\":\", \"-\").replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "QA = './results/alur2-idk-mrc'\n",
    "CHECKPOINT_DIR = f'{QA}-{TIME_NOW}/checkpoint/'\n",
    "MODEL_DIR = f'{QA}-{TIME_NOW}/model/'\n",
    "OUTPUT_DIR = f'{QA}-{TIME_NOW}/output/'\n",
    "ACCURACY_DIR = f'{QA}-{TIME_NOW}/accuracy/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9YdVKNRbPHk"
   },
   "source": [
    "# Mendefinisikan Training Arguments untuk train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3VMs4pBjgXE",
    "outputId": "577270d3-fe73-44be-f1c2-e710bb5e1d1b"
   },
   "outputs": [],
   "source": [
    "training_args_qa = TrainingArguments(\n",
    "        \n",
    "    # Checkpoint\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy='steps',\n",
    "    save_total_limit=EPOCH,\n",
    "\n",
    "    # Log\n",
    "    report_to='tensorboard',\n",
    "    logging_strategy='steps',\n",
    "    logging_first_step=True,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "\n",
    "    # Train\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    bf16=False,\n",
    "    dataloader_num_workers=cpu_count(),\n",
    "\n",
    "    # Miscellaneous\n",
    "    evaluation_strategy='steps',\n",
    "    save_steps=int((tokenized_data_qas_id_train.num_rows / (BATCH_SIZE * GRADIENT_ACCUMULATION)) * 0.5),\n",
    "    eval_steps=int((tokenized_data_qas_id_train.num_rows / (BATCH_SIZE * GRADIENT_ACCUMULATION)) * 0.5),\n",
    "    seed=SEED,\n",
    "    #hub_token=HUB_TOKEN,\n",
    "    #push_to_hub=True,\n",
    "    #hub_model_id=REPO_NAME,\n",
    "    #load_best_model_at_end=True,\n",
    "    #metric_for_best_model='f1',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INibCqT5bR3d"
   },
   "source": [
    "# Pendefinisian model Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqTEhdqqtL4z",
    "outputId": "5b4394f3-63f5-4610-e72d-d383d9b8940a"
   },
   "outputs": [],
   "source": [
    "model_qa = BertForQuestionAnswering.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qa = model_qa.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyJvhxZ6bbbF"
   },
   "source": [
    "# Melakukan pengumpulan data dengan padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "rGeJNonStSXT"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nF_E_CHbb8m"
   },
   "source": [
    "# Mulai training untuk fine-tune SQUAD diatas IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import collections\n",
    "\n",
    "# # Melakukan evaluasi dari prediksi\n",
    "def normalize_text(s):\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_f1_prec_rec(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split() # True positive + False positive = Untuk precision\n",
    "    gold_tokens = normalize_text(gold).split() # True positive + False negatives = Untuk recall\n",
    "    common = collections.Counter(pred_tokens) & collections.Counter(gold_tokens)\n",
    "    num_same = sum(common.values()) # True positive\n",
    "\n",
    "    if len(gold_tokens) == 0 or len(pred_tokens) == 0: \n",
    "        return int(gold_tokens == pred_tokens)\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(pred_tokens)\n",
    "    recall = 1.0 * num_same / len(gold_tokens)\n",
    "    f1 = (2.0 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1\n",
    "\n",
    "def compute_metrics(predict_result):\n",
    "    predictions_idx = np.argmax(predict_result.predictions, axis=2)\n",
    "    denominator = len(predictions_idx[0])\n",
    "    label_array = np.asarray(predict_result.label_ids)\n",
    "    total_correct = 0\n",
    "    f1_array = []\n",
    "    precision_array = []\n",
    "    recall_array = []\n",
    "\n",
    "    for i in range(len(predict_result.predictions[0])):\n",
    "        start_pred_idx = predictions_idx[0][i]\n",
    "        end_pred_idx = predictions_idx[1][i] + 1\n",
    "        start_gold_idx = label_array[0][i]\n",
    "        end_gold_idx = label_array[1][i] + 1\n",
    "\n",
    "        pred_text = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                    [start_pred_idx: end_pred_idx])\n",
    "        gold_text = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                    [start_gold_idx: end_gold_idx])\n",
    "\n",
    "        if pred_text == gold_text:\n",
    "            total_correct += 1\n",
    "\n",
    "        f1 = compute_f1_prec_rec(pred=pred_text, gold=gold_text)\n",
    "\n",
    "        f1_array.append(f1)\n",
    "\n",
    "    exact_match = ((total_correct / denominator) * 100.0)\n",
    "    final_f1 = np.mean(f1_array) * 100.0\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': final_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "G6pLmNzCjhra"
   },
   "outputs": [],
   "source": [
    "trainer_qa = Trainer(\n",
    "    model=model_qa,\n",
    "    args=training_args_qa,\n",
    "    train_dataset=tokenized_data_qas_id_train,\n",
    "    eval_dataset=tokenized_data_qas_id_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset idk_mrc (/root/.cache/huggingface/datasets/idk_mrc/idk_mrc_source/1.0.0/cf468d86fa7341e69998db1449851672ebfb4fa46036929d66b9de15c421334f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cad28d50a5540fe8afedeca4318c054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3659/3659 [00:22<00:00, 164.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 358/358 [00:01<00:00, 201.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 378/378 [00:01<00:00, 234.52it/s]\n"
     ]
    }
   ],
   "source": [
    "conhelps = NusantaraConfigHelper()\n",
    "data_qas_id = conhelps.filtered(lambda x: 'idk_mrc' in x.dataset_name)[0].load_dataset()\n",
    "\n",
    "df_train = pd.DataFrame(data_qas_id['train'])\n",
    "df_validation = pd.DataFrame(data_qas_id['validation'])\n",
    "df_test = pd.DataFrame(data_qas_id['test'])\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_train = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_train['context']))):\n",
    "    for j in df_train[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                           \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                           \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                           ignore_index=True)\n",
    "        else:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": str(), \n",
    "                                                           \"answer_start\": 0, \n",
    "                                                           \"answer_end\": 0}}, \n",
    "                                                           ignore_index=True)\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_val = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_validation['context']))):\n",
    "    for j in df_validation[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                       \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                       \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                       ignore_index=True)\n",
    "        else:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": str(), \n",
    "                                                       \"answer_start\": 0, \n",
    "                                                       \"answer_end\": 0}}, \n",
    "                                                       ignore_index=True)        \n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_test = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_test['context']))):\n",
    "    for j in df_test[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_test = new_df_test.append({'context': df_test[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                       \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                       \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                       ignore_index=True)\n",
    "        else:\n",
    "            new_df_test = new_df_test.append({'context': df_test[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": str(), \n",
    "                                                       \"answer_start\": 0, \n",
    "                                                       \"answer_end\": 0}}, \n",
    "                                                       ignore_index=True)\n",
    "\n",
    "train_dataset = Dataset.from_dict(new_df_train)\n",
    "validation_dataset = Dataset.from_dict(new_df_val)\n",
    "test_dataset = Dataset.from_dict(new_df_test)\n",
    "\n",
    "data_qas_id = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 9332\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 764\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 844\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_qas_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qp96lB8vjtXG",
    "outputId": "b118d7ac-a90d-43c2-bd8b-ee0a0c4ab8de",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 05:54, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.185500</td>\n",
       "      <td>1.190539</td>\n",
       "      <td>46.465969</td>\n",
       "      <td>50.663675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.305800</td>\n",
       "      <td>1.115683</td>\n",
       "      <td>49.607330</td>\n",
       "      <td>53.467761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=147, training_loss=1.584446754585318, metrics={'train_runtime': 365.0544, 'train_samples_per_second': 25.84, 'train_steps_per_second': 0.403, 'total_flos': 1920531162009600.0, 'train_loss': 1.584446754585318, 'epoch': 1.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_qa.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DreIeglDbrHY"
   },
   "source": [
    "# Menyimpan model Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "KJick7YDjuoq"
   },
   "outputs": [],
   "source": [
    "trainer_qa.save_model(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX-y8EbobugF"
   },
   "source": [
    "# Melakukan prediksi dari model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "6aE7w2yMkWxj",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=(array([[-6.435914  , -6.9685993 , -6.298404  , ..., -9.324049  ,\n",
       "        -9.257196  , -9.116114  ],\n",
       "       [-4.8937144 , -4.2243876 , -1.7159504 , ..., -8.833822  ,\n",
       "        -8.811077  , -8.838762  ],\n",
       "       [-4.718739  , -4.789043  , -0.86703014, ..., -8.805774  ,\n",
       "        -8.817287  , -8.819923  ],\n",
       "       ...,\n",
       "       [-3.012759  , -4.7193727 , -7.117803  , ..., -7.823757  ,\n",
       "        -7.709506  , -7.9639964 ],\n",
       "       [-3.542631  , -4.5105743 , -7.0690026 , ..., -6.9549575 ,\n",
       "        -6.9480996 , -7.521445  ],\n",
       "       [-3.6498957 , -4.6104627 , -7.0684705 , ..., -6.96615   ,\n",
       "        -6.959043  , -7.6362724 ]], dtype=float32), array([[-4.8501034 , -6.4872174 , -7.496827  , ..., -7.904192  ,\n",
       "        -7.8175154 , -7.897848  ],\n",
       "       [-2.5316944 , -3.6562064 , -3.9014554 , ..., -7.812762  ,\n",
       "        -7.800026  , -7.813285  ],\n",
       "       [-2.9194224 , -3.3534932 , -3.4802475 , ..., -7.994805  ,\n",
       "        -7.988219  , -8.050567  ],\n",
       "       ...,\n",
       "       [-0.39873138, -4.569075  , -6.403874  , ..., -7.895459  ,\n",
       "        -7.6578655 , -7.8957205 ],\n",
       "       [-1.2242731 , -4.634083  , -6.0397263 , ..., -5.20852   ,\n",
       "        -5.2012515 , -6.5427775 ],\n",
       "       [-1.4033347 , -4.7205286 , -6.078583  , ..., -5.2300215 ,\n",
       "        -5.222459  , -6.598669  ]], dtype=float32)), label_ids=(array([ 19,  71,  13,  25,  10,  15,  14,  48,   9,  29,   6,  12,   8,\n",
       "        13,   9,  10,  12,  11,  31,  13,  14,  10,  15,  12,  13,  12,\n",
       "        26,   9,  26,  21,  10,  27,  10,  37,  10,  20,   8,  27,  15,\n",
       "        24,   7,  51,   8,  54,   8, 106,  15,  15,  30,   8,  24,  29,\n",
       "        11,  14,   8,  77,  47,  15,  11,  16,  11,  16,  51,  12,  20,\n",
       "        28,  11,  26,  11,  37,   8,  80,   9,   9,   8,  31,  10,  48,\n",
       "        12,  40,  58,   7,  76,   9,   8,   8,   8,  82,  10,  72,   9,\n",
       "        33,  10,  10,  10,  17,  21,  11,  10,  35,   8,  12,   7,  47,\n",
       "        10,  12,   8,  12,  40,  13,  31,  13, 136,  13,   9,  16,  43,\n",
       "         9,  25,  10,  12,  10,  14,  40,  10,  47,  20,  45,   9,  29,\n",
       "        14,  10,  12,  38,   9,  38,   9, 133,  13, 134,  13,  57,   7,\n",
       "        18,  19,  12,  11, 183,  15,  12,  13,  11,   9,  55,  13,  14,\n",
       "        86,   8,   9,   9,  17,  16,  14,   9,  24,  21,   9,  61,  12,\n",
       "        20,   8,   7,   7,   8,  17,   8,   8,   9, 364,   9,  60,  13,\n",
       "        11,  11,  13,  22,  11,  20,  15,  92,  12,  79,  12,  81,   9,\n",
       "         9,   8,   8,   7,  12,  14,  16,  10,  31,  10,   8,  11,  16,\n",
       "        11,  65,  10,  11,  11, 155,  11,  13,  16,  27,  11,  11,  23,\n",
       "        10,  15,  12,   8,  12,  10,   7,  31,   9,  15,   7,  13,   9,\n",
       "       142,   9, 136,  10,   9,  12,   9,  29,   6,   7,   6,   7,  37,\n",
       "        10,  20,   9, 127,   8,   9,   9,   8,  10,   9,   9,  11,  12,\n",
       "         9,  13,  38,   9,  11,  12,  11,  23, 186,  15,   9,  36,   7,\n",
       "        31,  25,  15,  35,  10,  24,  14,  35,   7,  57,   8,  96,   8,\n",
       "        95,   7,  10,  11,  10,  15,   9,   7,  10,   8,  24,  11,  24,\n",
       "         8,  20,  10,   9,  12,  10,  13,   8,  31,   9,  51,  24,   8,\n",
       "         9,  13,   9,  22,  10,  41,  10, 106,  21,   7,   7,  17,   9,\n",
       "         7,   7,  68,  10,  28,  11,  76,  12,  96,  10,  11,  14, 150,\n",
       "        13,  35,  10,  74,  15,  20,  10, 110,  14,  92,  11,  30,  10,\n",
       "       108, 146,  10,  14,  25,  10,  31,  15,  14,  11,  14,  12,  11,\n",
       "        23,  57,   8,  50,  13,  46,  10,  12,   7,  13,  10,  10,   9,\n",
       "         9,   9,   8,   8,  26,   9,  91,  11, 112,  14,  13,  10,   9,\n",
       "         5,   9,  12,   8,  41,  38,   7,  21,   6,  44,   7, 125,  11,\n",
       "        37,  12,   9,  16,  10,  21,  10,  10,  38,   9,  13,   8,  25,\n",
       "        10,  30,  10,  45,  10,  36,   8,  11,  16,  18,  11,   9,  12,\n",
       "        71,  13,  47,  10,   9,   9,  12,  10,  53,   8,  18,   7,  26,\n",
       "        10,  41,  10,  12, 106,  12,  34,  13,  13,  10,  46,  15,  44,\n",
       "         8,   9,  12,  76,   8, 123,   6,  19,  10,  10,  60,   9,  42,\n",
       "         7,   8,   9,  31,   9,  11, 220,  11,  10,  13, 164,  10,  26,\n",
       "        11,  48,  11,  11,   7,  10,  85,   9,  18,  11,  82,   8,   8,\n",
       "         8,  32,  15,  17,  10,   8,   8,  51,   9,  52,  10,   9,   9,\n",
       "        44,  12,  10,  39,  13,  52,  11,  13,  21,  20,   9,  10,   6,\n",
       "       107,  13,  37,   7,  18,   7,  13,  13,  48,   7,  39,  11,  13,\n",
       "         7,   8,  27,  17,  39,  11,  13,   9,  12,   7,  18,   9,   7,\n",
       "        10,  10,  15,  12,  22,  13,  12,  43,   8,  11,  21,  43,  44,\n",
       "        18,  14,  26,   8,  22,  12,  11,   9,  11,  30,   9,  73,  14,\n",
       "        14,  12,  15,  17,  18,   9,  39,  12,  13,  11,   9,  12,   8,\n",
       "        10,  39,  10,  48, 178,  11,  47,   9,  14,   8,  38,  25,   9,\n",
       "         8,  42,  56,   9,  20,   9,  57,  12,  21,  22,   8,   9,  21,\n",
       "        11,  46,  10,  12, 101,  12,  14,  21,  11,   9,  78,  11,  13,\n",
       "        35,  15,  29,  11,  13,  15,  15,  11,   8,   8,  36,   8,  19,\n",
       "        12,  14,  21,  12,   9,  14,  78,  19,  13,   9,  37,  11,   9,\n",
       "        43,  21,  12,  17,  10,  49,  38,   9,  11,  11,   8,  53,  24,\n",
       "        12,  35,   7,  15,  15,   8, 222,  14,  20,  24,  10,  10, 125,\n",
       "        52,  14,  25,  13,  10,  29,  24,  28,   7, 110,  10,  89,  10,\n",
       "        14,  16,  23,   6,  38,  17,  71,  12,  10,  11,  14,  19,  11,\n",
       "       223,  26,  65,   9,  72,  44,  17,  10,  31,   9,  33,  10,  33,\n",
       "        19,  11,  42,  91,  11,  26,   9,  30,   7,  13, 151, 174,  11,\n",
       "        18,  15,  12,   7,  50,   8,  27,  12,  56,  11,  58,  18,  59,\n",
       "       222, 223,  11,  42,  77,   9,  30,   8,  76,  83,  47,  88,  11,\n",
       "        35,  34,  10,  45,   7,  18,   9,  11,  16,  15]), array([ 22,  73,  12,  32,   9,  15,  13,  78,   8,  29,   5,  52,   7,\n",
       "        14,   8,   9,  23,  10,  33,  12,  16,   9,  28,  11,  21,  11,\n",
       "        36,   8,  28,  22,   9,  29,   9,  38,   9,  20,   7,  54,  14,\n",
       "        26,   6,  76,   7,  61,   7, 106,  14,  14,  31,   7,  48,  29,\n",
       "        10,  17,   7,  78,  48,  14,  10,  21,  10,  33,  53,  11,  24,\n",
       "        34,  10,  27,  10,  40,   7,  81,   8,   9,   7,  34,   9,  59,\n",
       "        11,  53,  60,   6,  79,   8,   7,   7,   7,  88,   9, 103,   8,\n",
       "        35,   9,   9,   9,  34,  32,  10,   9,  41,   7,  12,   6,  60,\n",
       "         9,  15,   7,  14,  41,  12,  70,  12, 137,  12,   8,  15,  56,\n",
       "         8,  32,   9,  12,   9,  13,  50,   9,  47,  19,  46,   8,  29,\n",
       "        13,   9,  11,  39,   8,  39,   8, 133,  12, 138,  12,  81,   6,\n",
       "        20,  18,  19,  10, 187,  14,  11,  12,  11,   8,  55,  12,  13,\n",
       "        86,   7,  11,   8,  19,  15,  28,   8,  34,  21,   8,  61,  11,\n",
       "        20,   7,  16,   6,   8,  18,   7,   7,   8, 366,   8,  63,  12,\n",
       "        19,  10,  12,  56,  10,  20,  14, 111,  11,  83,  11,  83,   8,\n",
       "        12,   7,   9,   6,  13,  13,  18,   9,  31,   9,   9,  10,  18,\n",
       "        10,  65,   9,  10,  10, 156,  10,  12,  15,  99,  10,  10,  30,\n",
       "         9,  51,  11,   7,  11,  19,   6,  32,   8,  20,   6,  33,   8,\n",
       "       144,   8, 136,   9,   8,  11,   8,  31,   5,   6,   5,   6,  37,\n",
       "         9,  24,   8, 132,   7,  20,   8,  19,  11,   8,   8,  10,  21,\n",
       "         8,  12,  39,   8,  14,  11,  10,  23, 196,  34,   8,  41,   6,\n",
       "        32,  26,  14,  42,   9,  24,  13,  59,   6,  65,   7,  97,   7,\n",
       "        96,   6,  16,  10,  40,  14,   8,   6,  55,   7,  26,  10,  25,\n",
       "         7,  21,   9,   8,  14,   9,  30,   7,  32,   8,  51,  24,   7,\n",
       "         8,  15,   8,  22,   9,  59,   9, 122,  20,   8,   6,  19,   8,\n",
       "        10,   6,  70,   9,  30,  10,  90,  11,  99,   9,  10,  13, 152,\n",
       "        12,  45,   9,  84,  14,  20,   9, 112,  13,  92,  10,  36,   9,\n",
       "       110, 161,   9,  13,  38,   9,  49,  16,  13,  10,  31,  11,  10,\n",
       "        23,  59,   7,  50,  12,  50,   9,  14,   6,  18,   9,  37,   8,\n",
       "        11,   8,   9,   7,  28,   8,  91,  10, 114,  13,  12,   9,  22,\n",
       "         4,  18,  11,   7,  44,  43,   6,  23,   5,  48,   6, 129,  10,\n",
       "        41,  12,   8,  27,   9,  25,   9,   9,  38,   8,  27,   7,  27,\n",
       "         9,  38,   9,  47,   9,  36,   7,  15,  15,  29,  32,   8,  13,\n",
       "        74,  12,  47,   9,  15,   8,  41,   9,  58,   7,  32,   6,  29,\n",
       "         9,  61,   9,  22, 109,  11,  46,  12,  55,   9,  47,  14,  64,\n",
       "         7,   8,  11,  84,   7, 128,   5,  21,   9,  15,  60,   8,  44,\n",
       "         6,  10,   8,  31,   8,  10, 242,  10,   9,  12, 169,   9,  27,\n",
       "        10,  72,  10,  76,   6,   9,  86,   8,  18,  10,  82,   7,  17,\n",
       "         7,  33,  14,  17,   9,   7,   7,  52,   8,  53,   9,  10,   8,\n",
       "        51,  11,   9,  70,  12,  54,  10,  38,  24,  19,   8,  16,   5,\n",
       "       108,  12,  53,   6,  22,   6,  14,  12,  51,   6,  41,  10,  14,\n",
       "         6,   7,  39,  16,  46,  10,  17,   8,  36,   6,  40,   8,   6,\n",
       "        12,   9,  17,  11,  23,  14,  11,  49,   7,  10,  25,  48,  49,\n",
       "        22,  13,  28,   7,  30,  11,  13,   8,  10,  38,   8,  76,  15,\n",
       "        15,  11,  14,  47,  48,   8,  46,  32,  33,  31,   8,  11,   7,\n",
       "         9,  41,   9,  58, 202,  10,  48,   8,  14,   7,  42,  31,   8,\n",
       "        10,  46,  59,   8,  21,   8,  62,  11,  24,  25,   7,   8,  27,\n",
       "        10,  49,   9,  11, 105,  11,  13,  24,  10,   8,  86,  10,  12,\n",
       "        38,  14,  38,  10,  12,  21,  20,  19,  16,   7,  37,   7,  19,\n",
       "        11,  13,  22,  28,   8,  13,  83,  18,  16,   8,  38,  12,   8,\n",
       "        65,  22,  11,  16,   9,  58,  39,  12,  11,  10,   9,  55,  25,\n",
       "        11,  39,   6,  27,  37,  11, 223,  13,  20,  31,   9,   9, 125,\n",
       "        59,  13,  25,  28,   9,  30,  32,  56,   6, 113,   9,  93,   9,\n",
       "        13,  18,  25,   5,  43,  24,  83,  11,  11,  10,  13,  36,  10,\n",
       "       232,  39,  77,   8,  74,  53,  24,   9,  34,   8,  36,   9,  56,\n",
       "        19,  10,  42,  92,  10,  36,   8,  39,   6,  15, 155, 178,  10,\n",
       "        25,  14,  18,   6,  52,   7,  43,  11,  60,  10,  58,  20,  61,\n",
       "       225, 224,  10,  44,  77,   8,  35,   7,  89,  85,  49,  91,  10,\n",
       "        37,  36,   9,  50,   6,  20,  13,  10,  23,  14])), metrics={'test_loss': 1.115529179573059, 'test_exact_match': 49.60732984293193, 'test_f1': 53.46776111438457, 'test_runtime': 15.1879, 'test_samples_per_second': 50.303, 'test_steps_per_second': 3.16})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result = trainer_qa.predict(tokenized_data_qas_id_validation)\n",
    "predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "with open(f'{OUTPUT_DIR}/output.txt', \"w\") as f:\n",
    "  f.write(str(predict_result))\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2BS4XC5byxE"
   },
   "source": [
    "# Melakukan evaluasi dari prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "9J-zKl_zkUze"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 49.60732984293193, 'f1': 53.46776111438457}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_result_before_filtering = compute_metrics(predict_result)\n",
    "metric_result_before_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(ACCURACY_DIR), exist_ok=True)\n",
    "with open(f'{ACCURACY_DIR}/accuracy.txt', \"w\") as f:\n",
    "  f.write(str(metric_result_before_filtering))\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coba Alur 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_qas_dataframe(predict_result=predict_result, index_largest=1):\n",
    "    predictions_idx = np.argsort(predict_result.predictions, axis=2)[:, :, index_largest * -1]\n",
    "    #predictions_idx = np.argmax(predict_result.predictions, axis=2)\n",
    "    label_array = np.asarray(predict_result.label_ids)\n",
    "    question_decoded = []\n",
    "    context_decoded = []\n",
    "    pred_answer_decoded = []\n",
    "    gold_answer_decoded = []\n",
    "    \n",
    "    for i in tqdm(range(len(predict_result.predictions[0]))):\n",
    "        start_pred_idx = predictions_idx[0][i]\n",
    "        end_pred_idx = predictions_idx[1][i] + 1\n",
    "        pred_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_pred_idx: end_pred_idx], skip_special_tokens=True)\n",
    "        pred_answer_decoded.append(pred_answer)\n",
    "        \n",
    "        start_gold_idx = label_array[0][i]\n",
    "        end_gold_idx = label_array[1][i] + 1\n",
    "        gold_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_gold_idx: end_gold_idx], skip_special_tokens=True)\n",
    "        gold_answer_decoded.append(gold_answer)\n",
    "        \n",
    "        question = []\n",
    "        context = []\n",
    "         \n",
    "        for j in range(len(tokenized_data_qas_id_validation[i]['token_type_ids'])):\n",
    "            if tokenized_data_qas_id_validation[i]['token_type_ids'][j] == 0:\n",
    "                question.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "            else:\n",
    "                context.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "\n",
    "        question_decoded.append(tokenizer.decode(question, skip_special_tokens=True))\n",
    "        context_decoded.append(tokenizer.decode(context, skip_special_tokens=True))\n",
    "    \n",
    "    qas_df = pd.DataFrame({'Context': context_decoded, \n",
    "                           'Question': question_decoded, \n",
    "                           'Prediction Answer': pred_answer_decoded,\n",
    "                          'Gold Answer': gold_answer_decoded})\n",
    "                      \n",
    "    return qas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 764/764 [08:43<00:00,  1.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Question</th>\n",
       "      <th>Prediction Answer</th>\n",
       "      <th>Gold Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sistem pemosisi global [ 1 ] ( bahasa inggris ...</td>\n",
       "      <td>apa kepanjangan dari gps?</td>\n",
       "      <td></td>\n",
       "      <td>global positioning system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ukuran reptil bervariasi, dari yang berukuran ...</td>\n",
       "      <td>apakah cabang ilmu pengetahuan alam yang mempe...</td>\n",
       "      <td>herpetologi</td>\n",
       "      <td>herpetologi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ukuran reptil bervariasi, dari yang berukuran ...</td>\n",
       "      <td>apa cabang ilmu pengetahuan alam yang tidak me...</td>\n",
       "      <td>herpetologi</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reptil ( binatang melata, atau dalam bahasa la...</td>\n",
       "      <td>apakah maksud reptil dalam bahasa latin?</td>\n",
       "      <td>kelompok hewan vertebrata berdarah dingin dan ...</td>\n",
       "      <td>' melata'atau'merayap '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reptil ( binatang melata, atau dalam bahasa la...</td>\n",
       "      <td>apakah maksud reptil ganas dalam bahasa latin?</td>\n",
       "      <td>reptil ( binatang melata, atau dalam bahasa la...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>realme meluncurkan smartphone pertamanya \" rea...</td>\n",
       "      <td>apakah smartphone pertama yang diproduksi realme?</td>\n",
       "      <td></td>\n",
       "      <td>realme 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>thariq bin ziyad lebih banyak dikenal sebagai ...</td>\n",
       "      <td>siapa yang dikenal sebagai penakluk spanyol?</td>\n",
       "      <td>thariq bin ziyad</td>\n",
       "      <td>thariq bin ziyad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>thariq bin ziyad lebih banyak dikenal sebagai ...</td>\n",
       "      <td>siapa yang dikenal sebagai penakluk spanyol pa...</td>\n",
       "      <td>thariq bin ziyad</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>adalah angkatan laut kekaisaran jepang ( kaigu...</td>\n",
       "      <td>siapa yang menggunakan kapal induk secara efek...</td>\n",
       "      <td></td>\n",
       "      <td>angkatan laut kekaisaran jepang ( kaigun )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>adalah angkatan laut kekaisaran jepang ( kaigu...</td>\n",
       "      <td>siapa yang menggunakan kapal induk secara efek...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>764 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Context  \\\n",
       "0    sistem pemosisi global [ 1 ] ( bahasa inggris ...   \n",
       "1    ukuran reptil bervariasi, dari yang berukuran ...   \n",
       "2    ukuran reptil bervariasi, dari yang berukuran ...   \n",
       "3    reptil ( binatang melata, atau dalam bahasa la...   \n",
       "4    reptil ( binatang melata, atau dalam bahasa la...   \n",
       "..                                                 ...   \n",
       "759  realme meluncurkan smartphone pertamanya \" rea...   \n",
       "760  thariq bin ziyad lebih banyak dikenal sebagai ...   \n",
       "761  thariq bin ziyad lebih banyak dikenal sebagai ...   \n",
       "762  adalah angkatan laut kekaisaran jepang ( kaigu...   \n",
       "763  adalah angkatan laut kekaisaran jepang ( kaigu...   \n",
       "\n",
       "                                              Question  \\\n",
       "0                            apa kepanjangan dari gps?   \n",
       "1    apakah cabang ilmu pengetahuan alam yang mempe...   \n",
       "2    apa cabang ilmu pengetahuan alam yang tidak me...   \n",
       "3             apakah maksud reptil dalam bahasa latin?   \n",
       "4       apakah maksud reptil ganas dalam bahasa latin?   \n",
       "..                                                 ...   \n",
       "759  apakah smartphone pertama yang diproduksi realme?   \n",
       "760       siapa yang dikenal sebagai penakluk spanyol?   \n",
       "761  siapa yang dikenal sebagai penakluk spanyol pa...   \n",
       "762  siapa yang menggunakan kapal induk secara efek...   \n",
       "763  siapa yang menggunakan kapal induk secara efek...   \n",
       "\n",
       "                                     Prediction Answer  \\\n",
       "0                                                        \n",
       "1                                          herpetologi   \n",
       "2                                          herpetologi   \n",
       "3    kelompok hewan vertebrata berdarah dingin dan ...   \n",
       "4    reptil ( binatang melata, atau dalam bahasa la...   \n",
       "..                                                 ...   \n",
       "759                                                      \n",
       "760                                   thariq bin ziyad   \n",
       "761                                   thariq bin ziyad   \n",
       "762                                                      \n",
       "763                                                      \n",
       "\n",
       "                                    Gold Answer  \n",
       "0                     global positioning system  \n",
       "1                                   herpetologi  \n",
       "2                                                \n",
       "3                       ' melata'atau'merayap '  \n",
       "4                                                \n",
       "..                                          ...  \n",
       "759                                    realme 1  \n",
       "760                            thariq bin ziyad  \n",
       "761                                              \n",
       "762  angkatan laut kekaisaran jepang ( kaigun )  \n",
       "763                                              \n",
       "\n",
       "[764 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qas_df = create_qas_dataframe(predict_result)\n",
    "qas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_mark = ['siapa', 'siapakah',\n",
    "                    'apa', 'apakah', 'adakah',\n",
    "                    'dimana', 'dimanakah', 'darimanakah',\n",
    "                    'kapan', 'kapankah',\n",
    "                    'bagaimana', 'bagaimanakah',\n",
    "                    'kenapa', 'mengapa',\n",
    "                    'berapa', 'berapakah', 'seberapa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mencoba cara retrieve model dari HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72832c13c0054609b1027ef7d29cd3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/942 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d660be63514417b77725037ed3318e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265859863a734e228f54901e60ea2aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a0855dc07740f8b6285d4663dd84e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/234k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3221de9ecdb0410888b4a5b489afbccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/737k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5506470bde2e48a99e822e105484c77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pretrained_name_sc = \"afaji/fine-tuned-IndoNLI-Augmented-with-indobert-base-uncased\"\n",
    "pretrained_name_qa = \"afaji/fine-tuned-DatasetQAS-TYDI-QA-ID-with-indobert-base-uncased-with-ITTL-without-freeze-LR-1e-05\"\n",
    "tokenizer_kwargs = {'padding': True, 'truncation': True, 'max_length': MAX_LENGTH}\n",
    "\n",
    "nlp_sc = pipeline(task=\"text-classification\", model=pretrained_name_sc, tokenizer=pretrained_name_sc, \n",
    "                   **tokenizer_kwargs)\n",
    "nlp_qa = pipeline(task=\"question-answering\", model=pretrained_name_qa, tokenizer=pretrained_name_qa, \n",
    "                   **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "pretrained_name_ind_tg = \"Wikidepia/IndoT5-base-paraphrase\"\n",
    "pretrained_name_eng_tg = \"humarin/chatgpt_paraphraser_on_T5_base\"\n",
    "\n",
    "tokenizer_kwargs = {'truncation': True, 'max_length': MAX_LENGTH}\n",
    "\n",
    "nlp_tg = pipeline(task=\"text2text-generation\", model=pretrained_name_ind_tg, tokenizer=pretrained_name_ind_tg, **tokenizer_kwargs)\n",
    "nlp_tg_eng = pipeline(task=\"text2text-generation\", model=pretrained_name_eng_tg, tokenizer=pretrained_name_eng_tg, **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coba buat method evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context\n",
      "Question\n",
      "Prediction Answer\n",
      "Gold Answer\n"
     ]
    }
   ],
   "source": [
    "def question_type(data):\n",
    "    num_apa = pd.DataFrame()\n",
    "    num_dimana = pd.DataFrame()\n",
    "    num_kapan = pd.DataFrame()\n",
    "    num_siapa = pd.DataFrame()\n",
    "    num_bagaimana = pd.DataFrame()\n",
    "    num_kenapa = pd.DataFrame()\n",
    "    num_berapa = pd.DataFrame()\n",
    "    num_others = pd.DataFrame()\n",
    "    \n",
    "    for i in data['question']:\n",
    "        current_question = i.split()\n",
    "        \n",
    "        if 'Apa' in current_question: num_apa += 1\n",
    "        elif 'Apakah' in current_question: num_apa += 1\n",
    "        elif 'apa' in current_question: num_apa += 1\n",
    "        elif 'apakah' in current_question: num_apa += 1\n",
    "        \n",
    "        elif 'Dimana' in current_question: num_dimana += 1\n",
    "        elif 'dimana' in current_question: num_dimana += 1\n",
    "        elif 'mana' in current_question: num_dimana += 1\n",
    "        \n",
    "        elif 'Kapan' in current_question: num_kapan += 1\n",
    "        elif 'kapan' in current_question: num_kapan += 1\n",
    "        \n",
    "        elif 'Siapa' in current_question: num_siapa += 1\n",
    "        elif 'siapa' in current_question: num_siapa += 1\n",
    "        \n",
    "        elif 'Bagaimana' in current_question: num_bagaimana += 1\n",
    "        elif 'bagaimana' in current_question: num_bagaimana += 1\n",
    "\n",
    "        elif 'Mengapa' in current_question: num_kenapa += 1\n",
    "        elif 'Kenapa' in current_question: num_kenapa += 1\n",
    "        elif 'mengapa' in current_question: num_kenapa += 1\n",
    "        elif 'kenapa' in current_question: num_kenapa += 1\n",
    "        \n",
    "        elif 'Berapa' in current_question: num_berapa += 1\n",
    "        elif 'Berapakah' in current_question: num_berapa += 1\n",
    "        elif 'berapa' in current_question: num_berapa += 1\n",
    "        elif 'berapakah' in current_question: num_berapa += 1\n",
    "        \n",
    "        else: num_others += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: hari bastille adalah nama lain untuk hari nasional perancis yang dirayakan tanggal 14 juli setiap tahunnya. di perancis, nama resminya adalah la fete nationale ( perayaan nasional ) dan umumnya le quatorze juillet ( empat belas juli ). hari ini merayakan fete de la federation 1790 yang diadakan pada ulang tahun pertama penyerbuan bastille tanggal 14 juli 1789 ; ulang tahun penyerbuan benteng penjara bastille dipandang sebagai simbol pemberontakan bangsa yang modern ini, dan rekonsiliasi seluruh rakyat perancis di dalam kekuasaan monarki konstitusional sebelum republik pertama. pesta dan upacara resmi diselenggarakan di seluruh perancis. parade militer rutin tertua dan terbesar di eropa diadakan pada pagi 14 juli, di jalan champs - elysees, paris, di hadapan presiden republik, pejabat pemerintahan perancis, dan perwakilan asing. [ 1 ] [ 2 ]\n",
      "Question: bagaimana hari kemerdekaan prancis dirayakan?\n",
      "Answer: \n",
      "Hypothesis: hari kemerdekaan prancis dirayakan adalah \n",
      "\n",
      "Context: musik gospel diduga pertama kali muncul dari gereja - gereja afrika - amerika dalam kuartal pertama dari abad ke - 20, atau dalam pengertian yang lebih luas, merujuk kepada musik gospel kulit hitam maupun musik religius yang diciptakan dan dinyanyikan oleh para artis gospel selatan kulit putih pada umumnya. perbedaan yang tajam antara amerika yang berkulit hitam dan putih, khususnya antara gereja - gereja warga kulit hitam dan putih, menyebabkan kedua golongan ini terpisah. meskipun perbedaannya telah berkurang sedikit selama lima puluh tahun terakhir, kedua tradisi ini masih berbeda. dalam kedua tradisi ini, sejumlah penyanyi, seperti misalnya mahalia jackson membatasi dirinya untuk tampil hanya dalam konteks - konteks keagamaan saja, sementar ayang lainnya seperti sister rosetta tharpe, golden gate quartet dan clara ward, juga membawakan musik gospel dalam lingkup sekuler, bahkan juga di klab - klab malam. banyak penyanyi, seperti misalnya the jordanaires, the blackwood brothers, al green, dan solomon burke pernah tampil membawakan musik sekuler dan maupun keagamaan. penyanyi - penyanyi seperti ini biasa menyisipkan lagu - lagu gospel dalam pertunjukan - pertunjukan sekuler mereka, meskipun yang kebalikannya hampir tak pernah terjadi.\n",
      "Question: bagaimana musik gospel muncul?\n",
      "Answer: \n",
      "Hypothesis: musik gospel muncul adalah \n",
      "\n",
      "Context: fungi melakukan reproduksi secara aseksual dan seksual. reproduksi secara aseksual terjadi dengan pembentukan kuncup atau tunas pada jamur uniseluler serta pemutusan benang hifa ( fragmentasi miselium ) dan pembentukan spora aseksual ( spora vegetatif ) pada fungi multiseluler. reproduksi jamur secara seksual dilakukan oleh spora seksual. spora seksual dihasilkan secara singami. singgami terdiri dari dua tahap, yaitu tahap plasmogami dan tahap kariogami.\n",
      "Question: bagaimana proses pembentukan jamur?\n",
      "Answer: melakukan reproduksi secara aseksual dan seksual\n",
      "Hypothesis: proses pembentukan jamur adalah melakukan reproduksi secara aseksual dan seksual\n",
      "\n",
      "Context: fungi melakukan reproduksi secara aseksual dan seksual. reproduksi secara aseksual terjadi dengan pembentukan kuncup atau tunas pada jamur uniseluler serta pemutusan benang hifa ( fragmentasi miselium ) dan pembentukan spora aseksual ( spora vegetatif ) pada fungi multiseluler. reproduksi jamur secara seksual dilakukan oleh spora seksual. spora seksual dihasilkan secara singami. singgami terdiri dari dua tahap, yaitu tahap plasmogami dan tahap kariogami.\n",
      "Question: bagaimana proses pembentukan jamur yang ditutup?\n",
      "Answer: \n",
      "Hypothesis: proses pembentukan jamur yang ditutup adalah \n",
      "\n",
      "Context: pemerintahan jepang adalah sebuah monarki konstitusional yang di dalamnya terdapat kuasa dari seorang kaisar yang masih dibatasi dan hanya diturunkan terutama ketika melakukan tugas resmi. seperti di negara - negara lainnya, pemerintahan dipecah menjadi tiga cabang : eksekutif, legislatif dan yudikatif.\n",
      "Question: bagaimana sistem pemerintahan jepang?\n",
      "Answer: monarki konstitusional yang di dalamnya terdapat kuasa dari seorang kaisar yang masih dibatasi dan hanya diturunkan terutama ketika melakukan tugas resmi\n",
      "Hypothesis: sistem pemerintahan jepang adalah monarki konstitusional yang di dalamnya terdapat kuasa dari seorang kaisar yang masih dibatasi dan hanya diturunkan terutama ketika melakukan tugas resmi\n",
      "\n",
      "Context: pemerintahan jepang adalah sebuah monarki konstitusional yang di dalamnya terdapat kuasa dari seorang kaisar yang masih dibatasi dan hanya diturunkan terutama ketika melakukan tugas resmi. seperti di negara - negara lainnya, pemerintahan dipecah menjadi tiga cabang : eksekutif, legislatif dan yudikatif.\n",
      "Question: bagaimana sistem pemerintahan jepang pada masa penjajahan?\n",
      "Answer: \n",
      "Hypothesis: sistem pemerintahan jepang pada masa penjajahan adalah \n",
      "\n",
      "Context: porifera bereproduksi secara seksual atau aseksual, spesies yang bereproduksi secara seksual mengeluarkan sel sperma ke air dan bertemu sel telur ( ada yang dilepas ke air dan ada yang tetap di tubuh \" ibu \" ). telur yang berfertilisasi berenang mencari tempat untuk menempel, dan tumbuh menjadi individu baru. spons dapat beregenerasi dari bagian yang terpotong, apabila bagian tersebut punya sel yang tepat. sedangkan yang bereproduksi secara aseksual menggunakan tiga cara, yaitu : tunas, gemula, dan fragmentasi.\n",
      "Question: bagaimana porifera berkembangbiak?\n",
      "Answer: seksual atau aseksual\n",
      "Hypothesis: porifera berkembangbiak adalah seksual atau aseksual\n",
      "\n",
      "Context: porifera bereproduksi secara seksual atau aseksual, spesies yang bereproduksi secara seksual mengeluarkan sel sperma ke air dan bertemu sel telur ( ada yang dilepas ke air dan ada yang tetap di tubuh \" ibu \" ). telur yang berfertilisasi berenang mencari tempat untuk menempel, dan tumbuh menjadi individu baru. spons dapat beregenerasi dari bagian yang terpotong, apabila bagian tersebut punya sel yang tepat. sedangkan yang bereproduksi secara aseksual menggunakan tiga cara, yaitu : tunas, gemula, dan fragmentasi.\n",
      "Question: bagaimana aseksual berkembangbiak?\n",
      "Answer: \n",
      "Hypothesis: aseksual berkembangbiak adalah \n",
      "\n",
      "Context: kabupaten banyuwangi adalah sebuah kabupaten di provinsi jawa timur, indonesia. ibu kotanya adalah kota banyuwangi. kabupaten ini terletak di ujung paling timur pulau jawa, di kawasan tapal kuda, dan berbatasan dengan kabupaten situbondo di utara, selat bali di timur, samudra hindia di selatan serta kabupaten jember dan kabupaten bondowoso di barat. kabupaten banyuwangi merupakan kabupaten terluas di jawa timur sekaligus menjadi yang terluas di pulau jawa, dengan luas wilayahnya yang mencapai 5. 782, 50km2, atau lebih luas dari pulau bali ( 5. 636, 66km2 ). di pesisir kabupaten banyuwangi, terdapat pelabuhan ketapang, yang merupakan perhubungan utama antara pulau jawa dengan pulau bali ( pelabuhan gilimanuk ).\n",
      "Question: bagaimana keadaan kabupaten terluas di provinsi jawa timur?\n",
      "Answer: \n",
      "Hypothesis: keadaan kabupaten terluas di provinsi jawa timur adalah \n",
      "\n",
      "Context: pt hm sampoerna tbk / hanjaya mandala sampoerna ( ) adalah perusahaan rokok terbesar di indonesia. kantor pusatnya berada di surabaya, jawa timur. perusahaan ini sebelumnya merupakan perusahaan yang dimiliki keluarga sampoerna, namun sejak mei 2005 kepemilikan mayoritasnya berpindah tangan ke philip morris international, perusahaan rokok terbesar di dunia dari amerika serikat, mengakhiri tradisi keluarga yang melebihi 90 tahun.\n",
      "Question: bagaimana keadaan kantor pusat perusahaan silver?\n",
      "Answer: \n",
      "Hypothesis: keadaan kantor pusat perusahaan silver adalah \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cek rule based, mengikuti context\n",
    "\n",
    "def three_print(data=qas_df):\n",
    "    print(f\"Context: {data['Context'][i]}\")\n",
    "    print(f\"Question: {data['Question'][i]}\")\n",
    "    print(f\"Answer: {data['Gold Answer'][i]}\")\n",
    "    pred_hypothesis, gold_hypothesis = smoothing(data['Question'][i], data['Prediction Answer'][i], data['Gold Answer'][i], \n",
    "                                                 type=\"rule based\")\n",
    "    print(f\"Hypothesis: {gold_hypothesis}\")\n",
    "    print()\n",
    "\n",
    "for i in range(len(qas_df['Question'])):\n",
    "    \n",
    "    current_question = qas_df['Question'][i].split()\n",
    "        \n",
    "    if 'Bagaimana' in current_question: three_print()\n",
    "    elif 'bagaimana' in current_question: three_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(question, pred_answer, gold_answer, type, question_mark=question_mark):\n",
    "    \n",
    "    if type == 'replace first':\n",
    "        pred_hypothesis = question.replace('?', '')\n",
    "        pred_hypothesis = pred_hypothesis.replace(question.split()[0], pred_answer)\n",
    "\n",
    "        gold_hypothesis = question.replace('?', '')\n",
    "        gold_hypothesis = gold_hypothesis.replace(question.split()[0], gold_answer)\n",
    "    \n",
    "    elif type == 'replace question mark':\n",
    "        for i in question.split():\n",
    "            if i in question_mark:\n",
    "                pred_hypothesis = question.replace('?', '')\n",
    "                pred_hypothesis = pred_hypothesis.replace(i, pred_answer)\n",
    "\n",
    "                gold_hypothesis = question.replace('?', '')\n",
    "                gold_hypothesis = gold_hypothesis.replace(i, gold_answer)\n",
    "    \n",
    "    elif type == 'add adalah':\n",
    "        pred_hypothesis = question.replace('?', '')\n",
    "        pred_hypothesis = pred_hypothesis.replace(question.split()[0], '')\n",
    "        pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "        gold_hypothesis = question.replace('?', '')\n",
    "        gold_hypothesis = gold_hypothesis.replace(question.split()[0], '')\n",
    "        gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "    \n",
    "    elif type == 'just concat answer and question':\n",
    "        pred_hypothesis = f\"{question} {pred_answer}\"         \n",
    "        gold_hypothesis = f\"{question} {gold_answer}\"\n",
    "        \n",
    "    elif type == 'rule based':\n",
    "        question = question.replace('kah', '')\n",
    "        for j in question.split():\n",
    "            if j in question_mark:\n",
    "                if j == 'siapa' or j == 'siapakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_answer} merupakan {pred_hypothesis}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_answer} merupakan {gold_hypothesis}\"\n",
    "\n",
    "                elif j == 'apa' or j == 'apakah' or j == 'adakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "\n",
    "                elif j == 'dimana' or j == 'dimanakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} di {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} di {gold_answer}\"\n",
    "\n",
    "                elif j == 'darimanakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} dari {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} dari {gold_answer}\"\n",
    "\n",
    "                elif j == 'kapan' or j == 'kapankah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} pada {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} pada {gold_answer}\"\n",
    "\n",
    "                elif j == 'bagaimana' or j == 'bagaimanakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '')\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "\n",
    "                elif j == 'kenapa' or j == 'mengapa':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, 'alasan').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah karena {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, 'alasan').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah karena {gold_answer}\"\n",
    "\n",
    "                elif j == 'berapa' or j == 'berapakah' or j == 'seberapa': \n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    \n",
    "                    if 'luas' in pred_hypothesis.split():\n",
    "                        pred_hypothesis = pred_hypothesis.replace('luas', '')\n",
    "                        pred_hypothesis = f\"{pred_hypothesis} memiliki luas {pred_answer}\"\n",
    "                    \n",
    "                    elif 'jumlah'in pred_hypothesis.split():\n",
    "                        pred_hypothesis = pred_hypothesis.replace('jumlah', '')\n",
    "                        pred_hypothesis = f\"{pred_hypothesis} berjumlah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    \n",
    "                    if 'luas' in gold_hypothesis.split():\n",
    "                        gold_hypothesis = gold_hypothesis.replace('luas', '')\n",
    "                        gold_hypothesis = f\"{gold_hypothesis} memiliki luas {gold_answer}\"\n",
    "                    \n",
    "                    elif 'jumlah'in gold_hypothesis.split():\n",
    "                        gold_hypothesis = gold_hypothesis.replace('jumlah', '')\n",
    "                        gold_hypothesis = f\"{gold_hypothesis} berjumlah {gold_answer}\"\n",
    "                    \n",
    "    elif type == 'machine generation': \n",
    "        pred_hypothesis, gold_hypothesis = smoothing(question, pred_answer, gold_answer, type=\"rule based\")\n",
    "        pred_hypothesis = nlp_tg(pred_hypothesis)[0]['generated_text']\n",
    "        gold_hypothesis = nlp_tg(gold_hypothesis)[0]['generated_text']\n",
    "    \n",
    "    elif type == 'machine generation with translation':\n",
    "        pred_hypothesis, gold_hypothesis = smoothing(question, pred_answer, gold_answer, type=\"rule based\")\n",
    "\n",
    "        pred_hypothesis = GoogleTranslator(source='id', target='en').translate(pred_hypothesis)\n",
    "        gold_hypothesis = GoogleTranslator(source='id', target='en').translate(gold_hypothesis)\n",
    "        \n",
    "        pred_hypothesis = nlp_tg_eng(pred_hypothesis)[0]['generated_text']\n",
    "        gold_hypothesis = nlp_tg_eng(gold_hypothesis)[0]['generated_text']\n",
    "        \n",
    "        pred_hypothesis = GoogleTranslator(source='en', target='id').translate(pred_hypothesis)\n",
    "        gold_hypothesis = GoogleTranslator(source='en', target='id').translate(gold_hypothesis)\n",
    "        \n",
    "    return pred_hypothesis, gold_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemenangan Messi mungkin karena kemahirannya di area tersebut, yang mungkin menjelaskan mengapa dia menang.\n",
      "Keunggulan tim messi dalam tim menjadi alasan di balik kemenangan tersebut, demikian disampaikan sang pelatih.\n"
     ]
    }
   ],
   "source": [
    "pred_hypothesis, gold_hypothesis = smoothing(\"kenapa messi menang?\", \"mungkin karena jago\", \"tim yang unggul\",\n",
    "                                            type=\"machine generation with translation\")\n",
    "print(pred_hypothesis)\n",
    "print(gold_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_for_evaluation(predict_result, type_smoothing, type_qas, MAXIMUM_SEARCH_ITER=MAXIMUM_SEARCH_ITER):\n",
    "    \n",
    "    # Ekstrak dari PredictionOutput QAS\n",
    "    predictions_idx = np.argsort(predict_result.predictions, axis=2)[:, :, 1 * -1]\n",
    "    label_array = np.asarray(predict_result.label_ids)\n",
    "    \n",
    "    question_array = []\n",
    "    context_array = []\n",
    "    \n",
    "    pred_answer_before_filtering_array = []\n",
    "    pred_answer_after_filtering_array = []\n",
    "    \n",
    "    label_before_filtering_array = []\n",
    "    label_after_filtering_array = []\n",
    "    \n",
    "    pred_hypothesis_before_filtering_array = []\n",
    "    pred_hypothesis_after_filtering_array = []\n",
    "    \n",
    "    gold_answer_array = []\n",
    "    gold_hypothesis_array = []\n",
    "    \n",
    "    # Iterasi ini ditujukan untuk retrieve answer\n",
    "    for i in tqdm(range(len(predict_result.predictions[0]))):\n",
    "        \n",
    "        isFoundBiggest = False\n",
    "        \n",
    "        start_pred_idx = predictions_idx[0][i]\n",
    "        end_pred_idx = predictions_idx[1][i] + 1\n",
    "        \n",
    "        start_gold_idx = label_array[0][i]\n",
    "        end_gold_idx = label_array[1][i] + 1\n",
    "        \n",
    "        # Retrieve answer prediksi\n",
    "        pred_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_pred_idx: end_pred_idx], skip_special_tokens=True)\n",
    "        \n",
    "        # Retrieve answer gold\n",
    "        gold_answer = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                       [start_gold_idx: end_gold_idx], skip_special_tokens=True)\n",
    "        \n",
    "        question = []\n",
    "        context = []\n",
    "        \n",
    "        # Iterasi ini untuk retrieve question dan context index yang bersangkutan\n",
    "        for j in range(len(tokenized_data_qas_id_validation[i]['token_type_ids'])):\n",
    "            \n",
    "            # Bila token_type_ids-nya 0, maka itu question (sesuai dengan urutan tokenisasi)\n",
    "            if tokenized_data_qas_id_validation[i]['token_type_ids'][j] == 0:\n",
    "                question.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "            \n",
    "            # Bila token_type_ids-nya 1, maka itu context (sesuai dengan urutan tokenisasi)\n",
    "            else:\n",
    "                context.append(tokenized_data_qas_id_validation[i]['input_ids'][j])\n",
    "        \n",
    "        question_decoded = tokenizer.decode(question, skip_special_tokens=True)\n",
    "        context_decoded = tokenizer.decode(context, skip_special_tokens=True)\n",
    "        pred_hypothesis, gold_hypothesis = smoothing(question_decoded, pred_answer, gold_answer, type_smoothing)\n",
    "\n",
    "        # Cek label dari answer prediksi dan context\n",
    "        predicted_label = nlp_sc({'text': context_decoded, \n",
    "                                  'text_pair': pred_hypothesis}, \n",
    "                                 **tokenizer_kwargs)\n",
    "        \n",
    "        pred_answer_before_filtering_array.append([pred_answer])\n",
    "        pred_hypothesis_before_filtering_array.append([pred_hypothesis])\n",
    "        label_before_filtering_array.append([predicted_label])\n",
    "        \n",
    "        # Cek label dari answer prediksi dan context, bila labelnya entailment (atau neutral), maka answernya jadi hasil akhir\n",
    "        if predicted_label['label'] == 'neutral':\n",
    "            if type_qas == 'entailment or neutral':\n",
    "                question_array.append(question_decoded)\n",
    "                context_array.append(context_decoded)\n",
    "                pred_answer_after_filtering_array.append([pred_answer])\n",
    "                gold_answer_array.append(gold_answer)\n",
    "                pred_hypothesis_after_filtering_array.append([pred_hypothesis])\n",
    "                gold_hypothesis_array.append(gold_hypothesis)\n",
    "                label_after_filtering_array.append([predicted_label])\n",
    "\n",
    "        if predicted_label['label'] == 'entailment':\n",
    "            if type_qas == 'entailment only' or type_qas == 'entailment or neutral':\n",
    "                question_array.append(question_decoded)\n",
    "                context_array.append(context_decoded)\n",
    "                pred_answer_after_filtering_array.append([pred_answer])\n",
    "                gold_answer_array.append(gold_answer)\n",
    "                pred_hypothesis_after_filtering_array.append([pred_hypothesis])\n",
    "                gold_hypothesis_array.append(gold_hypothesis)\n",
    "                label_after_filtering_array.append([predicted_label])\n",
    "            \n",
    "        # Cek label dari answer prediksi dan context, bila labelnya bukan entailment (atau neutral), \n",
    "        # -- maka masuk ke for-loop untuk iterasi ke argmax selanjutnya, dengan menggunakan argsort\n",
    "        else:\n",
    "            \n",
    "            if predicted_label == 'neutral' and type_qas == 'entailment or neutral': continue\n",
    "            \n",
    "            # Bila MAXIMUM_SEARCH_ITER dibawah 2, maka continue langsung\n",
    "            if MAXIMUM_SEARCH_ITER < 2: continue\n",
    "\n",
    "            # Bila MAXIMUM_SEARCH_ITER diatas 2, maka continue langsung\n",
    "            \n",
    "            else:\n",
    "                # Bila bukan entailment, loop sebanyak MAXIMUM_SEARCH_ITER kali.\n",
    "                pred_answer_after_filtering_array_msi_recorded = []\n",
    "                pred_hypothesis_after_filtering_array_msi_recorded = []\n",
    "                label_after_filtering_array_msi_recorded = []\n",
    "                for index_largest in range(MAXIMUM_SEARCH_ITER - 1):\n",
    "                    \n",
    "                    #pred_answer_after_filtering_array_msi_recorded = []\n",
    "                    #pred_hypothesis_after_filtering_array_msi_recorded = []\n",
    "                    #label_after_filtering_array_msi_recorded = []\n",
    "\n",
    "                    # Cari di index kedua, ketiga, keempat, dan seterusnya\n",
    "                    predictions_idx_inside_loop = np.argsort(predict_result.predictions, \n",
    "                                                             axis=2)[:, :, (index_largest + 2) * -1]\n",
    "\n",
    "                    start_pred_idx = predictions_idx_inside_loop[0][i]\n",
    "                    end_pred_idx = predictions_idx_inside_loop[1][i] + 1\n",
    "\n",
    "                    # Retrieve answer prediksi\n",
    "                    pred_answer_inside_loop = tokenizer.decode(tokenized_data_qas_id_validation[i]['input_ids']\n",
    "                                                   [start_pred_idx: end_pred_idx], skip_special_tokens=True)\n",
    "                    \n",
    "                    pred_hypothesis_inside_loop, gold_hypothesis = smoothing(\n",
    "                        question_decoded, pred_answer_inside_loop, gold_answer, type_smoothing)\n",
    "                    \n",
    "                    # Cek label dari answer prediksi dan context\n",
    "                    predicted_label_inside_loop = nlp_sc({'text': context_decoded, \n",
    "                                                          'text_pair': pred_hypothesis_inside_loop}\n",
    "                                                           , **tokenizer_kwargs)\n",
    "                    \n",
    "                    pred_answer_after_filtering_array_msi_recorded.append(pred_answer_inside_loop)\n",
    "                    pred_hypothesis_after_filtering_array_msi_recorded.append(pred_hypothesis_inside_loop)\n",
    "                    label_after_filtering_array_msi_recorded.append(predicted_label_inside_loop)\n",
    "                    \n",
    "                    # Bila label-nya sudah entailment (atau neutral), maka answernya jadi hasil akhir, dan break\n",
    "                    if type_qas == 'entailment only':\n",
    "                        if predicted_label_inside_loop['label'] == 'entailment':\n",
    "                            isFoundBiggest = True\n",
    "                            question_array.append(question_decoded)\n",
    "                            context_array.append(context_decoded)\n",
    "                            gold_answer_array.append(gold_answer)   \n",
    "                            gold_hypothesis_array.append(gold_hypothesis)\n",
    "                            \n",
    "                            pred_answer_after_filtering_array.append(pred_answer_after_filtering_array_msi_recorded)\n",
    "                            pred_hypothesis_after_filtering_array.append(pred_hypothesis_after_filtering_array_msi_recorded)\n",
    "                            label_after_filtering_array.append(label_after_filtering_array_msi_recorded)\n",
    "                            break\n",
    "                            \n",
    "                    elif type_qas == 'entailment or neutral':\n",
    "                        if predicted_label_inside_loop['label'] == 'entailment' or predicted_label_inside_loop['label'] == 'neutral':\n",
    "                            isFoundBiggest = True\n",
    "                            question_array.append(question_decoded)\n",
    "                            context_array.append(context_decoded)\n",
    "                            gold_answer_array.append(gold_answer)   \n",
    "                            gold_hypothesis_array.append(gold_hypothesis)\n",
    "                            \n",
    "                            pred_answer_after_filtering_array.append(pred_answer_after_filtering_array_msi_recorded)\n",
    "                            pred_hypothesis_after_filtering_array.append(pred_hypothesis_after_filtering_array_msi_recorded)\n",
    "                            label_after_filtering_array.append(label_after_filtering_array_msi_recorded)\n",
    "                            break\n",
    "\n",
    "                if isFoundBiggest == False:\n",
    "                    # Bila sampai iterasi terakhir, belum entailment (atau neutral) juga, maka append saja jawaban kosong\n",
    "                    \n",
    "                    pred_answer_not_found_biggest = \"\" # Disini, jawaban kosong\n",
    "                    \n",
    "                    question_array.append(question_decoded)\n",
    "                    context_array.append(context_decoded)\n",
    "                    \n",
    "                    pred_hypothesis_not_found_biggest, gold_hypothesis = smoothing(\n",
    "                        question_decoded, pred_answer_not_found_biggest, gold_answer, type_smoothing)\n",
    "                    \n",
    "                    pred_answer_after_filtering_array_msi_recorded.append(pred_answer_not_found_biggest)\n",
    "                    pred_hypothesis_after_filtering_array_msi_recorded.append(pred_hypothesis_not_found_biggest)\n",
    "                    label_after_filtering_array_msi_recorded.append(predicted_label_inside_loop)\n",
    "                    \n",
    "                    gold_answer_array.append(gold_answer)\n",
    "                    gold_hypothesis_array.append(gold_hypothesis)\n",
    "                    \n",
    "                    pred_answer_after_filtering_array.append(pred_answer_after_filtering_array_msi_recorded)\n",
    "                    pred_hypothesis_after_filtering_array.append(pred_hypothesis_after_filtering_array_msi_recorded)\n",
    "                    label_after_filtering_array.append(label_after_filtering_array_msi_recorded)\n",
    "    \n",
    "    # Buat DataFrame QAS\n",
    "    qas_df = pd.DataFrame({'Context': context_array, \n",
    "                           'Question': question_array, \n",
    "                           \n",
    "                           'Prediction Answer Before Filtering': pred_answer_before_filtering_array,\n",
    "                           'Prediction Hypothesis Before Filtering': pred_hypothesis_before_filtering_array,\n",
    "                           'Label Before Filtering': label_before_filtering_array,\n",
    "                                 \n",
    "                           'Prediction Answer After Filtering': pred_answer_after_filtering_array,\n",
    "                           'Prediction Hypothesis After Filtering': pred_hypothesis_after_filtering_array,\n",
    "                           'Label After Filtering': label_after_filtering_array,\n",
    "                          \n",
    "                           'Gold Answer': gold_answer_array,\n",
    "                          'Gold Hypothesis': gold_hypothesis_array})\n",
    "                          \n",
    "    assert len(predict_result.predictions[0]) == len(qas_df), \"Jumlah prediksi berbeda dengan jumlah evaluasi\"\n",
    "    \n",
    "    # Return DataFrame QAS\n",
    "    return qas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                                | 13/764 [00:32<31:05,  2.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_df_for_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreplace first\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_qas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentailment only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAXIMUM_SEARCH_ITER\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m eval_df\n",
      "Cell \u001b[0;32mIn[61], line 126\u001b[0m, in \u001b[0;36mcreate_df_for_evaluation\u001b[0;34m(predict_result, type_smoothing, type_qas, MAXIMUM_SEARCH_ITER)\u001b[0m\n\u001b[1;32m    122\u001b[0m pred_hypothesis_inside_loop, gold_hypothesis \u001b[38;5;241m=\u001b[39m smoothing(\n\u001b[1;32m    123\u001b[0m     question_decoded, pred_answer_inside_loop, gold_answer, type_smoothing)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Cek label dari answer prediksi dan context\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m predicted_label_inside_loop \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_sc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_decoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m                                      \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_pair\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_hypothesis_inside_loop\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m pred_answer_after_filtering_array_msi_recorded\u001b[38;5;241m.\u001b[39mappend(pred_answer_inside_loop)\n\u001b[1;32m    131\u001b[0m pred_hypothesis_after_filtering_array_msi_recorded\u001b[38;5;241m.\u001b[39mappend(pred_hypothesis_inside_loop)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py:155\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:1109\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1103\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         )\n\u001b[1;32m   1107\u001b[0m     )\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:1116\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1115\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1116\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:1015\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1014\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1015\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py:182\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_inputs):\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    534\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    535\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 537\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:549\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 549\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:449\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 449\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_df = create_df_for_evaluation(predict_result, type_smoothing='replace first', type_qas='entailment only', MAXIMUM_SEARCH_ITER=2)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['Question'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['Context'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['Prediction Answer After Filtering'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_from_df(df, type_qas):\n",
    "    \n",
    "    denominator = len(df)\n",
    "    total_correct = 0\n",
    "    f1_array = []\n",
    "    \n",
    "    true_positive_before_filtering = 0\n",
    "    false_positive_before_filtering = 0\n",
    "    false_negative_before_filtering = 0\n",
    "    true_negative_before_filtering = 0\n",
    "    \n",
    "    true_positive_after_filtering = 0\n",
    "    false_positive_after_filtering = 0\n",
    "    false_negative_after_filtering = 0\n",
    "    true_negative_after_filtering = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        pred_answer_before_filtering = df[\"Prediction Answer Before Filtering\"][i][-1]\n",
    "        pred_answer_after_filtering = df[\"Prediction Answer After Filtering\"][i][-1]\n",
    "        \n",
    "        pred_label_before_filtering = df[\"Label Before Filtering\"][i][-1]['label']\n",
    "        pred_label_after_filtering = df[\"Label After Filtering\"][i][-1]['label']\n",
    "        \n",
    "        gold_text = df[\"Gold Answer\"][i]\n",
    "\n",
    "        if pred_answer_after_filtering == gold_text:\n",
    "            total_correct += 1\n",
    "\n",
    "        f1 = compute_f1_prec_rec(pred=pred_answer_after_filtering, gold=gold_text)\n",
    "\n",
    "        f1_array.append(f1)\n",
    "        \n",
    "        # Terprediksi dengan label yang benar, dan hasil answernya benar -> True positive\n",
    "        # Terprediksi dengan label yang benar, padahal hasil answernya salah -> False positive\n",
    "        # Terprediksi dengan label yang salah, padahal hasil answernya benar -> False negative\n",
    "        # Terprediksi dengan label yang salah, dan hasil answernya salah -> True negative\n",
    "        \n",
    "        if type_qas == 'entailment only':\n",
    "        \n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering == 'entailment'):\n",
    "                true_positive_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering == 'entailment'):\n",
    "                false_positive_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering != 'entailment'):\n",
    "                false_negative_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering != 'entailment'):\n",
    "                true_negative_after_filtering += 1\n",
    "\n",
    "            if (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'entailment'):\n",
    "                true_positive_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'entailment'):\n",
    "                false_positive_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering != 'entailment'):\n",
    "                false_negative_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering != 'entailment'):\n",
    "                true_negative_before_filtering += 1\n",
    "        \n",
    "        elif type_qas == 'entailment or neutral':\n",
    "        \n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering == 'entailment' \n",
    "                                                               or pred_label_after_filtering == 'neutral'):\n",
    "                true_positive_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering == 'entailment' \n",
    "                                                                 or pred_label_after_filtering == 'neutral'):\n",
    "                false_positive_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering != 'entailment' \n",
    "                                                                 and pred_label_after_filtering != 'neutral'):\n",
    "                false_negative_after_filtering += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering != 'entailment' \n",
    "                                                                 and pred_label_after_filtering != 'neutral'):\n",
    "                true_negative_after_filtering += 1\n",
    "\n",
    "            if (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'entailment' \n",
    "                                                                or pred_label_after_filtering == 'neutral'):\n",
    "                true_positive_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'entailment' \n",
    "                                                                  or pred_label_after_filtering == 'neutral'):\n",
    "                false_positive_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering != 'entailment' \n",
    "                                                                  and pred_label_after_filtering != 'neutral'):\n",
    "                false_negative_before_filtering += 1\n",
    "            elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering != 'entailment' \n",
    "                                                                  and pred_label_after_filtering != 'neutral'):\n",
    "                true_negative_before_filtering += 1\n",
    "\n",
    "    exact_match = ((total_correct / denominator) * 100.0)\n",
    "    final_f1 = np.mean(f1_array) * 100.0\n",
    "    after_filtering_metric_array = [true_positive_after_filtering, false_positive_after_filtering, \n",
    "                          false_negative_after_filtering, true_negative_after_filtering]\n",
    "    before_filtering_metric_array = [true_positive_before_filtering, false_positive_before_filtering, \n",
    "                          false_negative_before_filtering, true_negative_before_filtering]\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': final_f1}, after_filtering_metric_array, before_filtering_metric_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_result_after_filtering, after_filtering_metric_array, before_filtering_metric_array = compute_metrics_from_df(\n",
    "    eval_df, \"entailment only\")\n",
    "metric_result_after_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_non_zero(number):\n",
    "    if number == 0:\n",
    "        number += sys.float_info.min\n",
    "    return number\n",
    "\n",
    "def compute_f1_prec_rec_whole(metric_array):\n",
    "    accuracy = (metric_array[0] + metric_array[3]) / \\\n",
    "        (metric_array[0] + metric_array[1] + \n",
    "         metric_array[2] + metric_array[3])\n",
    "    \n",
    "    precision = (metric_array[0]) / (metric_array[0] + metric_array[1])\n",
    "    \n",
    "    recall = (metric_array[0]) / (metric_array[0] + metric_array[2])\n",
    "    \n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def diff_verbose_metric(metric_result_before, metric_result_after, metric):\n",
    "    \n",
    "    percentage = round(((metric_result_after - metric_result_before) / metric_result_before) * 100, 2)\n",
    "    \n",
    "    if '&' in metric: vocab = \"nilai\"\n",
    "    else: vocab = \"metrik\"\n",
    "    \n",
    "    if metric_result_before ==  metric_result_after:\n",
    "        print(f\"Hasil {vocab} {metric} sebelum filtering NLI SAMA DENGAN metrik setelah filtering NLI\")\n",
    "    elif metric_result_before <  metric_result_after:\n",
    "        print(f\"Hasil {vocab} {metric} setelah filtering NLI mengalami KENAIKAN sebesar: {percentage} %\")\n",
    "    elif metric_result_before >  metric_result_after:\n",
    "        print(f\"Hasil {vocab} {metric} setelah filtering NLI mengalami PENURUNAN sebesar: {-1 * percentage} %\")\n",
    "    \n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_metrics(metrics_before, metrics_after, \n",
    "                    after_filtering_metric_array=after_filtering_metric_array, \n",
    "                    before_filtering_metric_array=before_filtering_metric_array):\n",
    "    \n",
    "    em_before = metrics_before['exact_match']\n",
    "    f1_before = metrics_before['f1']\n",
    "\n",
    "    print(\"~ METRIK PER TOKEN ~\")\n",
    "    print(f\"Skor Exact Match sebelum filtering NLI: {em_before}\")\n",
    "    print(f\"Skor F1 sebelum filtering NLI: {f1_before}\")\n",
    "    print()\n",
    "\n",
    "    em_after = metrics_after['exact_match']\n",
    "    f1_after = metrics_after['f1']\n",
    "\n",
    "    print(f\"Skor Exact Match setelah filtering NLI: {em_after}\")\n",
    "    print(f\"Skor F1 setelah filtering NLI: {f1_after}\")\n",
    "    print()\n",
    "\n",
    "    em_before = convert_to_non_zero(em_before)\n",
    "    f1_before = convert_to_non_zero(f1_before)\n",
    "\n",
    "    em_after = convert_to_non_zero(em_after)\n",
    "    f1_after = convert_to_non_zero(f1_after)\n",
    "\n",
    "    print(\"~ METRIK DENGAN PARAMETER NLI ~\")\n",
    "    print(f\"[BEFORE FILTERING] Jawaban benar & label NLI yang sesuai: {before_filtering_metric_array[0]}\")\n",
    "    print(f\"[BEFORE FILTERING] Jawaban TIDAK benar & label NLI yang sesuai: {before_filtering_metric_array[1]}\")\n",
    "    print(f\"[BEFORE FILTERING] Jawaban benar & label NLI yang TIDAK sesuai: {before_filtering_metric_array[2]}\")\n",
    "    print(f\"[BEFORE FILTERING] Jawaban TIDAK benar & label NLI yang TIDAK sesuai: {before_filtering_metric_array[3]}\")\n",
    "    print()\n",
    "\n",
    "    print(f\"[AFTER FILTERING] Jawaban benar & label NLI yang sesuai: {after_filtering_metric_array[0]}\")\n",
    "    print(f\"[AFTER FILTERING] Jawaban TIDAK benar & label NLI yang sesuai: {after_filtering_metric_array[1]}\")\n",
    "    print(f\"[AFTER FILTERING] Jawaban benar & label NLI yang TIDAK sesuai: {after_filtering_metric_array[2]}\")\n",
    "    print(f\"[AFTER FILTERING] Jawaban TIDAK benar & label NLI yang TIDAK sesuai: {after_filtering_metric_array[3]}\")\n",
    "    print()\n",
    "\n",
    "    print(\"Metrik di atas, bisa direpresentasikan menjadi:\")\n",
    "\n",
    "    acc_before_whole, prec_before_whole, rec_before_whole, f1_before_whole = compute_f1_prec_rec_whole(\n",
    "        before_filtering_metric_array)\n",
    "    acc_after_whole, prec_after_whole, rec_after_whole, f1_after_whole = compute_f1_prec_rec_whole(\n",
    "        after_filtering_metric_array)\n",
    "\n",
    "    print(f\"[BEFORE FILTERING] Akurasi: {acc_before_whole}\")\n",
    "    print(f\"[BEFORE FILTERING] Precision: {prec_before_whole}\")\n",
    "    print(f\"[BEFORE FILTERING] Recall: {rec_before_whole}\")\n",
    "    print(f\"[BEFORE FILTERING] F1: {f1_before_whole}\")\n",
    "    print()\n",
    "\n",
    "    print(f\"[AFTER FILTERING] Akurasi: {acc_after_whole}\")\n",
    "    print(f\"[AFTER FILTERING] Precision: {prec_after_whole}\")\n",
    "    print(f\"[AFTER FILTERING] Recall: {rec_after_whole}\")\n",
    "    print(f\"[AFTER FILTERING] F1: {f1_after_whole}\")\n",
    "    print()\n",
    "\n",
    "    print(\"--- Persentase perubahan hasil metrik ---\")\n",
    "    print(\"~ METRIK PER TOKEN ~\")\n",
    "    diff_verbose_metric(em_before, em_after, \"Exact Match\")\n",
    "    diff_verbose_metric(f1_before, f1_after, \"F1\")\n",
    "    print()\n",
    "\n",
    "    print(\"~ METRIK DENGAN PARAMETER NLI ~\")\n",
    "    diff_verbose_metric(before_filtering_metric_array[0], after_filtering_metric_array[0], \n",
    "                        \"Jawaban benar & label NLI yang sesuai\")\n",
    "    diff_verbose_metric(before_filtering_metric_array[1], after_filtering_metric_array[1], \n",
    "                        \"Jawaban TIDAK benar & label NLI yang sesuai\")\n",
    "    diff_verbose_metric(before_filtering_metric_array[2], after_filtering_metric_array[2], \n",
    "                        \"Jawaban benar & label NLI yang TIDAK sesuai\")\n",
    "    diff_verbose_metric(before_filtering_metric_array[3], after_filtering_metric_array[3], \n",
    "                        \"Jawaban TIDAK benar & label NLI yang TIDAK sesuai\")\n",
    "    print()\n",
    "\n",
    "    print(\"Metrik di atas, bisa direpresentasikan menjadi:\")\n",
    "    diff_verbose_metric(acc_before_whole, acc_after_whole, \"Akurasi\")\n",
    "    diff_verbose_metric(prec_before_whole, prec_after_whole, \"Precision\")\n",
    "    diff_verbose_metric(rec_before_whole, rec_after_whole, \"Recall\")\n",
    "    diff_verbose_metric(f1_before_whole, f1_after_whole, \"F1\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metrics(metric_result_before_filtering, metric_result_after_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(ACCURACY_DIR), exist_ok=True)\n",
    "with open(f'{ACCURACY_DIR}/metric_comparison_results.txt', \"w\") as f, contextlib.redirect_stdout(f):\n",
    "    compare_metrics(metric_result_before_filtering, metric_result_after_filtering)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"Context\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_evaluation(df):\n",
    "    \n",
    "    num_apa_right = 0\n",
    "    num_dimana_right = 0\n",
    "    num_kapan_right = 0\n",
    "    num_siapa_right = 0\n",
    "    num_bagaimana_right = 0\n",
    "    num_kenapa_right = 0\n",
    "    num_berapa_right = 0\n",
    "    num_others_right = 0\n",
    "\n",
    "    num_apa_wrong = 0\n",
    "    num_dimana_wrong = 0\n",
    "    num_kapan_wrong = 0\n",
    "    num_siapa_wrong = 0\n",
    "    num_bagaimana_wrong = 0\n",
    "    num_kenapa_wrong = 0\n",
    "    num_berapa_wrong = 0\n",
    "    num_others_wrong = 0\n",
    "\n",
    "    under_hundred_right = 0\n",
    "    _101_to_150_right = 0\n",
    "    _151_to_200_right = 0\n",
    "    _201_to_250_right = 0\n",
    "    _251_to_300_right = 0\n",
    "    _over_301_right = 0\n",
    "\n",
    "    under_hundred_wrong = 0\n",
    "    _101_to_150_wrong = 0\n",
    "    _151_to_200_wrong = 0\n",
    "    _201_to_250_wrong = 0\n",
    "    _251_to_300_wrong = 0\n",
    "    _over_301_wrong = 0\n",
    "\n",
    "    q_one_to_five_right = 0\n",
    "    q_six_to_ten_right = 0\n",
    "    q_eleven_to_fifteen_right = 0\n",
    "    q_sixteen_to_twenty_right = 0\n",
    "    q_over_twenty_right = 0\n",
    "\n",
    "    q_one_to_five_wrong = 0\n",
    "    q_six_to_ten_wrong = 0\n",
    "    q_eleven_to_fifteen_wrong = 0\n",
    "    q_sixteen_to_twenty_wrong = 0\n",
    "    q_over_twenty_wrong = 0\n",
    "\n",
    "    a_zero_right = 0\n",
    "    a_one_to_five_right = 0\n",
    "    a_six_to_ten_right = 0\n",
    "    a_eleven_to_fifteen_right = 0\n",
    "    a_sixteen_to_twenty_right = 0\n",
    "    a_over_twenty_right = 0\n",
    "\n",
    "    a_zero_wrong = 0\n",
    "    a_one_to_five_wrong = 0\n",
    "    a_six_to_ten_wrong = 0\n",
    "    a_eleven_to_fifteen_wrong = 0\n",
    "    a_sixteen_to_twenty_wrong = 0\n",
    "    a_over_twenty_wrong = 0\n",
    "\n",
    "    # Cek semua properti EDA, yang berhasil berapa, yang gagal berapa?\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        pred_answer_after_filtering = df[\"Prediction Answer After Filtering\"][i][-1]       \n",
    "        gold_text = df[\"Gold Answer\"][i]\n",
    "        current_question = df[\"Question\"][i].split()\n",
    "        len_current_passage = len(df[\"Context\"][i].split())\n",
    "        len_current_question = len(df[\"Question\"][i].split())\n",
    "        len_current_gold_text = len(df[\"Gold Answer\"][i].split())\n",
    "\n",
    "        if (pred_answer_after_filtering == gold_text):\n",
    "            if 'Apa' in current_question: num_apa_right += 1\n",
    "            elif 'Apakah' in current_question: num_apa_right += 1\n",
    "            elif 'apa' in current_question: num_apa_right += 1\n",
    "            elif 'apakah' in current_question: num_apa_right += 1\n",
    "\n",
    "            elif 'Dimana' in current_question: num_dimana_right += 1\n",
    "            elif 'dimana' in current_question: num_dimana_right += 1\n",
    "            elif 'mana' in current_question: num_dimana_right += 1\n",
    "\n",
    "            elif 'Kapan' in current_question: num_kapan_right += 1\n",
    "            elif 'kapan' in current_question: num_kapan_right += 1\n",
    "\n",
    "            elif 'Siapa' in current_question: num_siapa_right += 1\n",
    "            elif 'siapa' in current_question: num_siapa_right += 1\n",
    "\n",
    "            elif 'Bagaimana' in current_question: num_bagaimana_right += 1\n",
    "            elif 'bagaimana' in current_question: num_bagaimana_right += 1\n",
    "\n",
    "            elif 'Mengapa' in current_question: num_kenapa_right += 1\n",
    "            elif 'Kenapa' in current_question: num_kenapa_right += 1\n",
    "            elif 'mengapa' in current_question: num_kenapa_right += 1\n",
    "            elif 'kenapa' in current_question: num_kenapa_right += 1\n",
    "\n",
    "            elif 'Berapa' in current_question: num_berapa_right += 1\n",
    "            elif 'Berapakah' in current_question: num_berapa_right += 1\n",
    "            elif 'berapa' in current_question: num_berapa_right += 1\n",
    "            elif 'berapakah' in current_question: num_berapa_right += 1\n",
    "\n",
    "            else: num_others_right += 1\n",
    "\n",
    "            if len_current_passage <= 100: \n",
    "                under_hundred_right += 1\n",
    "            elif len_current_passage >= 101 & len_current_passage <= 150:\n",
    "                _101_to_150_right += 1\n",
    "            elif len_current_passage >= 151 & len_current_passage <= 200:\n",
    "                _151_to_200_right += 1\n",
    "            elif len_current_passage >= 201 & len_current_passage <= 250:\n",
    "                _201_to_250_right += 1\n",
    "            elif len_current_passage >= 251 & len_current_passage <= 300:\n",
    "                _251_to_300_right += 1\n",
    "            elif len_current_passage >= 301:\n",
    "                _over_301_right += 1\n",
    "\n",
    "            if len_current_question <= 5: \n",
    "                q_one_to_five_right += 1\n",
    "            elif len_current_question >= 6 & len_current_question <= 10:\n",
    "                q_six_to_ten_right += 1\n",
    "            elif len_current_question >= 11 & len_current_question <= 15:\n",
    "                q_eleven_to_fifteen_right += 1\n",
    "            elif len_current_question >= 16 & len_current_question <= 20:\n",
    "                q_sixteen_to_twenty_right += 1\n",
    "            elif len_current_question >= 21: \n",
    "                q_over_twenty_right += 1\n",
    "\n",
    "            if len_current_gold_text <= 5: \n",
    "                a_one_to_five_right += 1\n",
    "            elif len_current_gold_text >= 6 & len_current_gold_text <= 10:\n",
    "                a_six_to_ten_right += 1\n",
    "            elif len_current_gold_text >= 11 & len_current_gold_text <= 15:\n",
    "                a_eleven_to_fifteen_right += 1\n",
    "            elif len_current_gold_text >= 16 & len_current_gold_text <= 20:\n",
    "                a_sixteen_to_twenty_right += 1\n",
    "            elif len_current_gold_text >= 21: \n",
    "                a_over_twenty_right += 1\n",
    "            elif len_current_gold_text == 0:\n",
    "                a_zero_right += 1\n",
    "\n",
    "        elif (pred_answer_after_filtering != gold_text):\n",
    "            if 'Apa' in current_question: num_apa_wrong += 1\n",
    "            elif 'Apakah' in current_question: num_apa_wrong += 1\n",
    "            elif 'apa' in current_question: num_apa_wrong += 1\n",
    "            elif 'apakah' in current_question: num_apa_wrong += 1\n",
    "\n",
    "            elif 'Dimana' in current_question: num_dimana_wrong += 1\n",
    "            elif 'dimana' in current_question: num_dimana_wrong += 1\n",
    "            elif 'mana' in current_question: num_dimana_wrong += 1\n",
    "\n",
    "            elif 'Kapan' in current_question: num_kapan_wrong += 1\n",
    "            elif 'kapan' in current_question: num_kapan_wrong += 1\n",
    "\n",
    "            elif 'Siapa' in current_question: num_siapa_wrong += 1\n",
    "            elif 'siapa' in current_question: num_siapa_wrong += 1\n",
    "\n",
    "            elif 'Bagaimana' in current_question: num_bagaimana_wrong += 1\n",
    "            elif 'bagaimana' in current_question: num_bagaimana_wrong += 1\n",
    "\n",
    "            elif 'Mengapa' in current_question: num_kenapa_wrong += 1\n",
    "            elif 'Kenapa' in current_question: num_kenapa_wrong += 1\n",
    "            elif 'mengapa' in current_question: num_kenapa_wrong += 1\n",
    "            elif 'kenapa' in current_question: num_kenapa_wrong += 1\n",
    "\n",
    "            elif 'Berapa' in current_question: num_berapa_wrong += 1\n",
    "            elif 'Berapakah' in current_question: num_berapa_wrong += 1\n",
    "            elif 'berapa' in current_question: num_berapa_wrong += 1\n",
    "            elif 'berapakah' in current_question: num_berapa_wrong += 1\n",
    "\n",
    "            else: num_others_wrong += 1\n",
    "\n",
    "            if len_current_passage <= 100: \n",
    "                under_hundred_wrong += 1\n",
    "            elif len_current_passage >= 101 & len_current_passage <= 150:\n",
    "                _101_to_150_wrong += 1\n",
    "            elif len_current_passage >= 151 & len_current_passage <= 200:\n",
    "                _151_to_200_wrong += 1\n",
    "            elif len_current_passage >= 201 & len_current_passage <= 250:\n",
    "                _201_to_250_wrong += 1\n",
    "            elif len_current_passage >= 251 & len_current_passage <= 300:\n",
    "                _251_to_300_wrong += 1\n",
    "            elif len_current_passage >= 301:\n",
    "                _over_301_wrong += 1\n",
    "\n",
    "            if len_current_question <= 5: \n",
    "                q_one_to_five_wrong += 1\n",
    "            elif len_current_question >= 6 & len_current_question <= 10:\n",
    "                q_six_to_ten_wrong += 1\n",
    "            elif len_current_question >= 11 & len_current_question <= 15:\n",
    "                q_eleven_to_fifteen_wrong += 1\n",
    "            elif len_current_question >= 16 & len_current_question <= 20:\n",
    "                q_sixteen_to_twenty_wrong += 1\n",
    "            elif len_current_question >= 21: \n",
    "                q_over_twenty_wrong += 1\n",
    "\n",
    "            if len_current_gold_text <= 5: \n",
    "                a_one_to_five_wrong += 1\n",
    "            elif len_current_gold_text >= 6 & len_current_gold_text <= 10:\n",
    "                a_six_to_ten_wrong += 1\n",
    "            elif len_current_gold_text >= 11 & len_current_gold_text <= 15:\n",
    "                a_eleven_to_fifteen_wrong += 1\n",
    "            elif len_current_gold_text >= 16 & len_current_gold_text <= 20:\n",
    "                a_sixteen_to_twenty_wrong += 1\n",
    "            elif len_current_gold_text >= 21: \n",
    "                a_over_twenty_wrong += 1\n",
    "            elif len_current_gold_text == 0:\n",
    "                a_zero_wrong += 1\n",
    "\n",
    "    assert len(df) == num_apa_right+num_dimana_right+num_kapan_right+num_siapa_right+\\\n",
    "                        num_bagaimana_right+num_kenapa_right+num_berapa_right+num_others_right+\\\n",
    "                        num_apa_wrong+num_dimana_wrong+num_kapan_wrong+num_siapa_wrong+\\\n",
    "                        num_bagaimana_wrong+num_kenapa_wrong+num_berapa_wrong+num_others_wrong\n",
    "\n",
    "    assert len(df) == under_hundred_right+_101_to_150_right+_151_to_200_right+_201_to_250_right+\\\n",
    "                        _251_to_300_right+_over_301_right+\\\n",
    "                        under_hundred_wrong+_101_to_150_wrong+_151_to_200_wrong+_201_to_250_wrong+\\\n",
    "                        _251_to_300_wrong+_over_301_wrong\n",
    "\n",
    "    assert len(df) == q_one_to_five_right+q_six_to_ten_right+q_eleven_to_fifteen_right+q_sixteen_to_twenty_right+\\\n",
    "                        q_over_twenty_right+\\\n",
    "                        q_one_to_five_wrong+q_six_to_ten_wrong+q_eleven_to_fifteen_wrong+q_sixteen_to_twenty_wrong+\\\n",
    "                        q_over_twenty_wrong\n",
    "\n",
    "    assert len(df) == a_one_to_five_right+a_six_to_ten_right+a_eleven_to_fifteen_right+a_sixteen_to_twenty_right+\\\n",
    "                        a_over_twenty_right+a_zero_right+\\\n",
    "                        a_one_to_five_wrong+a_six_to_ten_wrong+a_eleven_to_fifteen_wrong+a_sixteen_to_twenty_wrong+\\\n",
    "                        a_over_twenty_wrong+a_zero_wrong\n",
    "\n",
    "    # Ambil berapa contoh yang gagal, coba pelajari reasoning type-nya.\n",
    "    new_df = df.sample(n=15, random_state=42)\n",
    "\n",
    "    print(\"--- Bagian tentang question type ---\")\n",
    "    print(f\"-- Bagian tentang question type yang terprediksi BENAR --\")\n",
    "    print(f\"Banyak pertanyaan APA: {num_apa_right}, sebesar: {round((num_apa_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan DIMANA: {num_dimana_right}, sebesar: {round((num_dimana_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan KAPAN: {num_kapan_right}, sebesar: {round((num_kapan_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan SIAPA: {num_siapa_right}, sebesar: {round((num_siapa_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan BAGAIMANA: {num_bagaimana_right}, sebesar: {round((num_bagaimana_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan KENAPA: {num_kenapa_right}, sebesar: {round((num_kenapa_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan BERAPA: {num_berapa_right}, sebesar: {round((num_berapa_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan LAINNYA: {num_others_right}, sebesar: {round((num_others_right/len(df) * 100), 2)} %\")\n",
    "    print()\n",
    "    print(f\"-- Bagian tentang question type yang terprediksi SALAH --\")\n",
    "    print(f\"Banyak pertanyaan APA: {num_apa_wrong}, sebesar: {round((num_apa_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan DIMANA: {num_dimana_wrong}, sebesar: {round((num_dimana_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan KAPAN: {num_kapan_wrong}, sebesar: {round((num_kapan_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan SIAPA: {num_siapa_wrong}, sebesar: {round((num_siapa_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan BAGAIMANA: {num_bagaimana_wrong}, sebesar: {round((num_bagaimana_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan KENAPA: {num_kenapa_wrong}, sebesar: {round((num_kenapa_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan BERAPA: {num_berapa_wrong}, sebesar: {round((num_berapa_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan LAINNYA: {num_others_wrong}, sebesar: {round((num_others_wrong/len(df) * 100), 2)} %\")\n",
    "    print()\n",
    "    print(f\"-- Presentase kebenaran --\")\n",
    "    print(f\"Banyak pertanyaan APA yang terpediksi benar sebesar: {round((num_apa_right/(num_apa_right+num_apa_wrong) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan DIMANA yang terpediksi benar sebesar: {round((num_dimana_right/(num_dimana_right+num_dimana_wrong) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan KAPAN yang terpediksi benar sebesar: {round((num_kapan_right/(num_kapan_right+num_kapan_wrong) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan SIAPA yang terpediksi benar sebesar: {round((num_siapa_right/(num_siapa_right+num_siapa_wrong) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan BAGAIMANA yang terpediksi benar sebesar: {round((num_bagaimana_right/(num_bagaimana_right+num_bagaimana_wrong) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan KENAPA yang terpediksi benar sebesar: {round((num_kenapa_right/(num_kenapa_right+num_kenapa_wrong) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan BERAPA yang terpediksi benar sebesar: {round((num_berapa_right/(num_berapa_right+num_berapa_wrong) * 100), 2)} %\")\n",
    "    print(f\"Banyak pertanyaan LAINNYA yang terpediksi benar sebesar: {round((num_others_right/(num_others_right+num_others_wrong) * 100), 2)} %\")\n",
    "    print()\n",
    "\n",
    "    print(\"--- Bagian tentang panjang context ---\")\n",
    "    print(f\"-- Bagian tentang panjang context yang terprediksi BENAR --\")\n",
    "    print(f\"Panjang konteks < 100: {under_hundred_right}, sebesar: {round((under_hundred_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks 101 <= x <= 150: {_101_to_150_right}, sebesar: {round((_101_to_150_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks 151 <= x <= 200: {_151_to_200_right}, sebesar: {round((_151_to_200_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks 201 <= x <= 250: {_201_to_250_right}, sebesar: {round((_201_to_250_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks 251 <= x <= 300: {_251_to_300_right}, sebesar: {round((_251_to_300_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks > 300: {_over_301_right}, sebesar: {round((_over_301_right/len(df) * 100), 2)} %\")\n",
    "    print()\n",
    "    print(f\"-- Bagian tentang panjang context yang terprediksi SALAH --\")\n",
    "    print(f\"Panjang konteks < 100: {under_hundred_wrong}, sebesar: {round((under_hundred_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks 101 <= x <= 150: {_101_to_150_wrong}, sebesar: {round((_101_to_150_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks 151 <= x <= 200: {_151_to_200_wrong}, sebesar: {round((_151_to_200_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks 201 <= x <= 250: {_201_to_250_wrong}, sebesar: {round((_201_to_250_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks 251 <= x <= 300: {_251_to_300_wrong}, sebesar: {round((_251_to_300_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks > 300: {_over_301_wrong}, sebesar: {round((_over_301_wrong/len(df) * 100), 2)} %\")\n",
    "    print()\n",
    "    print(f\"-- Presentase kebenaran --\")\n",
    "    print(f\"Panjang konteks < 100 yang terprediksi benar sebesar: {(under_hundred_right+under_hundred_wrong) and round((under_hundred_right/(under_hundred_right+under_hundred_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks 101 <= x <= 150 yang terprediksi benar sebesar: {(_101_to_150_right+_101_to_150_wrong) and round((_101_to_150_right/(_101_to_150_right+_101_to_150_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks 151 <= x <= 200 yang terprediksi benar sebesar: {(_151_to_200_right+_151_to_200_wrong) and round((_151_to_200_right/(_151_to_200_right+_151_to_200_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks 201 <= x <= 250 yang terprediksi benar sebesar: {(_201_to_250_right+_201_to_250_wrong) and round((_201_to_250_right/(_201_to_250_right+_201_to_250_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks 251 <= x <= 300 yang terprediksi benar sebesar: {(_251_to_300_right+_251_to_300_wrong) and round((_251_to_300_right/(_251_to_300_right+_251_to_300_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang konteks > 300 yang terprediksi benar sebesar: {(_over_301_right+_over_301_wrong) and round((_over_301_right/(_over_301_right+_over_301_wrong) * 100), 2)} %\")\n",
    "    print()\n",
    "\n",
    "    print(\"--- Bagian tentang panjang question ---\")\n",
    "    print(f\"-- Bagian tentang panjang question yang terprediksi BENAR --\")\n",
    "    print(f\"Panjang question 1 <= x <= 5: {q_one_to_five_right}, sebesar: {round((q_one_to_five_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 6 <= x <= 10: {q_six_to_ten_right}, sebesar: {round((q_six_to_ten_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 11 <= x <= 15: {q_eleven_to_fifteen_right}, sebesar: {round((q_eleven_to_fifteen_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 16 <= x <= 20: {q_sixteen_to_twenty_right}, sebesar: {round((q_sixteen_to_twenty_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question > 20: {q_over_twenty_right}, sebesar: {round((q_over_twenty_right/len(df) * 100), 2)} %\")\n",
    "    print()\n",
    "    print(f\"-- Bagian tentang panjang question yang terprediksi SALAH --\")\n",
    "    print(f\"Panjang question 1 <= x <= 5: {q_one_to_five_wrong}, sebesar: {round((q_one_to_five_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 6 <= x <= 10: {q_six_to_ten_wrong}, sebesar: {round((q_six_to_ten_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 11 <= x <= 15: {q_eleven_to_fifteen_wrong}, sebesar: {round((q_eleven_to_fifteen_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 16 <= x <= 20: {q_sixteen_to_twenty_wrong}, sebesar: {round((q_sixteen_to_twenty_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question > 20: {q_over_twenty_wrong}, sebesar: {round((q_over_twenty_wrong/len(df) * 100), 2)} %\")\n",
    "    print()\n",
    "    print(f\"-- Presentase kebenaran --\")\n",
    "    print(f\"Panjang question 1 <= x <= 5 yang terprediksi benar sebesar: {(q_one_to_five_right+q_one_to_five_wrong) and round((q_one_to_five_right/(q_one_to_five_right+q_one_to_five_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 6 <= x <= 10 yang terprediksi benar sebesar: {(q_six_to_ten_right+q_six_to_ten_wrong) and round((q_six_to_ten_right/(q_six_to_ten_right+q_six_to_ten_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 11 <= x <= 15 yang terprediksi benar sebesar: {(q_eleven_to_fifteen_right+q_eleven_to_fifteen_wrong) and round((q_eleven_to_fifteen_right/(q_eleven_to_fifteen_right+q_eleven_to_fifteen_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 16 <= x <= 20 yang terprediksi benar sebesar: {(q_sixteen_to_twenty_right+q_sixteen_to_twenty_wrong) and round((q_sixteen_to_twenty_right/(q_sixteen_to_twenty_right+q_sixteen_to_twenty_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang question > 20 yang terprediksi benar sebesar: {round((q_over_twenty_right+q_over_twenty_wrong) and (q_over_twenty_right/(q_over_twenty_right+q_over_twenty_wrong) * 100), 2)} %\")\n",
    "    print()\n",
    "\n",
    "    print(\"--- Bagian tentang panjang gold answer ---\")\n",
    "    print(f\"-- Bagian tentang panjang gold answer yang terprediksi BENAR --\")\n",
    "    print(f\"Panjang question 1 <= x <= 5: {a_one_to_five_right}, sebesar: {round((a_one_to_five_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 6 <= x <= 10: {a_six_to_ten_right}, sebesar: {round((a_six_to_ten_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 11 <= x <= 15: {a_eleven_to_fifteen_right}, sebesar: {round((a_eleven_to_fifteen_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 16 <= x <= 20: {a_sixteen_to_twenty_right}, sebesar: {round((a_sixteen_to_twenty_right/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question > 20: {a_over_twenty_right}, sebesar: {round((a_over_twenty_right/len(df) * 100), 2)} %\")\n",
    "    print()\n",
    "    print(f\"-- Bagian tentang panjang gold answer yang terprediksi SALAH --\")\n",
    "    print(f\"Panjang question 1 <= x <= 5: {a_one_to_five_wrong}, sebesar: {round((a_one_to_five_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 6 <= x <= 10: {a_six_to_ten_wrong}, sebesar: {round((a_six_to_ten_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 11 <= x <= 15: {a_eleven_to_fifteen_wrong}, sebesar: {round((a_eleven_to_fifteen_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 16 <= x <= 20: {a_sixteen_to_twenty_wrong}, sebesar: {round((a_sixteen_to_twenty_wrong/len(df) * 100), 2)} %\")\n",
    "    print(f\"Panjang question > 20: {a_over_twenty_wrong}, sebesar: {round((a_over_twenty_wrong/len(df) * 100), 2)} %\")\n",
    "    print()\n",
    "    print(f\"-- Presentase kebenaran --\")\n",
    "    print(f\"Panjang question 1 <= x <= 5 yang terprediksi benar sebesar: {(a_one_to_five_right+a_one_to_five_wrong) and round((a_one_to_five_right/(a_one_to_five_right+a_one_to_five_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 6 <= x <= 10 yang terprediksi benar sebesar: {(a_six_to_ten_right+a_six_to_ten_wrong) and round((a_six_to_ten_right/(a_six_to_ten_right+a_six_to_ten_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 11 <= x <= 15 yang terprediksi benar sebesar: {(a_eleven_to_fifteen_right+a_eleven_to_fifteen_wrong) and round((a_eleven_to_fifteen_right/(a_eleven_to_fifteen_right+a_eleven_to_fifteen_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang question 16 <= x <= 20 yang terprediksi benar sebesar: {(a_sixteen_to_twenty_right+a_sixteen_to_twenty_wrong) and round((a_sixteen_to_twenty_right/(a_sixteen_to_twenty_right+a_sixteen_to_twenty_wrong) * 100), 2)} %\")\n",
    "    print(f\"Panjang question > 20 yang terprediksi benar sebesar: {round((a_over_twenty_right+a_over_twenty_wrong) and (a_over_twenty_right/(a_over_twenty_right+a_over_twenty_wrong) * 100), 2)} %\")\n",
    "    print()\n",
    "\n",
    "    print(\"--- Bagian untuk analisis REASONING TYPE ---\")\n",
    "    display(new_df)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_evaluation(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakdown_evaluation(df, TYPE_QAS):\n",
    "    \n",
    "    if TYPE_QAS == 'entailment only': compatible_label = ['entailment']\n",
    "    elif TYPE_QAS == 'entailment or neutral': compatible_label = ['entailment', 'neutral']\n",
    "\n",
    "    exist_true_answer_label_entailment = 0\n",
    "    exist_true_answer_label_neutral = 0\n",
    "    exist_true_answer_label_contradiction = 0\n",
    "\n",
    "    exist_false_answer_label_entailment = 0\n",
    "    exist_false_answer_label_neutral = 0\n",
    "    exist_false_answer_label_contradiction = 0\n",
    "\n",
    "    no_exist_true_answer_label_entailment = 0\n",
    "    no_exist_true_answer_label_neutral = 0\n",
    "    no_exist_true_answer_label_contradiction = 0\n",
    "\n",
    "    no_exist_false_answer_label_entailment = 0\n",
    "    no_exist_false_answer_label_neutral = 0\n",
    "    no_exist_false_answer_label_contradiction = 0\n",
    "\n",
    "    filtered_in_right_answer_to_filtered_in_right_answer = 0\n",
    "    filtered_in_right_answer_to_filtered_in_wrong_answer = 0\n",
    "    filtered_in_right_answer_to_filtered_out_right_answer = 0\n",
    "    filtered_in_right_answer_to_filtered_out_wrong_answer = 0\n",
    "\n",
    "    filtered_in_wrong_answer_to_filtered_in_right_answer = 0\n",
    "    filtered_in_wrong_answer_to_filtered_in_wrong_answer = 0\n",
    "    filtered_in_wrong_answer_to_filtered_out_right_answer = 0\n",
    "    filtered_in_wrong_answer_to_filtered_out_wrong_answer = 0\n",
    "\n",
    "    filtered_out_right_answer_to_filtered_in_right_answer = 0\n",
    "    filtered_out_right_answer_to_filtered_in_wrong_answer = 0\n",
    "    filtered_out_right_answer_to_filtered_out_right_answer = 0\n",
    "    filtered_out_right_answer_to_filtered_out_wrong_answer = 0\n",
    "\n",
    "    filtered_out_wrong_answer_to_filtered_in_right_answer = 0\n",
    "    filtered_out_wrong_answer_to_filtered_in_wrong_answer = 0\n",
    "    filtered_out_wrong_answer_to_filtered_out_right_answer = 0\n",
    "    filtered_out_wrong_answer_to_filtered_out_wrong_answer = 0\n",
    "\n",
    "    filtered_in_right_answer_to_filtered_in_right_answer_unanswered = 0\n",
    "    filtered_in_right_answer_to_filtered_in_wrong_answer_unanswered = 0\n",
    "    filtered_in_right_answer_to_filtered_out_right_answer_unanswered = 0\n",
    "    filtered_in_right_answer_to_filtered_out_wrong_answer_unanswered = 0\n",
    "\n",
    "    filtered_in_wrong_answer_to_filtered_in_right_answer_unanswered = 0\n",
    "    filtered_in_wrong_answer_to_filtered_in_wrong_answer_unanswered = 0\n",
    "    filtered_in_wrong_answer_to_filtered_out_right_answer_unanswered = 0\n",
    "    filtered_in_wrong_answer_to_filtered_out_wrong_answer_unanswered = 0\n",
    "\n",
    "    filtered_out_right_answer_to_filtered_in_right_answer_unanswered = 0\n",
    "    filtered_out_right_answer_to_filtered_in_wrong_answer_unanswered = 0\n",
    "    filtered_out_right_answer_to_filtered_out_right_answer_unanswered = 0\n",
    "    filtered_out_right_answer_to_filtered_out_wrong_answer_unanswered = 0\n",
    "\n",
    "    filtered_out_wrong_answer_to_filtered_in_right_answer_unanswered = 0\n",
    "    filtered_out_wrong_answer_to_filtered_in_wrong_answer_unanswered = 0\n",
    "    filtered_out_wrong_answer_to_filtered_out_right_answer_unanswered = 0\n",
    "    filtered_out_wrong_answer_to_filtered_out_wrong_answer_unanswered = 0\n",
    "\n",
    "    filtered_score_labels_before_filtering = []\n",
    "    filtered_score_labels_after_filtering = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        pred_answer_before_filtering = df[\"Prediction Answer Before Filtering\"][i][-1]\n",
    "        pred_answer_after_filtering = df[\"Prediction Answer After Filtering\"][i][-1]\n",
    "\n",
    "        pred_label_before_filtering = df[\"Label Before Filtering\"][i][-1]['label']\n",
    "        pred_label_after_filtering = df[\"Label After Filtering\"][i][-1]['label']\n",
    "\n",
    "        pred_prob_dist_before_filtering = df[\"Label Before Filtering\"][i][-1]['score']\n",
    "        pred_prob_dist_after_filtering = df[\"Label After Filtering\"][i][-1]['score']\n",
    "\n",
    "        gold_text = df[\"Gold Answer\"][i]\n",
    "\n",
    "        # Bagian untuk jawaban sebelum filtering SAMA DENGAN ground truth\n",
    "\n",
    "        if (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'entailment') \\\n",
    "                and (pred_answer_before_filtering != \"\"): \n",
    "            exist_true_answer_label_entailment += 1\n",
    "\n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_in_right_answer_to_filtered_in_right_answer += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_in_right_answer_to_filtered_in_wrong_answer += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_in_right_answer_to_filtered_out_right_answer += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_in_right_answer_to_filtered_out_wrong_answer += 1\n",
    "\n",
    "        elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'neutral') \\\n",
    "                and (pred_answer_before_filtering != \"\"): \n",
    "            exist_true_answer_label_neutral += 1\n",
    "\n",
    "            if (TYPE_QAS == 'entailment only'):\n",
    "\n",
    "                filtered_score_labels_before_filtering.append(pred_prob_dist_before_filtering)\n",
    "\n",
    "                if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_out_right_answer_to_filtered_in_right_answer += 1\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_out_right_answer_to_filtered_in_wrong_answer += 1\n",
    "                elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_out_right_answer_to_filtered_out_right_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_out_right_answer_to_filtered_out_wrong_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "            elif (TYPE_QAS == 'entailment or neutral'):\n",
    "                if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_in_right_answer_to_filtered_in_right_answer += 1\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_in_right_answer_to_filtered_in_wrong_answer += 1\n",
    "                elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_in_right_answer_to_filtered_out_right_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_in_right_answer_to_filtered_out_wrong_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "        elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'contradiction') \\\n",
    "                and (pred_answer_before_filtering != \"\"): \n",
    "            exist_true_answer_label_contradiction += 1\n",
    "\n",
    "            filtered_score_labels_before_filtering.append(pred_prob_dist_before_filtering)\n",
    "\n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_out_right_answer_to_filtered_in_right_answer += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_out_right_answer_to_filtered_in_wrong_answer += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_out_right_answer_to_filtered_out_right_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_out_right_answer_to_filtered_out_wrong_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "        # Bagian untuk jawaban sebelum filtering BERBEDA DENGAN ground truth\n",
    "\n",
    "        elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'entailment') \\\n",
    "                and (pred_answer_before_filtering != \"\"):\n",
    "            exist_false_answer_label_entailment += 1\n",
    "\n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_in_wrong_answer_to_filtered_in_right_answer += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_in_wrong_answer_to_filtered_in_wrong_answer += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_in_wrong_answer_to_filtered_out_right_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_in_wrong_answer_to_filtered_out_wrong_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "        elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'neutral') \\\n",
    "                and (pred_answer_before_filtering != \"\"):\n",
    "            exist_false_answer_label_neutral += 1\n",
    "\n",
    "            if (TYPE_QAS == 'entailment only'):\n",
    "\n",
    "                filtered_score_labels_before_filtering.append(pred_prob_dist_before_filtering)\n",
    "\n",
    "                if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_out_wrong_answer_to_filtered_in_right_answer += 1\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_out_wrong_answer_to_filtered_in_wrong_answer += 1\n",
    "                elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_out_wrong_answer_to_filtered_out_right_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_out_wrong_answer_to_filtered_out_wrong_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "            elif (TYPE_QAS == 'entailment or neutral'):\n",
    "                if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_in_wrong_answer_to_filtered_in_right_answer += 1\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_in_wrong_answer_to_filtered_in_wrong_answer += 1\n",
    "                elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_in_wrong_answer_to_filtered_out_right_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_in_wrong_answer_to_filtered_out_wrong_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "        elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'contradiction') \\\n",
    "                and (pred_answer_before_filtering != \"\"):\n",
    "            exist_false_answer_label_contradiction += 1\n",
    "\n",
    "            filtered_score_labels_before_filtering.append(pred_prob_dist_before_filtering)\n",
    "\n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_out_wrong_answer_to_filtered_in_right_answer += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_out_wrong_answer_to_filtered_in_wrong_answer += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_out_wrong_answer_to_filtered_out_right_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_out_wrong_answer_to_filtered_out_wrong_answer += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "        # Bagian untuk jawaban sebelum filtering SAMA DENGAN ground truth (unanswered)\n",
    "\n",
    "        elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'entailment') \\\n",
    "                and (pred_answer_before_filtering == \"\"): \n",
    "            no_exist_true_answer_label_entailment += 1\n",
    "\n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_in_right_answer_to_filtered_in_right_answer_unanswered += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_in_right_answer_to_filtered_in_wrong_answer_unanswered += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_in_right_answer_to_filtered_out_right_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_in_right_answer_to_filtered_out_wrong_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "        elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'neutral') \\\n",
    "                and (pred_answer_before_filtering == \"\"): \n",
    "            no_exist_true_answer_label_neutral += 1\n",
    "\n",
    "            if (TYPE_QAS == 'entailment only'):\n",
    "\n",
    "                filtered_score_labels_before_filtering.append(pred_prob_dist_before_filtering)\n",
    "\n",
    "                if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_out_right_answer_to_filtered_in_right_answer_unanswered += 1\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_out_right_answer_to_filtered_in_wrong_answer_unanswered += 1\n",
    "                elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_out_right_answer_to_filtered_out_right_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_out_right_answer_to_filtered_out_wrong_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "            elif (TYPE_QAS == 'entailment or neutral'):\n",
    "                if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_in_right_answer_to_filtered_in_right_answer_unanswered += 1\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_in_right_answer_to_filtered_in_wrong_answer_unanswered += 1\n",
    "                elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_in_right_answer_to_filtered_out_right_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_in_right_answer_to_filtered_out_wrong_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "        elif (pred_answer_before_filtering == gold_text) and (pred_label_before_filtering == 'contradiction') \\\n",
    "                and (pred_answer_before_filtering == \"\"): \n",
    "            no_exist_true_answer_label_contradiction += 1\n",
    "\n",
    "            filtered_score_labels_before_filtering.append(pred_prob_dist_before_filtering)\n",
    "\n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_out_right_answer_to_filtered_in_right_answer_unanswered += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_out_right_answer_to_filtered_in_wrong_answer_unanswered += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_out_right_answer_to_filtered_out_right_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_out_right_answer_to_filtered_out_wrong_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "        # Bagian untuk jawaban sebelum filtering BERBEDA DENGAN ground truth (unanswered)\n",
    "\n",
    "        elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'entailment') \\\n",
    "                and (pred_answer_before_filtering == \"\"):\n",
    "            no_exist_false_answer_label_entailment += 1\n",
    "\n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_in_wrong_answer_to_filtered_in_right_answer_unanswered += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_in_wrong_answer_to_filtered_in_wrong_answer_unanswered += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_in_wrong_answer_to_filtered_out_right_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_in_wrong_answer_to_filtered_out_wrong_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "        elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'neutral') \\\n",
    "                and (pred_answer_before_filtering == \"\"):\n",
    "            no_exist_false_answer_label_neutral += 1\n",
    "\n",
    "            if (TYPE_QAS == 'entailment only'):\n",
    "\n",
    "                filtered_score_labels_before_filtering.append(pred_prob_dist_before_filtering)\n",
    "\n",
    "                if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_out_wrong_answer_to_filtered_in_right_answer_unanswered += 1\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_out_wrong_answer_to_filtered_in_wrong_answer_unanswered += 1\n",
    "                elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_out_wrong_answer_to_filtered_out_right_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_out_wrong_answer_to_filtered_out_wrong_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "            elif (TYPE_QAS == 'entailment or neutral'):\n",
    "                if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_in_wrong_answer_to_filtered_in_right_answer_unanswered += 1\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                        : filtered_in_wrong_answer_to_filtered_in_wrong_answer_unanswered += 1\n",
    "                elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_in_wrong_answer_to_filtered_out_right_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "                elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                        : filtered_in_wrong_answer_to_filtered_out_wrong_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "        elif (pred_answer_before_filtering != gold_text) and (pred_label_before_filtering == 'contradiction') \\\n",
    "                and (pred_answer_before_filtering == \"\"):\n",
    "            no_exist_false_answer_label_contradiction += 1\n",
    "\n",
    "            filtered_score_labels_before_filtering.append(pred_prob_dist_before_filtering)\n",
    "\n",
    "            if (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_out_wrong_answer_to_filtered_in_right_answer_unanswered += 1\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering in compatible_label) \\\n",
    "                    : filtered_out_wrong_answer_to_filtered_in_wrong_answer_unanswered += 1\n",
    "            elif (pred_answer_after_filtering == gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_out_wrong_answer_to_filtered_out_right_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "            elif (pred_answer_after_filtering != gold_text) and (pred_label_after_filtering not in compatible_label) \\\n",
    "                    : filtered_out_wrong_answer_to_filtered_out_wrong_answer_unanswered += 1; filtered_score_labels_after_filtering.append(pred_prob_dist_after_filtering)\n",
    "\n",
    "        #print(f\"Pred answer before filtering: {pred_answer_before_filtering}\")\n",
    "        #print(f\"Pred answer after filtering: {pred_answer_after_filtering}\")\n",
    "        #print(f\"Gold answer: {gold_text}\")\n",
    "        #print()\n",
    "\n",
    "    print(f\"--- Bagian ini hanya memperhatikan sebelum filtering ---\")\n",
    "    print(f\"Jawaban benar (answer exist) entailment: {exist_true_answer_label_entailment}, sebesar: {round(exist_true_answer_label_entailment/len(df) * 100, 2)} %\")\n",
    "    print(f\"Jawaban benar (answer exist) neutral: {exist_true_answer_label_neutral}, sebesar: {round(exist_true_answer_label_neutral/len(df) * 100, 2)} %\")\n",
    "    print(f\"Jawaban benar (answer exist) contradiction: {exist_true_answer_label_contradiction}, sebesar: {round(exist_true_answer_label_contradiction/len(df) * 100, 2)} %\")\n",
    "    print()\n",
    "    print(f\"Jawaban salah (answer exist) entailment: {exist_false_answer_label_entailment}, sebesar: {round(exist_false_answer_label_entailment/len(df) * 100, 2)} %\")\n",
    "    print(f\"Jawaban salah (answer exist) neutral: {exist_false_answer_label_neutral}, sebesar: {round(exist_false_answer_label_neutral/len(df) * 100, 2)} %\")\n",
    "    print(f\"Jawaban salah (answer exist) contradiction: {exist_false_answer_label_contradiction}, sebesar: {round(exist_false_answer_label_contradiction/len(df) * 100, 2)} %\")\n",
    "    print()\n",
    "    print(f\"Jawaban benar (answer DO NOT exist) entailment: {no_exist_true_answer_label_entailment}, sebesar: {round(no_exist_true_answer_label_entailment/len(df) * 100, 2)} %\")\n",
    "    print(f\"Jawaban benar (answer DO NOT exist) neutral: {no_exist_true_answer_label_neutral}, sebesar: {round(no_exist_true_answer_label_neutral/len(df) * 100, 2)} %\")\n",
    "    print(f\"Jawaban benar (answer DO NOT exist) contradiction: {no_exist_true_answer_label_contradiction}, sebesar: {round(no_exist_true_answer_label_contradiction/len(df) * 100, 2)} %\")\n",
    "    print()\n",
    "    print(f\"Jawaban salah (answer DO NOT exist) entailment: {no_exist_false_answer_label_entailment}, sebesar: {round(no_exist_false_answer_label_entailment/len(df) * 100, 2)} %\")\n",
    "    print(f\"Jawaban salah (answer DO NOT exist) neutral: {no_exist_false_answer_label_neutral}, sebesar: {round(no_exist_false_answer_label_neutral/len(df) * 100, 2)} %\")\n",
    "    print(f\"Jawaban salah (answer DO NOT exist) contradiction: {no_exist_false_answer_label_contradiction}, sebesar: {round(no_exist_false_answer_label_contradiction/len(df) * 100, 2)} %\")\n",
    "    print()\n",
    "\n",
    "    print(f\"--- Bagian ini memperhatikan sebelum filtering dan setelah filtering ---\")\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Banyaknya data sebelum filtering: BENAR & LOLOS, setelah filtering: BENAR & LOLOS: {filtered_in_right_answer_to_filtered_in_right_answer}, sebesar: {round(filtered_in_right_answer_to_filtered_in_right_answer/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering: BENAR & LOLOS, setelah filtering: SALAH & LOLOS: {filtered_in_right_answer_to_filtered_in_wrong_answer}, sebesar: {round(filtered_in_right_answer_to_filtered_in_wrong_answer/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering: BENAR & LOLOS, setelah filtering: BENAR & TERFILTER: {filtered_in_right_answer_to_filtered_out_right_answer}, sebesar: {round(filtered_in_right_answer_to_filtered_out_right_answer/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering: BENAR & LOLOS, setelah filtering: SALAH & TERFILTER: {filtered_in_right_answer_to_filtered_out_wrong_answer}, sebesar: {round(filtered_in_right_answer_to_filtered_out_wrong_answer/len(df) * 100, 2)} %\")\n",
    "    print()\n",
    "\n",
    "    print(f\"Banyaknya data sebelum filtering: SALAH & LOLOS, setelah filtering: BENAR & LOLOS: {filtered_in_wrong_answer_to_filtered_in_right_answer}, sebesar: {round(filtered_in_wrong_answer_to_filtered_in_right_answer/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering: SALAH & LOLOS, setelah filtering: SALAH & LOLOS: {filtered_in_wrong_answer_to_filtered_in_wrong_answer}, sebesar: {round(filtered_in_wrong_answer_to_filtered_in_wrong_answer/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering: SALAH & LOLOS, setelah filtering: BENAR & TERFILTER: {filtered_in_wrong_answer_to_filtered_out_right_answer}, sebesar: {round(filtered_in_wrong_answer_to_filtered_out_right_answer/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering: SALAH & LOLOS, setelah filtering: SALAH & TERFILTER: {filtered_in_wrong_answer_to_filtered_out_wrong_answer}, sebesar: {round(filtered_in_wrong_answer_to_filtered_out_wrong_answer/len(df) * 100, 2)} %\")\n",
    "    print()\n",
    "    \"\"\"\n",
    "    print(f\"Banyaknya data sebelum filtering: BENAR & TERFILTER, setelah filtering: BENAR & LOLOS: {filtered_out_right_answer_to_filtered_in_right_answer}, sebesar: {round(filtered_out_right_answer_to_filtered_in_right_answer/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering: BENAR & TERFILTER, setelah filtering: SALAH & LOLOS: {filtered_out_right_answer_to_filtered_in_wrong_answer}, sebesar: {round(filtered_out_right_answer_to_filtered_in_wrong_answer/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering: BENAR & TERFILTER, setelah filtering: BENAR & TERFILTER: {filtered_out_right_answer_to_filtered_out_right_answer}, sebesar: {round(filtered_out_right_answer_to_filtered_out_right_answer/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering: BENAR & TERFILTER, setelah filtering: SALAH & TERFILTER: {filtered_out_right_answer_to_filtered_out_wrong_answer}, sebesar: {round(filtered_out_right_answer_to_filtered_out_wrong_answer/len(df) * 100, 2)} %\")\n",
    "    print()\n",
    "\n",
    "    print(f\"Banyaknya data sebelum filtering: SALAH & TERFILTER, setelah filtering: BENAR & LOLOS: {filtered_out_wrong_answer_to_filtered_in_right_answer}, sebesar: {round(filtered_out_wrong_answer_to_filtered_in_right_answer/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering: SALAH & TERFILTER, setelah filtering: SALAH & LOLOS: {filtered_out_wrong_answer_to_filtered_in_wrong_answer}, sebesar: {round(filtered_out_wrong_answer_to_filtered_in_wrong_answer/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering: SALAH & TERFILTER, setelah filtering: BENAR & TERFILTER: {filtered_out_wrong_answer_to_filtered_out_right_answer}, sebesar: {round(filtered_out_wrong_answer_to_filtered_out_right_answer/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering: SALAH & TERFILTER, setelah filtering: SALAH & TERFILTER: {filtered_out_wrong_answer_to_filtered_out_wrong_answer}, sebesar: {round(filtered_out_wrong_answer_to_filtered_out_wrong_answer/len(df) * 100, 2)} %\")\n",
    "    print()\n",
    "    \"\"\"\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): BENAR & LOLOS, setelah filtering: BENAR & LOLOS: {filtered_in_right_answer_to_filtered_in_right_answer_unanswered}, sebesar: {round(filtered_in_right_answer_to_filtered_in_right_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): BENAR & LOLOS, setelah filtering: SALAH & LOLOS: {filtered_in_right_answer_to_filtered_in_wrong_answer_unanswered}, sebesar: {round(filtered_in_right_answer_to_filtered_in_wrong_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): BENAR & LOLOS, setelah filtering: BENAR & TERFILTER: {filtered_in_right_answer_to_filtered_out_right_answer_unanswered}, sebesar: {round(filtered_in_right_answer_to_filtered_out_right_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): BENAR & LOLOS, setelah filtering: SALAH & TERFILTER: {filtered_in_right_answer_to_filtered_out_wrong_answer_unanswered}, sebesar: {round(filtered_in_right_answer_to_filtered_out_wrong_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print()\n",
    "\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): SALAH & LOLOS, setelah filtering: BENAR & LOLOS: {filtered_in_wrong_answer_to_filtered_in_right_answer_unanswered}, sebesar: {round(filtered_in_wrong_answer_to_filtered_in_right_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): SALAH & LOLOS, setelah filtering: SALAH & LOLOS: {filtered_in_wrong_answer_to_filtered_in_wrong_answer_unanswered}, sebesar: {round(filtered_in_wrong_answer_to_filtered_in_wrong_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): SALAH & LOLOS, setelah filtering: BENAR & TERFILTER: {filtered_in_wrong_answer_to_filtered_out_right_answer_unanswered}, sebesar: {round(filtered_in_wrong_answer_to_filtered_out_right_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): SALAH & LOLOS, setelah filtering: SALAH & TERFILTER: {filtered_in_wrong_answer_to_filtered_out_wrong_answer_unanswered}, sebesar: {round(filtered_in_wrong_answer_to_filtered_out_wrong_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print()\n",
    "    \"\"\"\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): BENAR & TERFILTER, setelah filtering: BENAR & LOLOS: {filtered_out_right_answer_to_filtered_in_right_answer_unanswered}, sebesar: {round(filtered_out_right_answer_to_filtered_in_right_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): BENAR & TERFILTER, setelah filtering: SALAH & LOLOS: {filtered_out_right_answer_to_filtered_in_wrong_answer_unanswered}, sebesar: {round(filtered_out_right_answer_to_filtered_in_wrong_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): BENAR & TERFILTER, setelah filtering: BENAR & TERFILTER: {filtered_out_right_answer_to_filtered_out_right_answer_unanswered}, sebesar: {round(filtered_out_right_answer_to_filtered_out_right_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): BENAR & TERFILTER, setelah filtering: SALAH & TERFILTER: {filtered_out_right_answer_to_filtered_out_wrong_answer_unanswered}, sebesar: {round(filtered_out_right_answer_to_filtered_out_wrong_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print()\n",
    "\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): SALAH & TERFILTER, setelah filtering: BENAR & LOLOS: {filtered_out_wrong_answer_to_filtered_in_right_answer_unanswered}, sebesar: {round(filtered_out_wrong_answer_to_filtered_in_right_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): SALAH & TERFILTER, setelah filtering: SALAH & LOLOS: {filtered_out_wrong_answer_to_filtered_in_wrong_answer_unanswered}, sebesar: {round(filtered_out_wrong_answer_to_filtered_in_wrong_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): SALAH & TERFILTER, setelah filtering: BENAR & TERFILTER: {filtered_out_wrong_answer_to_filtered_out_right_answer_unanswered}, sebesar: {round(filtered_out_wrong_answer_to_filtered_out_right_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print(f\"Banyaknya data sebelum filtering (unanswered): SALAH & TERFILTER, setelah filtering: SALAH & TERFILTER: {filtered_out_wrong_answer_to_filtered_out_wrong_answer_unanswered}, sebesar: {round(filtered_out_wrong_answer_to_filtered_out_wrong_answer_unanswered/len(df) * 100, 2)} %\")\n",
    "    print()\n",
    "\n",
    "    print(\"-- Pada pengecekan filtering awal: --\")\n",
    "    if TYPE_QAS == 'entailment only':\n",
    "        accept_right = (exist_true_answer_label_entailment) \\\n",
    "            / (exist_true_answer_label_entailment + exist_true_answer_label_neutral + exist_true_answer_label_contradiction)\n",
    "        reject_wrong = (exist_false_answer_label_neutral + exist_false_answer_label_contradiction) \\\n",
    "            / (exist_false_answer_label_entailment + exist_false_answer_label_neutral + exist_false_answer_label_contradiction)\n",
    "        print(f\"Berhasil menerima {round(accept_right * 100, 2)} % jawaban yang benar (answer exist)\")\n",
    "        print(f\"Berhasil menolak {round(reject_wrong * 100, 2)} % jawaban yang salah (answer exist)\") \n",
    "        print()\n",
    "\n",
    "        no_exist_accept_right = (no_exist_true_answer_label_entailment) \\\n",
    "            / (no_exist_true_answer_label_entailment + no_exist_true_answer_label_neutral + no_exist_true_answer_label_contradiction)\n",
    "        no_exist_reject_wrong = (no_exist_false_answer_label_neutral + no_exist_false_answer_label_contradiction) \\\n",
    "            / (no_exist_false_answer_label_entailment + no_exist_false_answer_label_neutral + no_exist_false_answer_label_contradiction)\n",
    "        print(f\"Berhasil menerima {round(no_exist_accept_right * 100, 2)} % jawaban yang benar (answer DO NOT exist)\")\n",
    "        print(f\"Berhasil menolak {round(no_exist_reject_wrong * 100, 2)} % jawaban yang salah (answer DO NOT exist)\") \n",
    "        print()\n",
    "\n",
    "    elif TYPE_QAS == 'entailment or neutral':\n",
    "        accept_right = (exist_true_answer_label_entailment + exist_true_answer_label_neutral) \\\n",
    "            / (exist_true_answer_label_entailment + exist_true_answer_label_neutral + exist_true_answer_label_contradiction)\n",
    "        reject_wrong = (exist_false_answer_label_contradiction) \\\n",
    "            / (exist_false_answer_label_entailment + exist_false_answer_label_neutral + exist_false_answer_label_contradiction)\n",
    "        print(f\"Berhasil menerima {round(accept_right * 100, 2)} % jawaban yang benar (answer exist)\")\n",
    "        print(f\"Berhasil menolak {round(reject_wrong * 100, 2)} % jawaban yang salah (answer exist)\") \n",
    "        print()\n",
    "\n",
    "        no_exist_accept_right = (no_exist_true_answer_label_entailment + no_exist_true_answer_label_neutral) \\\n",
    "            / (no_exist_true_answer_label_entailment + no_exist_true_answer_label_neutral + no_exist_true_answer_label_contradiction)\n",
    "        no_exist_reject_wrong = (no_exist_false_answer_label_contradiction) \\\n",
    "            / (no_exist_false_answer_label_entailment + no_exist_false_answer_label_neutral + no_exist_false_answer_label_contradiction)\n",
    "        print(f\"Berhasil menerima {round(no_exist_accept_right * 100, 2)} % jawaban yang benar (answer DO NOT exist)\")\n",
    "        print(f\"Berhasil menolak {round(no_exist_reject_wrong * 100, 2)} % jawaban yang salah (answer DO NOT exist)\") \n",
    "        print()\n",
    "\n",
    "    print(\"-- Setelah pengecekan filtering berdasarkan hasil akhir MSI: --\")\n",
    "    accept_right_after_filtering = (filtered_out_right_answer_to_filtered_in_right_answer + filtered_out_wrong_answer_to_filtered_in_right_answer) \\\n",
    "        / (filtered_out_right_answer_to_filtered_in_right_answer + filtered_out_wrong_answer_to_filtered_in_right_answer + filtered_out_right_answer_to_filtered_out_right_answer + filtered_out_wrong_answer_to_filtered_out_right_answer)\n",
    "\n",
    "    reject_wrong_after_filtering = (filtered_out_right_answer_to_filtered_out_wrong_answer + filtered_out_wrong_answer_to_filtered_out_wrong_answer) \\\n",
    "    / (filtered_out_right_answer_to_filtered_out_wrong_answer + filtered_out_wrong_answer_to_filtered_out_wrong_answer + filtered_out_right_answer_to_filtered_in_wrong_answer + filtered_out_wrong_answer_to_filtered_in_wrong_answer)\n",
    "\n",
    "    print(f\"Berhasil menerima {round(accept_right_after_filtering * 100, 2)} % jawaban yang benar (answer exist)\")\n",
    "    print(f\"Berhasil menolak {round(reject_wrong_after_filtering * 100, 2)} % jawaban yang salah (answer exist)\") \n",
    "    print()\n",
    "\n",
    "    no_exist_accept_right_after_filtering = (filtered_out_right_answer_to_filtered_in_right_answer_unanswered + filtered_out_wrong_answer_to_filtered_in_right_answer_unanswered) \\\n",
    "        / (filtered_out_right_answer_to_filtered_in_right_answer_unanswered + filtered_out_wrong_answer_to_filtered_in_right_answer_unanswered + filtered_out_right_answer_to_filtered_out_right_answer_unanswered + filtered_out_wrong_answer_to_filtered_out_right_answer_unanswered)\n",
    "\n",
    "    no_exist_reject_wrong_after_filtering = (filtered_out_right_answer_to_filtered_out_wrong_answer_unanswered + filtered_out_wrong_answer_to_filtered_out_wrong_answer_unanswered) \\\n",
    "    / (filtered_out_right_answer_to_filtered_out_wrong_answer_unanswered + filtered_out_wrong_answer_to_filtered_out_wrong_answer_unanswered + filtered_out_right_answer_to_filtered_in_wrong_answer_unanswered + filtered_out_wrong_answer_to_filtered_in_wrong_answer_unanswered)\n",
    "\n",
    "    print(f\"Berhasil menerima {round(no_exist_accept_right_after_filtering * 100, 2)} % jawaban yang benar (answer DO NOT exist)\")\n",
    "    print(f\"Berhasil menolak {round(no_exist_reject_wrong_after_filtering * 100, 2)} % jawaban yang salah (answer DO NOT exist)\") \n",
    "    print()\n",
    "\n",
    "    print(f\"Rerata skor yang membuat data menjadi TERFILTER sebelum iterasi MSI: {np.mean(filtered_score_labels_before_filtering)}\")\n",
    "    print(f\"Rerata skor yang membuat data menjadi TERFILTER setelah iterasi MSI: {np.mean(filtered_score_labels_after_filtering)}\")\n",
    "    print(f\"Total prediksi jawaban: {len(df)}\")\n",
    "    print()\n",
    "\n",
    "    assert len(df) == exist_true_answer_label_entailment+exist_true_answer_label_neutral+exist_true_answer_label_contradiction+\\\n",
    "            exist_false_answer_label_entailment+exist_false_answer_label_neutral+exist_false_answer_label_contradiction+\\\n",
    "            no_exist_true_answer_label_entailment+no_exist_true_answer_label_neutral+no_exist_true_answer_label_contradiction+\\\n",
    "            no_exist_false_answer_label_entailment+no_exist_false_answer_label_neutral+no_exist_false_answer_label_contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "breakdown_evaluation(eval_df, TYPE_QAS='entailment only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_df.to_csv(f'{OUTPUT_DIR}/eval_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df['Question'].str.contains(\"apa |apakah \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05c4878c74ad419f81e6abc2b9f9fae1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0633918b582f45c38356130338ccfb58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8aa4db7376e3445089af950c446aab70",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d7c6178f42ed4b9f8c5169c21783ce2d",
      "value": 2
     }
    },
    "065d96b02c7840019af97e3c135693a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ce874e529474f478a7acd45b4ffd649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_902bd6c12ea54039b8c913a7c1782ff4",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d86de7f1fbf452892fe6d4b177558f9",
      "value": 6
     }
    },
    "10fb50db270449b48382c815904245a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc7073fb093543fc990abc620348c1ca",
      "placeholder": "​",
      "style": "IPY_MODEL_acac4d071be4488fbb272df63b625213",
      "value": " 66/66 [23:11&lt;00:00, 14.47s/ba]"
     }
    },
    "113ae282ca5a499a8a2ad36a796e763f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f10632530a1b499ba3d20f0286446bab",
      "placeholder": "​",
      "style": "IPY_MODEL_94da3d6dfccc4a289c659b94bcc90658",
      "value": "#0: 100%"
     }
    },
    "12c3fd408f914038acb511c4ef57f965": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13ea541bb13b42738e74885a2c1db8df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14a450c5594148c3a4ca340521d167ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13ea541bb13b42738e74885a2c1db8df",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_68818d7eea5849afa82c3003a57a5b8d",
      "value": 6
     }
    },
    "1d86de7f1fbf452892fe6d4b177558f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23ce34e28a524ad9ab44a3715c8be175": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05c4878c74ad419f81e6abc2b9f9fae1",
      "placeholder": "​",
      "style": "IPY_MODEL_7fc9c87c6cc44cdf82ea4f74c0bfe7b4",
      "value": "100%"
     }
    },
    "2721b15477d741519a3041a8f256575b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a373f9cb48a481d85803f0792ff27b3",
       "IPY_MODEL_54a2637e8c8e4ca0917e35f4b58b48f3",
       "IPY_MODEL_7dc3dc97044c48afb06a14575fc8e91b"
      ],
      "layout": "IPY_MODEL_6c3c7e4da5af40b088f3046d7698edff"
     }
    },
    "29b13cbb638947d5a17071d299e2420f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_113ae282ca5a499a8a2ad36a796e763f",
       "IPY_MODEL_0ce874e529474f478a7acd45b4ffd649",
       "IPY_MODEL_935752d8733c41fbb25ed436606352ca"
      ],
      "layout": "IPY_MODEL_cf2b519cc5ff446dae45afa2439c840e"
     }
    },
    "2e0e9b7ee63b4c748d6294d63cc7b088": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd4d3c6663db46c9934c0d1f7a128d4c",
      "placeholder": "​",
      "style": "IPY_MODEL_3db65eb148504de6a4207708554874f0",
      "value": " 2/2 [00:00&lt;00:00,  8.75it/s]"
     }
    },
    "3db65eb148504de6a4207708554874f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f3c8a647bb24840ae71ce6e3219f4dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23ce34e28a524ad9ab44a3715c8be175",
       "IPY_MODEL_87c26a64111847f3a6cdb2f0797ae1f5",
       "IPY_MODEL_2e0e9b7ee63b4c748d6294d63cc7b088"
      ],
      "layout": "IPY_MODEL_750e7a8018e6415f8cfc15288d80f13f"
     }
    },
    "3f6183fb8ca348e0ac35b495681c705a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45f8bfb31705441c80a22dbb955dd933": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4836cff910304df894759890642ac333": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f2a3658a0f14f66a59c14d36558c7e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86b094a2e5564e4d8af8693dab674961",
      "placeholder": "​",
      "style": "IPY_MODEL_7fc619affab644758cfea5857adb7c93",
      "value": " 2/2 [00:00&lt;00:00,  4.72it/s]"
     }
    },
    "4f80ae88d884407ab8f9e08aa58632a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "540c7fcbf3b14e538cba18329bc1fc20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54a2637e8c8e4ca0917e35f4b58b48f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dce089e985634a67a381bc85cbaca017",
      "max": 66,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd9cb96d5a3c4400bb7d145590540a54",
      "value": 66
     }
    },
    "6516c5b02aa24db2bc42cdd4d1f2c260": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cbed771e77314789bf8817404e5d6f67",
       "IPY_MODEL_b0be384a887847e981868df7c551d3de",
       "IPY_MODEL_10fb50db270449b48382c815904245a8"
      ],
      "layout": "IPY_MODEL_b23174d38e3940718252770bd39177f8"
     }
    },
    "68818d7eea5849afa82c3003a57a5b8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a373f9cb48a481d85803f0792ff27b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a040ab99cb964c32a0a968d97dd55a16",
      "placeholder": "​",
      "style": "IPY_MODEL_9ef8c0b24cb5490b8fa7e8ea8ad619fe",
      "value": "#0: 100%"
     }
    },
    "6c3c7e4da5af40b088f3046d7698edff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "750e7a8018e6415f8cfc15288d80f13f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b80b681b46b4671a14a7cf82fd50991": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dc3dc97044c48afb06a14575fc8e91b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f940438113734ccdb198efd12b8a3936",
      "placeholder": "​",
      "style": "IPY_MODEL_c5b567cd34b340dea77cefd40e9aaec9",
      "value": " 66/66 [23:07&lt;00:00, 15.32s/ba]"
     }
    },
    "7fc619affab644758cfea5857adb7c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fc9c87c6cc44cdf82ea4f74c0bfe7b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8104b020ba8b498591e14a51af34306d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83abc2e663e447e7af2f826eacd155c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86b094a2e5564e4d8af8693dab674961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87c26a64111847f3a6cdb2f0797ae1f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb1fb197045a44ab8c204a8bdc3c4584",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4836cff910304df894759890642ac333",
      "value": 2
     }
    },
    "8aa4db7376e3445089af950c446aab70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ce95fb7cd2f4cf49b54dfa319f1e835": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83abc2e663e447e7af2f826eacd155c7",
      "placeholder": "​",
      "style": "IPY_MODEL_8104b020ba8b498591e14a51af34306d",
      "value": "100%"
     }
    },
    "902bd6c12ea54039b8c913a7c1782ff4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "935752d8733c41fbb25ed436606352ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f6183fb8ca348e0ac35b495681c705a",
      "placeholder": "​",
      "style": "IPY_MODEL_bdf2cb9d10cc4b2aab466a9dfe7be5bc",
      "value": " 6/6 [02:04&lt;00:00, 20.17s/ba]"
     }
    },
    "94da3d6dfccc4a289c659b94bcc90658": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c9b27a0b2ac4be3b8dcbb65e23b84a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_065d96b02c7840019af97e3c135693a8",
      "placeholder": "​",
      "style": "IPY_MODEL_45f8bfb31705441c80a22dbb955dd933",
      "value": " 6/6 [02:03&lt;00:00, 20.04s/ba]"
     }
    },
    "9ef8c0b24cb5490b8fa7e8ea8ad619fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a040ab99cb964c32a0a968d97dd55a16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acac4d071be4488fbb272df63b625213": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0be384a887847e981868df7c551d3de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12c3fd408f914038acb511c4ef57f965",
      "max": 66,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_540c7fcbf3b14e538cba18329bc1fc20",
      "value": 66
     }
    },
    "b23174d38e3940718252770bd39177f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bce02c83fbc142c5bb2d8c1f905a478d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd9cb96d5a3c4400bb7d145590540a54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bdf2cb9d10cc4b2aab466a9dfe7be5bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c069967001a542f5b0c3b88ff1841eba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f46e4c24b9294ffdb25e1e64bc594eee",
       "IPY_MODEL_14a450c5594148c3a4ca340521d167ba",
       "IPY_MODEL_9c9b27a0b2ac4be3b8dcbb65e23b84a2"
      ],
      "layout": "IPY_MODEL_c99472b37d644d63a471b65ecbe3bfbd"
     }
    },
    "c5b567cd34b340dea77cefd40e9aaec9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c99472b37d644d63a471b65ecbe3bfbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb1fb197045a44ab8c204a8bdc3c4584": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbed771e77314789bf8817404e5d6f67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b80b681b46b4671a14a7cf82fd50991",
      "placeholder": "​",
      "style": "IPY_MODEL_bce02c83fbc142c5bb2d8c1f905a478d",
      "value": "#1: 100%"
     }
    },
    "cf2b519cc5ff446dae45afa2439c840e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7c6178f42ed4b9f8c5169c21783ce2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dce089e985634a67a381bc85cbaca017": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd4d3c6663db46c9934c0d1f7a128d4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8eebd19f57348669da4353e690a64eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ce95fb7cd2f4cf49b54dfa319f1e835",
       "IPY_MODEL_0633918b582f45c38356130338ccfb58",
       "IPY_MODEL_4f2a3658a0f14f66a59c14d36558c7e6"
      ],
      "layout": "IPY_MODEL_f2758827907544acaa282673eebff9d0"
     }
    },
    "e9ca9f63982744aba53e883882373160": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f10632530a1b499ba3d20f0286446bab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2758827907544acaa282673eebff9d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f46e4c24b9294ffdb25e1e64bc594eee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9ca9f63982744aba53e883882373160",
      "placeholder": "​",
      "style": "IPY_MODEL_4f80ae88d884407ab8f9e08aa58632a4",
      "value": "#1: 100%"
     }
    },
    "f940438113734ccdb198efd12b8a3936": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc7073fb093543fc990abc620348c1ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
