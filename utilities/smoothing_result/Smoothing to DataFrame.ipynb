{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f911073",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5dd901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "import collections\n",
    "import string\n",
    "import contextlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from nusacrowd import NusantaraConfigHelper\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from deep_translator import GoogleTranslator\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BertForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback, \n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba65d2",
   "metadata": {},
   "source": [
    "# Import dataset QAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058a136a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset s_qu_ad_id_dataset (/root/.cache/huggingface/datasets/s_qu_ad_id_dataset/squad_id_source/1.0.0/e1e15a705d3cdb2ce8a2f55db6b8de7355eabe707dcb6ca309972d0d160c2011)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc41eb6b5b94b4694b46144c3496489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 11873/11873 [00:24<00:00, 487.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 130319/130319 [10:38<00:00, 204.22it/s]\n"
     ]
    }
   ],
   "source": [
    "conhelps = NusantaraConfigHelper()\n",
    "data_qas_id = conhelps.filtered(lambda x: 'squad_id' in x.dataset_name)[0].load_dataset()\n",
    "\n",
    "df_train = pd.DataFrame(data_qas_id['train'])\n",
    "df_test = pd.DataFrame(data_qas_id['validation'])\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_test = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_test['context']))):\n",
    "    new_df_test = new_df_test.append({'context': df_test[\"context\"][i], \n",
    "                                    'question': df_test[\"question\"][i], \n",
    "                                    'answer': {\"text\": eval(df_test[\"answer\"][i][0])['text'], \n",
    "                                    \"answer_start\": eval(df_test[\"answer\"][i][0])['answer_start'], \n",
    "                                    \"answer_end\": eval(df_test[\"answer\"][i][0])['answer_end']}}, \n",
    "                                ignore_index=True)\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_train = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_train['context']))):\n",
    "    new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                    'question': df_train[\"question\"][i], \n",
    "                                    'answer': {\"text\": eval(df_train[\"answer\"][i][0])['text'], \n",
    "                                    \"answer_start\": eval(df_train[\"answer\"][i][0])['answer_start'], \n",
    "                                    \"answer_end\": eval(df_train[\"answer\"][i][0])['answer_end']}}, \n",
    "                                ignore_index=True)\n",
    "\n",
    "train_final_df = new_df_train[:-11874]\n",
    "validation_final_df = new_df_train[-11874:]\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_final_df)\n",
    "validation_dataset = Dataset.from_dict(validation_final_df)\n",
    "test_dataset = Dataset.from_dict(df_test)\n",
    "\n",
    "data_qas_id_squad_id = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16f56f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset idk_mrc (/root/.cache/huggingface/datasets/idk_mrc/idk_mrc_source/1.0.0/cf468d86fa7341e69998db1449851672ebfb4fa46036929d66b9de15c421334f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eea8609e18740e1b4ba08ad90f0b5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3659/3659 [00:18<00:00, 199.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 358/358 [00:01<00:00, 240.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 378/378 [00:01<00:00, 251.67it/s]\n"
     ]
    }
   ],
   "source": [
    "conhelps = NusantaraConfigHelper()\n",
    "data_qas_id = conhelps.filtered(lambda x: 'idk_mrc' in x.dataset_name)[0].load_dataset()\n",
    "\n",
    "df_train = pd.DataFrame(data_qas_id['train'])\n",
    "df_validation = pd.DataFrame(data_qas_id['validation'])\n",
    "df_test = pd.DataFrame(data_qas_id['test'])\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_train = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_train['context']))):\n",
    "    for j in df_train[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                           \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                           \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                           ignore_index=True)\n",
    "        else:\n",
    "            new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                                'question': j['question'], \n",
    "                                                'answer': {\"text\": str(), \n",
    "                                                           \"answer_start\": 0, \n",
    "                                                           \"answer_end\": 0}}, \n",
    "                                                           ignore_index=True)\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_val = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_validation['context']))):\n",
    "    for j in df_validation[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                       \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                       \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                       ignore_index=True)\n",
    "        else:\n",
    "            new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": str(), \n",
    "                                                       \"answer_start\": 0, \n",
    "                                                       \"answer_end\": 0}}, \n",
    "                                                       ignore_index=True)        \n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_test = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df_test['context']))):\n",
    "    for j in df_test[\"qas\"][i]:\n",
    "        if len(j['answers']) != 0:\n",
    "            new_df_test = new_df_test.append({'context': df_test[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": j['answers'][0]['text'], \n",
    "                                                       \"answer_start\": j['answers'][0]['answer_start'], \n",
    "                                                       \"answer_end\": j['answers'][0]['answer_start'] + len(j['answers'][0]['text'])}}, \n",
    "                                                       ignore_index=True)\n",
    "        else:\n",
    "            new_df_test = new_df_test.append({'context': df_test[\"context\"][i], \n",
    "                                            'question': j['question'], \n",
    "                                            'answer': {\"text\": str(), \n",
    "                                                       \"answer_start\": 0, \n",
    "                                                       \"answer_end\": 0}}, \n",
    "                                                       ignore_index=True)\n",
    "\n",
    "train_dataset = Dataset.from_dict(new_df_train)\n",
    "validation_dataset = Dataset.from_dict(new_df_val)\n",
    "test_dataset = Dataset.from_dict(new_df_test)\n",
    "\n",
    "data_qas_id_idk_mrc = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d4d2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset ty_di_qa_id_dataset (/root/.cache/huggingface/datasets/ty_di_qa_id_dataset/tydiqa_id_source/1.0.0/77be91de4a88147f2b8ff9e6c1d376d01d26f9b51dd36f775a5607518c034111)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267fe54e8a7a412582b9e4080dbf3518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conhelps = NusantaraConfigHelper()\n",
    "data_qas_id = conhelps.filtered(lambda x: 'tydiqa_id' in x.dataset_name)[0].load_dataset()\n",
    "\n",
    "df_train = pd.DataFrame(data_qas_id['train'])\n",
    "df_validation = pd.DataFrame(data_qas_id['validation'])\n",
    "df_test = pd.DataFrame(data_qas_id['test'])\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_train = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in range(len(df_train['context'])):\n",
    "    answer_start = df_train['context'][i].index(df_train['label'][i])\n",
    "    answer_end = answer_start + len(df_train['label'][i])\n",
    "    new_df_train = new_df_train.append({'context': df_train[\"context\"][i], \n",
    "                                        'question': df_train[\"question\"][i], \n",
    "                                        'answer': {\"text\": df_train[\"label\"][i], \n",
    "                                                   \"answer_start\": answer_start, \n",
    "                                                   \"answer_end\": answer_end}}, \n",
    "                                                   ignore_index=True)\n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_val = pd.DataFrame(columns=cols)    \n",
    "\n",
    "for i in range(len(df_validation['context'])):\n",
    "    answer_start = df_validation['context'][i].index(df_validation['label'][i])\n",
    "    answer_end = answer_start + len(df_validation['label'][i])\n",
    "    new_df_val = new_df_val.append({'context': df_validation[\"context\"][i], \n",
    "                                    'question': df_validation[\"question\"][i], \n",
    "                                    'answer': {\"text\": df_validation[\"label\"][i], \n",
    "                                               \"answer_start\": answer_start, \n",
    "                                               \"answer_end\": answer_end}}, \n",
    "                                               ignore_index=True)    \n",
    "\n",
    "cols = ['context', 'question', 'answer']\n",
    "new_df_test = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in range(len(df_test['context'])):\n",
    "    answer_start = df_test['context'][i].index(df_test['label'][i])\n",
    "    answer_end = answer_start + len(df_test['label'][i])\n",
    "    new_df_test = new_df_test.append({'context': df_test[\"context\"][i], \n",
    "                                    'question': df_test[\"question\"][i], \n",
    "                                    'answer': {\"text\": df_test[\"label\"][i], \n",
    "                                               \"answer_start\": answer_start, \n",
    "                                               \"answer_end\": answer_end}}, \n",
    "                                               ignore_index=True)\n",
    "\n",
    "train_dataset = Dataset.from_dict(new_df_train)\n",
    "validation_dataset = Dataset.from_dict(new_df_val)\n",
    "test_dataset = Dataset.from_dict(new_df_test)\n",
    "\n",
    "data_qas_id_tydiqa_id = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e744ec",
   "metadata": {},
   "source": [
    "# Mulai topik smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7a48d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_word = ['siapa', 'siapakah',\n",
    "                    'apa', 'apakah', 'adakah',\n",
    "                    'dimana', 'dimanakah', 'darimanakah',\n",
    "                    'kapan', 'kapankah',\n",
    "                    'bagaimana', 'bagaimanakah',\n",
    "                    'kenapa', 'mengapa',\n",
    "                    'berapa', 'berapakah', 'seberapa'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "546e62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TG_IND_NAME = \"Wikidepia/IndoT5-base-paraphrase\"\n",
    "MODEL_TG_ENG_NAME = \"humarin/chatgpt_paraphraser_on_T5_base\"\n",
    "MODEL_NER_NAME = \"cahya/xlm-roberta-base-indonesian-NER\"\n",
    "\n",
    "tokenizer_kwargs = {'truncation': True, 'max_length': 512}\n",
    "\n",
    "nlp_ner = pipeline(task=\"ner\", model=MODEL_NER_NAME, tokenizer=MODEL_NER_NAME)\n",
    "\n",
    "nlp_tg_ind = pipeline(task=\"text2text-generation\", model=MODEL_TG_IND_NAME, tokenizer=MODEL_TG_IND_NAME, \n",
    "              device=torch.cuda.current_device(), **tokenizer_kwargs)\n",
    "\n",
    "nlp_tg_eng = pipeline(task=\"text2text-generation\", model=MODEL_TG_ENG_NAME, tokenizer=MODEL_TG_ENG_NAME, \n",
    "              device=torch.cuda.current_device(), **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a203ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(question, pred_answer, gold_answer, type, question_word=question_word):\n",
    "    \n",
    "    question = question.lower()\n",
    "    pred_answer = pred_answer.lower()\n",
    "    gold_answer = gold_answer.lower()\n",
    "    \n",
    "    if type == 'replace first':\n",
    "        pred_hypothesis = question.replace('?', '')\n",
    "        pred_hypothesis = pred_hypothesis.replace(question.split()[0], pred_answer)\n",
    "\n",
    "        gold_hypothesis = question.replace('?', '')\n",
    "        gold_hypothesis = gold_hypothesis.replace(question.split()[0], gold_answer)\n",
    "\n",
    "    elif type == 'replace question word':\n",
    "        for i in question_word:\n",
    "            if i in question.split():\n",
    "                pred_hypothesis = question.replace('?', '')\n",
    "                pred_hypothesis = pred_hypothesis.replace(i, pred_answer)\n",
    "\n",
    "                gold_hypothesis = question.replace('?', '')\n",
    "                gold_hypothesis = gold_hypothesis.replace(i, gold_answer)\n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                pred_hypothesis = question.replace('?', '')\n",
    "                pred_hypothesis = f\"{pred_hypothesis.lstrip()} adalah {pred_answer}\"\n",
    "\n",
    "                gold_hypothesis = question.replace('?', '')\n",
    "                gold_hypothesis = f\"{gold_hypothesis.lstrip()} adalah {gold_answer}\"\n",
    "                break\n",
    "\n",
    "    elif type == 'add adalah':\n",
    "        pred_hypothesis = question.replace('?', '')\n",
    "        pred_hypothesis = pred_hypothesis.replace(question.split()[0], '')\n",
    "        pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "        gold_hypothesis = question.replace('?', '')\n",
    "        gold_hypothesis = gold_hypothesis.replace(question.split()[0], '')\n",
    "        gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "\n",
    "    elif type == 'just concat answer and question':\n",
    "        pred_hypothesis = f\"{question} {pred_answer}\"         \n",
    "        gold_hypothesis = f\"{question} {gold_answer}\"\n",
    "\n",
    "    elif type == 'rule based':\n",
    "        question = question.replace('kah', '')\n",
    "        for j in question_word:\n",
    "            if j in question.split():\n",
    "                if j == 'siapa' or j == 'siapakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_answer} merupakan {pred_hypothesis}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_answer} merupakan {gold_hypothesis}\"\n",
    "                    break\n",
    "\n",
    "                elif j == 'apa' or j == 'apakah' or j == 'adakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "                    break\n",
    "\n",
    "                elif j == 'dimana' or j == 'dimanakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} di {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} di {gold_answer}\"\n",
    "                    break\n",
    "\n",
    "                elif j == 'darimanakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} dari {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} dari {gold_answer}\"\n",
    "                    break\n",
    "\n",
    "                elif j == 'kapan' or j == 'kapankah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} pada {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} pada {gold_answer}\"\n",
    "                    break\n",
    "\n",
    "                elif j == 'bagaimana' or j == 'bagaimanakah':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '')\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "                    break\n",
    "\n",
    "                elif j == 'kenapa' or j == 'mengapa':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, 'alasan').lstrip()\n",
    "                    pred_hypothesis = f\"{pred_hypothesis} adalah karena {pred_answer}\"\n",
    "\n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, 'alasan').lstrip()\n",
    "                    gold_hypothesis = f\"{gold_hypothesis} adalah karena {gold_answer}\"\n",
    "                    break\n",
    "\n",
    "                elif j == 'berapa' or j == 'berapakah' or j == 'seberapa':\n",
    "                    pred_hypothesis = question.replace('?', '')\n",
    "                    pred_hypothesis = pred_hypothesis.replace(j, '').lstrip()\n",
    "\n",
    "                    if 'luas' in pred_hypothesis.split():\n",
    "                        pred_hypothesis = pred_hypothesis.replace('luas', '')\n",
    "                        pred_hypothesis = f\"{pred_hypothesis} memiliki luas {pred_answer}\"\n",
    "\n",
    "                    elif 'jumlah' in pred_hypothesis.split():\n",
    "                        pred_hypothesis = pred_hypothesis.replace('jumlah', '')\n",
    "                        pred_hypothesis = f\"{pred_hypothesis} berjumlah {pred_answer}\"\n",
    "                        \n",
    "                    else: pred_hypothesis = f\"{pred_hypothesis} adalah {pred_answer}\"\n",
    "                        \n",
    "                    gold_hypothesis = question.replace('?', '')\n",
    "                    gold_hypothesis = gold_hypothesis.replace(j, '').lstrip()\n",
    "\n",
    "                    if 'luas' in gold_hypothesis.split():\n",
    "                        gold_hypothesis = gold_hypothesis.replace('luas', '')\n",
    "                        gold_hypothesis = f\"{gold_hypothesis} memiliki luas {gold_answer}\"\n",
    "\n",
    "                    elif 'jumlah' in gold_hypothesis.split():\n",
    "                        gold_hypothesis = gold_hypothesis.replace('jumlah', '')\n",
    "                        gold_hypothesis = f\"{gold_hypothesis} berjumlah {gold_answer}\"\n",
    "                        \n",
    "                    else: gold_hypothesis = f\"{gold_hypothesis} adalah {gold_answer}\"\n",
    "                        \n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                pred_hypothesis = question.replace('?', '')\n",
    "                pred_hypothesis = f\"{pred_hypothesis.lstrip()} adalah {pred_answer}\"\n",
    "\n",
    "                gold_hypothesis = question.replace('?', '')\n",
    "                gold_hypothesis = f\"{gold_hypothesis.lstrip()} adalah {gold_answer}\"\n",
    "                break\n",
    "\n",
    "    elif type == 'machine generation with rule based':\n",
    "        pred_hypothesis, gold_hypothesis = smoothing(question, pred_answer, gold_answer, type=\"rule based\")\n",
    "        pred_hypothesis = nlp_tg_ind(pred_hypothesis)[0]['generated_text']\n",
    "        gold_hypothesis = nlp_tg_ind(gold_hypothesis)[0]['generated_text']\n",
    "\n",
    "    elif type == 'pure machine generation':\n",
    "        pred_hypothesis = f\"{question} {pred_answer}\"         \n",
    "        gold_hypothesis = f\"{question} {gold_answer}\"\n",
    "\n",
    "        pred_hypothesis = nlp_tg_ind(pred_hypothesis)[0]['generated_text']\n",
    "        gold_hypothesis = nlp_tg_ind(gold_hypothesis)[0]['generated_text']\n",
    "\n",
    "    elif type == 'machine generation with translation':\n",
    "        pred_hypothesis, gold_hypothesis = smoothing(question, pred_answer, gold_answer, type=\"rule based\")\n",
    "\n",
    "        pred_hypothesis = GoogleTranslator(source='id', target='en').translate(pred_hypothesis)\n",
    "        gold_hypothesis = GoogleTranslator(source='id', target='en').translate(gold_hypothesis)\n",
    "\n",
    "        pred_hypothesis = nlp_tg_eng(pred_hypothesis)[0]['generated_text']\n",
    "        gold_hypothesis = nlp_tg_eng(gold_hypothesis)[0]['generated_text']\n",
    "\n",
    "        pred_hypothesis = GoogleTranslator(source='en', target='id').translate(pred_hypothesis)\n",
    "        gold_hypothesis = GoogleTranslator(source='en', target='id').translate(gold_hypothesis)\n",
    "\n",
    "    return pred_hypothesis.strip(), gold_hypothesis.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f55978",
   "metadata": {},
   "source": [
    "# Pembuatan DataFrame dengan keseluruhan smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ef71dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_smoothing = ['replace first', 'replace question word', 'add adalah',\n",
    "       'just concat answer and question', 'rule based', 'machine generation with rule based',\n",
    "       'pure machine generation', 'machine generation with translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "54055304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_df_with_smoothing(data):\n",
    "\n",
    "    question_array = []\n",
    "    answer_array = []\n",
    "\n",
    "    replace_first_array = []\n",
    "    replace_question_word_array = []\n",
    "    add_adalah_array = []\n",
    "    just_concat_answer_and_question_array = []\n",
    "    rule_based_array = []\n",
    "    machine_generation_with_rule_based_array = []\n",
    "    pure_machine_generation_array = []\n",
    "    machine_generation_with_translation_array = []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        question = str(data['question'][i])\n",
    "        answer = str(data['answer'][i]['text'])\n",
    "\n",
    "        _, replace_first =  smoothing(question=question, \n",
    "                             pred_answer=\"-\", \n",
    "                             gold_answer=answer, \n",
    "                             type='replace first')\n",
    "\n",
    "        _, replace_question_word =  smoothing(question=question, \n",
    "                             pred_answer=\"-\", \n",
    "                             gold_answer=answer, \n",
    "                             type='replace question word')\n",
    "\n",
    "        _, add_adalah =  smoothing(question=question, \n",
    "                             pred_answer=\"-\", \n",
    "                             gold_answer=answer, \n",
    "                             type='add adalah')\n",
    "\n",
    "        _, just_concat_answer_and_question =  smoothing(question=question, \n",
    "                             pred_answer=\"-\", \n",
    "                             gold_answer=answer, \n",
    "                             type='just concat answer and question')\n",
    "\n",
    "        _, rule_based =  smoothing(question=question, \n",
    "                             pred_answer=\"-\", \n",
    "                             gold_answer=answer, \n",
    "                             type='rule based')\n",
    "\n",
    "        _, machine_generation_with_rule_based =  smoothing(question=question, \n",
    "                             pred_answer=\"-\", \n",
    "                             gold_answer=answer, \n",
    "                             type='machine generation with rule based')\n",
    "\n",
    "        _, pure_machine_generation =  smoothing(question=question, \n",
    "                             pred_answer=\"-\", \n",
    "                             gold_answer=answer, \n",
    "                             type='pure machine generation')\n",
    "\n",
    "        _, machine_generation_with_translation =  smoothing(question=question, \n",
    "                             pred_answer=\"-\", \n",
    "                             gold_answer=answer, \n",
    "                             type='machine generation with translation')\n",
    "\n",
    "        question_array.append(question)\n",
    "        answer_array.append(answer)\n",
    "\n",
    "        replace_first_array.append(replace_first)\n",
    "        replace_question_word_array.append(replace_question_word)\n",
    "        add_adalah_array.append(add_adalah)\n",
    "        just_concat_answer_and_question_array.append(just_concat_answer_and_question)\n",
    "        rule_based_array.append(rule_based)\n",
    "        machine_generation_with_rule_based_array.append(machine_generation_with_rule_based)\n",
    "        pure_machine_generation_array.append(pure_machine_generation)\n",
    "        machine_generation_with_translation_array.append(machine_generation_with_translation)\n",
    "\n",
    "        smoothing_df = pd.DataFrame({\n",
    "\n",
    "            'Question': question_array,\n",
    "            'Answer': answer_array,\n",
    "\n",
    "            'replace first': replace_first_array,\n",
    "            'replace question word': replace_question_word_array,\n",
    "            'add adalah': add_adalah_array,\n",
    "            'just concat answer and question': just_concat_answer_and_question_array,\n",
    "            'rule based': rule_based_array,\n",
    "            'machine generation with rule based': machine_generation_with_rule_based_array,\n",
    "            'pure machine generation': pure_machine_generation_array,\n",
    "            'machine generation with translation': machine_generation_with_translation_array\n",
    "\n",
    "        }) \n",
    "\n",
    "    assert len(smoothing_df) == len(data)\n",
    "    return smoothing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b089a931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 764/764 [52:22<00:00,  4.11s/it]\n"
     ]
    }
   ],
   "source": [
    "smoothing_df = create_df_with_smoothing(data_qas_id_idk_mrc['validation'])\n",
    "smoothing_df.to_excel(\"smoothing_idk_mrc.xlsx\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3f66cd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 565/565 [41:15<00:00,  4.38s/it]\n"
     ]
    }
   ],
   "source": [
    "smoothing_df = create_df_with_smoothing(data_qas_id_tydiqa_id['validation'])\n",
    "smoothing_df.to_excel(\"smoothing_tydiqa_id.xlsx\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858d420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                            | 58/11874 [04:00<15:26:50,  4.71s/it]"
     ]
    }
   ],
   "source": [
    "smoothing_df = create_df_with_smoothing(data_qas_id_squad_id['validation'])\n",
    "smoothing_df.to_excel(\"smoothing_squad_id.xlsx\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6031e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
